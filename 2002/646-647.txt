Proceedings of CSCL 2002                                                                                            page 646

         Continuous Evaluation of Web-based Cooperative
         Learning: The Conception and Development of an
                                            Evaluation Toolkit
                                            Shirley Holst, Torsten Holmer
                     Fraunhofer Institut integrierte Publikations- und Informationssysteme, Germany
                                               {holst, holmer}@ipsi.fhg.de

ABSTRACT
In this paper we propose the concept of continuous evaluation, which combines existing evaluation approaches in the
construction of an evaluation toolkit consisting of guidelines, methods and software tools for the monitoring, analysis and
optimization of cooperative   learning.  The concept   of continuous evaluation is interesting to those    who  wish to  make
systematic evaluations of CSCL systems. It describes those evaluation activities, which are appropriate when planning and
designing CSCL, during early field studies, and throughout the ongoing maintenance of established courses. The aim of
continuous evaluation is to build up a suite of evaluation methods and tools to be used by course organizers, authors, tutors
and learners, which are tailored to a specific e-learning setting and are iteratively improved over time.

Keywords
Evaluation, quality assurance, cooperative learning, web-based learning, formative evaluation, data logging, participatory
evaluation

THE CONTINUOUS EVALUATION APPROACH
The current  explosion   in the  use  of new web-based    technologies to support  cooperative  learning    brings many  new
challenges for those evaluating the effects of these new tools and methods on the learning process. Existing methods for the
evaluation and quality assurance of CSCL systems, in particular criterion catalogues, have been heavily criticized for their
lack of theoretical and empirical foundations (Fricke, 2000). Traditional evaluation approaches are not sufficient to tackle
the evaluation of web-based cooperative learning. Evaluation in this area is difficult because many factors influence the
cooperative learning process, which are always changing. This makes it necessary to make adjustments to the cooperative
tools and learning methods to fit particular settings and emerging requirements. Therefore, there is a need to develop
methods and tools for the formative evaluation of cooperative e-learning, which support course organizers, authors, tutors,
learners (we call these the course `stakeholders') to monitor and optimize their learning process at runtime. We should aim
to provide sufficient information, so that they can reflect on their own activities and take effective action to improve their
learning activities themselves.
We introduce our continuous evaluation approach to evaluating web-based cooperative learning in order to address the
above needs and criticisms. This approach culminates in the construction of an evaluation toolkit (which we call the Quality
Suite) consisting of guidelines, methods and software tools for the monitoring, analysis and optimization of cooperative
learning. Drawing on established theories of cooperative learning we are creating models of learning that explain and
predict the behavior of learners, tutors and learning groups with a particular technology in particular e-learning settings.
Based on these models, we operationalize our evaluation measures in terms of specific behaviors, that are observable during
the completion of a specific cooperative task. Via a series of laboratory and field evaluation activities, within representative
e-learning settings, these models are being explored and tested. Thereby, the factors which contribute to effective learning
are identified. The resulting Quality Suite provides methods and tools to support course stakeholders in their respective
roles within these learning settings.

THE EVALUATION TOOLKIT
The evaluation toolkit (the Quality Suite) will consist of the following tightly interwoven three elements: Guidelines,
Monitoring Tools and the Questionnaire Generator.
Guidelines for how to arrange effective cooperative learning will be used by the various stakeholders of the learning
process in order to plan or improve their learning activities, to give new ideas for ways in which the tools can be used, to
illustrate best practice examples and to support troubleshooting.
Secondly, we extend data logging methods, which have effectively been used to analyze online behavior in groups (Holmer
and Streitz, 1999, in order to support the ongoing monitoring and optimization of learning at runtime. The monitoring tools
will gather information about the learning process via both unobtrusive data logging and via brief online questionnaires that
learners and tutors fill out  at particular  stages of the  learning process. The  questionnaires  can     be made  to appear
automatically after certain events (e.g. immediately after performing a cooperative activity). The data is then automatically
Proceedings of CSCL 2002                                                                                           page   647

analyzed and made available in summarized form to course tutors, giving them a useful overview of how cooperative
learning is taking place in individual cooperative exercises as well as across the whole course.
Thirdly, the questionnaire generator will help those evaluating a course to apply known quality criteria in order to generate
questions about the learning process, which can be answered by the learners, giving feedback to the tutor about the course.
The evaluator selects those quality criteria that are particularly relevant at the time, and the software selects appropriate,
pre-defined questions from a database of quality criteria and an associated pool of questions. The questionnaires can also be
used to enquire about critical incidents, which may have occurred.

EMPIRICAL FOUNDATIONS
The following three evaluation types are contributing to the empirical foundations of the quality suite:
The Internal Evaluation of Cooperative Episodes assesses the usability of tools and the utility of the cooperative activities
independently of any specific course setting. Laboratory studies are used to predict the effects of particular features and
cooperative learning methods on the learning process;
The Evaluation of Course Effects assesses the effect that cooperative episodes have on the ongoing learning process. In
particular, we want to establish what effects the cooperative learning activities have on individual learning activities. We
monitor changes in learner behavior immediately before cooperation (e.g. preparations that are made before taking part in a
cooperative episode) and also afterwards (e.g. the reviewing activities of the learners after the cooperation is completed);
The Investigation of Moderating Variables    investigates how other   factors, such as learning    style  or type of   content,
interacts with the acceptance, appropriateness and effectiveness of particular types of cooperative learning activity.

STATUS
We are developing the Quality Suite in the ALBA project, which is almost one year underway at the time of publishing. We
are cooperating closely with our partners (the German software company, SAP AG; and the vocational training institution,
CJD Maximiliansau) in both corporate and public education settings to gather requirements for the Quality Suite. In a series
of field studies, the toolkit will be developed, used, and iteratively improved. The L³ learning platform, is the test bed for
many of our studies. The L³ project (Life Long Learning as a basic need) is a predecessor to ALBA, in which cooperative
services for the L³ learning platform were conceptualized, developed, and evaluated (Wessner and Pfister, 2000).

ACKNOWLEDGEMENTS
We thank our colleagues for their contributions to both the L³ and ALBA projects and for discussion of this paper.

REFERENCES
Fricke, R. (2000) Qualitätsbeurteilung durch  Kriterienkatalogue.   Auf der Suche   nach validen    Vorhersagemodellen.      In
         Schenkel et a. (2000). pp.75-88.
Holmer, T., and Streitz, N.A. (1999) Neue Möglichkeiten der Analyse der Mensch-Computer-Interaktion zur Evaluation
         von computerunterstützten Gruppensitzungen. In: U. Arend, E. Eberleh, K. Pitschke (Ed.): Software-Ergonomie
         '99. Design von Informationswelten, pp. 137-148, Stuttgart, Teubner, 1999.
Wessner, M., and Pfister, H.R. (2000) Points of Cooperation: Integrating Cooperative Learning into web-based courses. In
         Proceedings of the International Workshop on New Technologies for Collaborative Learning. NTCL 2000, Awaji-
         Yumebutai, Hyogo, Japan, Nov. 27-29, 2000, pp. 33-41.
