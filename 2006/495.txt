                 Scaffolding Learning from Contrasting Video Cases

   Anandi Nagarajan, Cindy Hmelo-Silver, Rutgers University, 10 Seminary Place, New Brunswick, NJ 08901.
                             Email: annagara@eden.rutgers.edu, chmelo@rci.rutgers.edu

         Abstract:    Video  cases can   serve    as valuable    instructional tools   for preservice  teachers  by
         presenting   examples  of effective   and   ineffective student  learning  and    pedagogical  approaches.
         However,     merely seeing  video-cases     without  in-depth,   interpretive  analysis   may  not  ensure
         learning. This study examined the effects of cognitive, metacognitive, combination, and affective
         questions,   on  preservice   teachers'    declarative  knowledge     of   formative   assessment,   case-
         interpretation skills and ability to redesign formative assessments in similar and novel classroom
         contexts. All participants showed significant improvements from pretest to posttest on declarative
         knowledge and case-interpretation skills. A-priori planned contrasts revealed that the cognitive-
         questions   group outperformed     the metacognitive    group  on declarative     knowledge   whereas   the
         metacognitive-questions group outperformed the cognitive group on video case-interpretation at
         posttest and transfer tasks. The results suggest practical implications for using specific scaffolding
         questions  that are effective   in improving  conceptual    knowledge    and  applied  case-interpretation
         skills during contrasting video case analysis activities.

Introduction
         There   is a growing   interest in   using  video cases  as instructional  tools  in  preservice teacher  education
(Cannings,  &  Talley,   2003). Video    cases  can  serve as a  feasible  means    to support  teachers  in learning  about
classroom   practice. Video  provides  a dynamic     medium   to  present authentic    cases of teacher  instruction, student
learning, and classroom interactions. Video cases that are contrasting to one another may further enhance learning
by supporting the development of well-differentiated knowledge of concepts and their relevance in different settings.
The task of analyzing a single video case, however, is complicated, leave alone analyzing contrasting video cases.
Not only do preservice teachers have to identify relevant aspects of a teaching situation, understand the specific
context and integrate conceptual knowledge they might have with the classroom interaction they are viewing, but
they also need to identify the similarities and differences between the contrasting video cases (van Es & Sherin,
2002).

         An  earlier  study  revealed    that analyzing    contrasting cases   without     any guidance   resulted in  lower
conceptual understanding than repeated viewings of a single case (Nagarajan, Chernobisky, & Hmelo-Silver, 2004).
Therefore, it is essential to find effective ways to support preservice teachers' learning from contrasting video cases
such that we can minimize cognitive load, direct focus on target concepts, and facilitate the connection between
theoretical knowledge and practical applications.     Scaffolding prompts or questions are one possible way to guide
video case analyses. However, there is little research about the effects of specific types of questions that positively
facilitate learning from video case analysis activities.

         In this study,  we  examined    the  use of  contrasting  video  cases  in  the   context of  formative assessment.
Formative   assessment   has consistently   shown   positive impact  on   student learning   and   has been identified as an
extremely important component in preservice teacher education (Black & Dylan, 1998). This study explored the
effects of  different  types of  scaffolding    questions  on   preservice teachers'   learning    of  formative assessment
principles, as they analyzed contrasting video cases.

         The conceptual framework for this study was grounded in the cognitive apprenticeship model of learning
whose three main processes of modeling, coaching and scaffolding are intended to help students acquire necessary
cognitive and metacognitive skills as they engage in a learning task (Collins, Brown,         & Newman, 1989). Preservice
teachers learn by noticing and analyzing video cases of what other teachers do in their classrooms. The teacher in
the video case serves as the model performing a task.      Preservice teachers can construct their own understanding by
observing  the model   teacher, interpreting   what  worked   and  what   did  not, internalizing  relevant knowledge,   and
applying that in other relevant contexts.     Coaching is provided to the preservice in the form of task structure and

                                                             495                                                   ICLS 2006
scaffolding prompts. Preservice teachers receive scaffolding in the form of question prompts as they engage in these
complex video case analysis activities.

Learning from Contrasting Cases
          Case based learning affords opportunities for understanding a principle in context of an authentic situation.
However,   a single  case   may   not  be sufficient   to capture    the complexities of certain  concepts  and   lead   to over-
contextualized  knowledge,     and   thereby    hinder   transfer. Transfer  occurs  when    a  person   applies  principles   and
concepts  learned in    one situation  to    solve a new   problem.    The  preparation  for   future learning    perspective  on
transfer  (Bransford &   Schwartz,    1999)     suggests  that cases  prepare  people to learn  and   subsequently    apply  their
understanding in other contexts. Providing a contrasting case prompts them to make comparisons, and may help
them   identify more    elements   than   if they  watched     a single  case. In  addition, if concepts   are   learned  in   this
contextualized and applied fashion, it is more likely that they will remember and apply these concepts to different
cases in other situations. For transfer to occur, the learner must have deep conceptual understanding of the relevant
concepts  and   principles  and   be able    to flexibly  modify   and   apply them  in new    contexts. This  type   of flexible
application is possible only when one has learned about the concepts in multiple contexts.

Why Video Cases?
          Preservice teachers     may  benefit   immensely     by  participating  in video   analysis  activities that focus   on
integrating  principled  theory   with  contextual     practice  (Derry  et al., 2005). After   all, the main    goal of teacher
education programs is to provide students with the necessary conceptual knowledge that can be flexibly applied in
practical classroom     settings. Video   cases    can bridge    the gap  between  theoretical  principles  learned   in teacher
education courses and actual classroom practice by presenting examples of effective as well as ineffective practice
in different classroom situations (Cannings & Talley, 2003).           Video cases have several advantages over text-based
cases in terms of the amount of information they convey as well as their ability to portray non-verbal information
such as facial expressions, voice intonations, gestures, environmental dynamics and so on.             In addition, video cases
present a complex portrayal of incidents and events as they present an authentic scenario unlike text-based stimuli
that may be comparatively simpler and artificial.

          van Es & Sherin (2002) examined how video cases could be used to help preservice and in-service teachers
notice what was happening in their classrooms. Although the authors acknowledged the need for teachers to be able
to attend to ideas brought up by students and flexibly adapt their teaching, they emphasized that this was not an easy
task and teachers needed some sort of support to help them in this process. Specific examples of support were not
discussed or examined in this study.      Owing to the dynamic differences between text-based and video-based cases,
one must be cautious in over-extending the results from studies on text-based cases to contexts where video cases
are used. The potential advantages inherent in video cases might actually make them a more challenging tool to use
in teacher education, therefore requiring additional guidance or scaffolding.

Role of Scaffolding on Learning
          Researchers have discussed the importance of providing preservice teachers with suitable viewing "lens" to
make learning from video cases an effective, meaningful experience (Abell, Bryan, & Anderson, 1998; Fong, Percy,
& Woodruff, 2004; van Es & Sherin, 2002). Preservice teachers need guidance with respect to specific concepts
they should focus on, questions to enable meaningful interpretations, and collaborative discussions.

          A comparison of 11 preservice and 11 experienced elementary teachers was conducted with respect to what
they noticed from exemplary video vignettes on astronomy instruction (Fong, Percy, & Woodruff, 2004). Analysis
of think-aloud protocols were categorized into one of four viewing lenses: a) content lens referred to the subject
matter in the video, b) form lens referred to identification of technical events such as classroom management, c)
pedagogy   lens referred   to viewing   the  teacher   as a master,   with  the  instructional approach   and  strategies   as the
primary focus, and d) surface-level media lens, which referred to comments on the presentation and quality of the
video.    The   results revealed   that   preservice   teachers    made   significantly more    content  lens  statements    than
experienced   teachers  who   made    significantly  greater   number    of pedagogy-lens    statements.  The  most   interesting
finding,  however,   was   that   both preservice    and  experienced     teachers who   were   explicitly asked    to   look  for
"pedagogically sound teaching principles" did exactly that (p. 4). Providing explicit instructions can therefore elicit
appropriate video case analysis behaviors.

                                                                 496                                                   ICLS 2006
          Embedding question prompts is a scaffolding strategy that has been used extensively in research (Hmelo &
Day, 1999). Questions can help the learner focus on contextual cues, help in organizing thoughts, or perhaps trigger
self-reflection and access of prior knowledge that might be relevant and useful to the given task.                The Knowledge
Integration  Environment     (KIE) is   an  example    of a scaffolding   environment    (Davis,   2003)    whose  goals   include:
making    student  thinking    visible, identify   models   of  scientific   phenomena     that   connect   to a  student's   prior
knowledge,     provide  social support   for   peer learning,   and  encourage    students  to  become    autonomous     and  self-
regulated learners. Students may or may not engage in this task unless explicitly prompted to do so.                  Davis (2003)
compared    the  effects of  content-specific     and generic   prompts   on  student   reflection and   found    that the generic
prompts produced more coherent understanding than the directed prompts.

          While   many    studies describe     different  types of  questions    to  scaffold  student   learning,  there  is little
empirical evidence on the effects of specific prompts to support video case analysis activities among preservice
teachers.  van Es & Sherin (2002) designed VAST (Video Analysis Support Tool) to support teachers in observing
and interpreting classroom interactions via video cases..          Six of the 12 intern teachers participated in three 1-hour
training sessions doing video case analysis using VAST, in which teachers were prompted to comment on three
aspects  of  instruction: student    thinking,    teacher's role,  and discourse.     Teachers   analyzed   the   video  cases   by
responding to a general "what do you notice?" question.             They were then provided with sequential prompts that
asked them to identify a noteworthy event, provide evidence from the case to justify the importance of the event, and
then interpret the event.   Results of the study revealed that all six experimental teachers reached the highest scoring
level on   their second   reflective essay     as compared   to   only two   teachers   from   the control  group.     The authors
concluded that experience with VAST helps teachers organize their analysis of events, identify relevant evidence to
support selected events, and make meaningful interpretations. A brief intervention with a scaffolding tool helped
teachers refine their ways of noticing and interpreting events in a video case.          There was however, no indication on
whether   the  scaffolds  in VAST    work    synergistically    to support   this development     or whether    specific   types of
prompts were more influential.

          Berthold, Nuckles, & Renkl (2004) investigated the effect of different types of questions on the quality of
learning protocols written by 84 undergraduate psychology students after watching a videotaped lecture. One group
received   only  cognitive   prompts,   another    received  metacognitive    prompts,    a third  received    a  combination    of
cognitive and metacognitive prompts and a control group received no prompts. Results showed that students in the
cognitive prompts or mixed prompts group outperformed the control group on the amount of cognitive activities in
the learning   protocols. Similarly,    the metacognitive    prompts   groups    and  mixed   prompts   group  outperformed      the
control group on the amount of metacognitive activities in the learning protocols. Clearly, specific prompts fostered
appropriate   condition-specific  cognitive    and  metacognitive    activities.  The   question  that arises  is whether   similar
results can be obtained by using similar prompts in video case analysis activities. There is no empirical research that
has studied the effects of cognitive and metacognitive prompts, similar to the ones used in this study, in video case
analysis activities.

Purpose of the Study
          The purpose of this study was to compare the effectiveness of four questioning techniques on preservice
teachers':  a) declarative   knowledge   of    formative  assessment,  b)    ability to interpret  video cases    for  incidence of
formative   assessment,   and  c) ability   to apply   that knowledge     in redesigning   suitable  formative    assessments    for
familiar and novel classroom scenarios.

Method
Participants
          Participants were 81 students enrolled in an undergraduate course, Introduction to Educational Psychology,
at a large Northeastern University. They were offered 4 course credits and $ 20.00 for their time and participation.
Participants   were  predominantly   female    (n=66),   in their  junior year   (n=51), and   mostly  white   (n=65).   Of all  81
participants, 52 reported having some teaching experience and 62 expressed an interest in pursuing teaching.

Design of the Study
          This study implemented a repeated measures four-group design. Time was the within-group factor.                      The
between-group factor was the questioning condition that had four levels.              The cognitive-questions group received
questions  related   to formative  assessment     that were   task-specific,  context-specific,    factual, and   comprehension-

                                                                497                                                      ICLS 2006
based.  For   example:  What     are the similarities     and differences  in    how the   two  teachers test   students during a
learning   activity and  at  the  end of an   activity?   The  metacognitive-questions       group received   high-level  thinking
questions that were more general, context-independent, and require reflection, self-monitoring, and self-evaluation.
For example: How will I know whether students really understand the concept as they participate in a learning
activity and at the end of the activity?    The combination-questions group received both cognitive and metacognitive
questions  to  guide   their case   analysis. The   control   group  received    a set of  neutral and   affective questions.  For
example: How are you feeling right now? Participants in each condition answered a total of six questions specific to
their condition in order to control for time on task.

Instrumentation
         The entire study was conducted in a customized online learning environment, STELLAR, a "Sociotechnical
Environment for Learning and Learning Activity Research" (Derry et al., 2005).                   Four different versions of the
system were developed to cater for each condition. Participants were randomly assigned to one of four conditions in
Session 1 and completed all activities online.

         Understanding       of formative   assessment     was  measured      at pretest  and  posttest  in terms  of   declarative
knowledge and video case analysis ability. Declarative knowledge was measured based on responses to a 10-item
multiple-choice test and 3 open-ended questions: what is formative assessment, why is it important, and give an
example    of how   you  would      implement   it in a    classroom.     Case    analysis  was  measured    in  two  parts: case-
interpretation and case redesign. All participants saw two contrasting video clips at pretest and posttest.               The 1st
teacher referred    to tests as   a way  of   assessing   knowledge    while  the  2nd   teacher engaged    students in  hands-on
activities and  questioning      to  facilitate learning    and   provide    evidence    of  student understanding.     For  case-
interpretations, participants described their initial observations and made inferences about the assessments used by
the  two   teachers.   The   case   redesign    required   participants   to  apply    their understanding    to   make  practical
recommendations of formative assessments to be implemented in one of the two classrooms.                    As part of a transfer
task, participants saw two novel video cases, completed a case-interpretation task and recommended redesigns for
both cases.

Procedure
         The study was conducted across 3 sessions. In Session 1, all participants logged into the online system and
were oriented to the system after which the researcher showed two short video cases to the entire group using a
projector. Participants then completed the pretest tasks and a demographic survey.

         Session    2  was   held one  week   after Session    1.   Here, all participants   saw a  set  of training video   cases,
conducted an unscaffolded comparison of the two cases, reviewed information on formative assessment, and then
engaged in scaffolded case analysis activities by answering questions specific to the condition they were assigned to.
After  completing    a brief  distracter task,  all participants    viewed    the  pretest video cases   again  and  completed  a
posttest case analysis and redesign.     In Session 3, held a week after Session 2, all participants viewed a set of novel
video cases and completed a transfer case analysis and redesign.

Scoring
         The responses to all open-ended questions, declarative knowledge and case redesign, were coded using a
set of predefined   formative     assessment    criteria.  A  total of nine   features were   identified and  used   to define the
coding criteria for formative assessment features. For instance, criteria 1 and 2 focused on the purpose of formative
assessment:    "Monitors     student  progress,     check    student   understanding     &   misunderstanding"     and   "Adjusts
instruction to meet student needs and/or repair student misunderstanding". Criteria 3 and 4 dealt with timing and
frequency of formative assessment: "Assessment will be carried out during a unit or activity" and "Assessment will
be carried out a number of times during unit/activity/course". In addition, examples of formative assessment were
coded into five different categories such as: i) weekly tests & quizzes, ii) assessment via hand-on activities, iii)
informal oral question and answer sessions either individually or in groups, iv) written essays or paper, and v) other
examples such as daily homework, presentations, journaling and so on.

         After identifying a particular feature in a participants' response, the responses were further classified as, i)
merely a suggested idea, ii) an elaboration of the idea, and/or iii) an explanation how that idea related to assessment
and learning. If the feature was merely suggested as an idea without any detailed description or justification, it was

                                                                 498                                                     ICLS 2006
scored as 1 point. For example: "I would use daily homework to assess student progress". If the response provided a
detailed description of the idea such as: "I would assign homework daily that consisted of questions that asked for
basic facts and questions that asked students to explain their thinking", it was coded as an elaborated idea and
scored an additional 2 points (i.e. 3 points for an elaborated idea). If the response further provided an explanation, it
was  scored   an additional   4  points (i.e. 5 points  for  a  justified  and  well-explained idea);     for example,  "The
experiments give a way to expand the students' grades while the oral prompts can be a simple way of gauging
understanding    without  assigning  an    actual  grade".  If  a response     included an idea,  an    elaboration,   and an
explanation of a specific formative assessment feature, the response was credited for each of them (i.e. a total of 7
points). The total composite score reflected the cumulative score of all features identified and discussed at each task
taking into account the depth of the response in terms of ideas, elaborations, and explanations.          Using this scoring
procedure, three composite scores, for declarative knowledge, case-interpretation skill, and case-redesign skill, were
computed for pretests and posttests. In addition, case-interpretation and case-redesign scores for the transfer task
were also computed. To ensure and reliable coding, two raters scored twenty-percent of all qualitative responses.
The overall percentage agreement between the two raters was 91% across all tasks.

Results
          A set of three repeated measures analysis of variance (ANOVA) was conducted with time (pre-post) as the
within-subjects factor, treatment group (3 scaffolding questions and 1 control group) as the between-subjects factor,
and the three measures of understanding as the dependent variables.        Separate ANOVAs were conducted for the two
transfer tasks.   Each    ANOVA    was   followed  by  a set   of three   orthogonal   planned-contrasts   tests to  examine
differences   between (a)  the  treatment  groups  and   the control,  (b)  the  single-questions  groups     (cognitive-  and
metacognitive-questions group) and the control, and (c) the cognitive- and metacognitive-questions group.

         Descriptive  statistics for all pretest,  posttest and   transfer scores are   shown  in Table    1.  Results of  the
repeated measures ANOVA revealed a significant main effect of time with significant pre-post improvements in
declarative knowledge: F (1, 77) = 5.75, p < .01, and case-interpretation skills: F (1, 77) = 17.35, p < .01, but a
significant decline in case redesign skills: F (1, 77) = 13.31, p < .01.

         The condition by time interaction for case-interpretation was significant, F (1, 77) = 5.73, p < .01. Simple
effects analysis of pre-post mean scores revealed a significant improvement in pre-post scores for the metacognitive:
F (1, 77) = 13.40, p = .01, and the combination-questions group: F (1, 77) = 21.40, p = .01.               Planned-contrasts
analysis revealed  that   the treatment  groups   significantly  outperformed   the  control group     on case-interpretation
scores: F (1, 77) = 9.71, p < .01.      The comparison between the treatment groups and the control group was not
statistically significant for declarative  knowledge   and   case  redesign    scores.  Interestingly, the planned    contrast
comparing cognitive- and metacognitive-questions group revealed that the cognitive-questions group showed greater
improvement from pretest to posttest than the metacognitive group on declarative knowledge: F (1,38) = 5.34, p <
.05.  However,    the metacognitive-questions     group  outperformed      the cognitive-questions group      on video  case-
interpretation: F (1, 38) = 4.01, p <. 05.

Table 1. Descriptive statistics for scores on pretest, posttest, and transfer tasks.

       Composite Score                        Cognitive        Metacognitive      Combination             Control
                                                N=20               N=20                 N=22               N=19
                                            Mean    S.D.       Mean       S.D.    Mean    S.D.         Mean      S.D.
       Pretest Conceptual Knowledge           9.00   3.76         9.90     5.41   8.59       3.76      12.63     9.22
       Posttest Conceptual Knowledge          18.5   6.28       14.60      7.24   16.68      8.14      17.16     7.14
       Pretest Case Interpretation            5.75   3.09         5.30     3.48   5.27       3.64         7.68   4.85
       Posttest Case Interpretation           6.85   4.34         9.60     5.98   10.45      6.46         6.84   4.56
       Pretest Case Redesign                  7.85   4.06       10.40      6.05   8.91       3.95      11.06     4.80
       Posttest Case Redesign                 5.35   4.80         6.55     5.38   8.64       4.74         7.42   5.60
       Transfer Case Analysis               26.95   10.30       36.40     18.11   35.64    17.41       22.42     11.51
       Transfer Case 1 Redesign             12.40    6.48       13.00      6.99   10.36      6.58      11.05     7.17
       Transfer Case 2 Redesign             10.10    6.44         9.50     4.99   7.73       3.78         7.47   4.51

                                                             499                                                    ICLS 2006
         Results of the analysis of variance for the transfer task revealed a significant difference among groups, F
(3, 77) =4.18, p = .01, on case-interpretation.   Planned contrasts revealed that the treatment groups outperformed the
control  group, t (77)   = 2.71,  p  = .01  on case-interpretation   scores.  As  well, the metacognitive-questions       group
outperformed the cognitive-questions group, t (77) = 2.01, p = .05 on case-interpretation. There were no statistical
differences among groups on the case-redesign task.

Discussion
         The salient finding of this study was that scaffolding questions: cognitive, metacognitive or a combination,
facilitated preservice   teachers as they   made  interpretations  from  contrasting video   cases  as opposed     to affective
questions,  in both   similar and  novel  classroom    contexts. This positive   effect was obtained   in spite  of   the short
duration of the intervention and emphasizes its value in facilitating critical observation and meaning-making, in the
complicated activity of contrasting video case analysis.

         The primary purpose of this study was to tease out and test the effects of specific scaffolding questions on
learning. The results showed the differential impact of cognitive and metacognitive questions on different measures
of understanding. Metacognitive questions facilitated participants' case-interpretation ability in both posttest and
transfer tasks, i.e., similar and   novel  contexts.   As  noted  by van  Es  and  Sherin   (2002), analyzing   a  video  case
involves not only identifying relevant events and contextual knowledge but also involves connecting those specific
interactions with abstract conceptual principles of learning and teaching. Therefore, one would expect that questions
that prompted participants to not only compare the contextual events of the two cases but also consider and connect
relevant conceptual knowledge with the events in the video cases would be most effective in eliciting meaningful,
principled-interpretations    from   the  cases.  Cognitive   questions,  on   the  other   hand,   facilitated participants'
development of declarative knowledge as compared to metacognitive questions. It is possible that the cues regarding
context as well conceptual information embedded in the cognitive questions facilitated the understanding of basic
facts about  formative    assessment   that was   the  focus of  the declarative  knowledge   measures.   Even     though  the
cognitive-questions    group    scored  lower  than    the metacognitive-    and  combination-questions     group     on  case-
interpretation tasks, they contributed to overall understanding by positively impacting declarative knowledge.

         The    within-group    analysis  revealed   a significant   improvement    in  declarative  knowledge     and    case-
interpretation skills. However, there was a significant decline in case-redesign scores for all groups from pretest to
posttest. While participants in the treatment groups were generally able to make accurate and relevant inferences
from video cases to a great extent, they failed to apply their understanding in designing assessments. This could be
attributed  to several   factors.  Firstly, the  scaffolding  questions   in  the treatment  groups    were geared    towards
interpreting events from the contrasting video cases and not specifically on redesigning assessments.        The decline in
redesign scores might also be due to the sequence of the three questions in the case analysis task. The first two
questions dealt with case-interpretation and the last question focused on the redesign. It is possible that participants
presented all their ideas in response to the case-interpretation questions and did not feel the need to repeat redundant
information in response to the case redesign question.       Finally, the decline in scores could be attributed to fatigue.
The redesign question was the last of 14 open-ended questions answered at Session 2, a 90-minute long session. The
cognitive load and mental fatigue experienced by participants may have resulted in lower engagement and interest.

         Overall, this study makes a significant and practical contribution to the field of teacher education. If video
cases are to be used as effective, feasible tools for preservice teachers to learn from, it is imperative that adequate
support  should   accompany     the  video   case  analysis  process. Clearly,    specific  types of   scaffolding    questions
contributed to preservice teachers' learning of formative assessment in terms of declarative knowledge and case-
interpretation  ability. Although   the  instructional intervention  did  not significantly  influence  the redesign     ability
among    preservice   teachers, significant  improvements    in  analytic and  interpretive  skills are an  important     start.
Perhaps a more time-intensive intervention with several training sessions and planning-related scaffolding questions
might lead to improved application.

Conclusions
         Engaging     in contrasting   video  case  analysis  activities  can be  beneficial  by  providing     an alternative
approach to gain conceptual knowledge about relevant pedagogical content in contextualized settings. As well, they
present opportunities to make meaningful interpretations from the perceptual stimuli represented in the case, when
accompanied by appropriate guiding questions. The relative advantage of metacognitive questioning strategies over

                                                              500                                                     ICLS 2006
cognitive  questions   on  case interpretation   skills suggests   the  overwhelming    need to  provide  suitable    training  to
teachers who   might    use video   cases as  instructional  tools   as well as to practitioners  who   are    required   to make
interpretations in actual classrooms on a regular basis.      It is also important to keep in mind that additional training
and tailored   support  is  needed    to support  the   development     of planning   and  redesign    skills. Whether    similar
questioning strategies that focus on those specific aspects would yield similar results is an empirical question.

         The   results  of this study  suggest   specific   ideas for  using contrasting  video  cases  effectively    in teacher
education. Firstly, it  is  essential to  engage  teachers   in   "active  participation" when   developing     an instructional
activity around video cases.    Passively watching video cases without a particular driving question may not facilitate
learning. Providing task goals in the form of analysis questions, no matter how general they are, makes the task
more generative, thereby engaging the participant teacher in critical observation and interpretive analysis.           Secondly,
preservice teachers need guidance along with task goals. While the guidance does not have to be so specialized that
it negates active processing on part of the teacher, it should be appropriately structured so as to facilitate the case
analysis process. As preservice teachers gain experience in analyzing contrasting cases with scaffolded practice, the
guidance  may  be   gradually   removed.   The   study  also showed     that specific questions  promoted      different  types of
understanding.  While      cognitive  and  context-specific   questions      were positively   related to   the  acquisition    of
declarative knowledge, metacognitive questions by themselves or in combination with cognitive questions helped
preservice teachers in making meaningful interpretations and inferences from multiple classroom contexts.                     One
would expect   that metacognitive     questions  that   are specifically  written to  scaffold  case-redesigns   would    also  be
extremely beneficial. However, this needs to be empirically investigated in a classroom context.

References
Abell, S.K., Bryan, L.A., & Anderson, M.A. (1998).           Investigating preservice elementary science teacher reflective
         thinking using integrated media case-based instruction in elementary science teacher preparation. Science
         Teacher Education, 82, 491-510.
Berthold,  K., Nückles,    M.,  & Renkl,   A. (2004).   Writing   learning   protocols: Prompts  foster  cognitive     as well  as
         metacognitive activities and learning outcomes. In J. Elen, P. Gerjets, R. Joiner, & P. Kirschner (Eds.),
         Proceedings of the Special Interest Meeting 2004 of EARLI SIGs 6 and 7, Tuebingen.
Black, P., & Dylan, W. (1998).        Assessment and classroom learning. Assessment in Education: Principles, Policy
         and Practice, 5(1), 7-75.
Bransford, J.D., & Schwartz, D.L. (1999).        Rethinking transfer: A simple proposal with multiple implications.             In
         A.Iran-Nejad,     &   P.D. Pearson   (Eds.),   Review    of Research   in Education,   24, 61-100.     DC:    American
         Educational Research Association.
Cannings, T., & Talley, S. (2003).    Bridging the gap between theory and practice in preservice education: The use of
         video case studies.    Paper presented at IFIP Working Groups 3.1 and 3.3 Working Conference: ICT and
         the Teacher of the Future, The University of Melbourne, Australia.
Collins, A., Brown,     J. S., &  Newman,     S. E. (1989).   Cognitive    apprenticeship:  Teaching    the    crafts of  reading,
         writing, and mathematics. In L. B. Resnick (Ed.), Knowing, learning, and instruction: Essays in honor of
         Robert Glaser (pp. 453-494). Hillsdale, NJ: Lawrence Erlbaum Associates.
Davis, E.A. (2003).     Prompting     middle  school    science   students for productive  reflection:  Generic    and    directed
         prompts.   Journal of the Learning Sciences, 12, 91-142.
Derry, S. J., Hmelo-Silver, C. E.,       Feltovich, J. Nagarajan, A., Chernobilsky, E. & Halfpap, B. (2005). Making a
         Mesh   of  It: A   STELLAR      Approach   to   Teacher   Professional   Development.   In T.   Koschmann,       D.    D.
         Suthers,  & T-W. Chan (Eds.), Proceedings of CSCL 2005 (pp. 105-114). Mahwah NJ: Erlbaum.
Fong, C., Percy, J., & Woodruff, E. (2004).      What do teachers see in an "exemplary" astronomy video? Astronomy
         Education Review, 3, 1-6.
Hmelo, C. E. & Day, R. (1999). Contextualized questioning to scaffold learning from simulations.                Computers and
         Education, 32, 151-164.
Nagarajan, A., Hmelo-Silver, C.E., & Chernobilsky, E. (2004). The benefits and challenges of learning from
      contrasting video cases. In Y. B. Kafai, W. A. Sandoval, N. Enyedy, A. S. Nixon & F. Herrera (Eds.),
      Proceedings of Sixth International Conference of the Learning Sciences (p. 624). Mahwah NJ: Erlbaum.
van Es,  E., &  Sherin,     M.(2002).    Learning   to  notice:   Scaffolding  new    teachers' interpretations    of  classroom
         interactions. Journal of Technology and Teacher Education, 10, 571-596.

                                                               501                                                      ICLS 2006
