     Give Learners Questions to Answer While Watching Animated
                                                   Examples

     Brian D. Gane & Richard Catrambone, Georgia Institute of Technology, 654 Cherry St., Atlanta, GA 30332
                                      bgane@gatech.edu, rc7@prism.gatech.edu

        Abstract: Participants (N = 93) watched animated examples of a computer algorithm. Participants
        had either a small set of examples (6) to watch or a larger set of examples (21) from which they
        could choose which examples to watch. Additionally, participants either watched the animations
        before answering questions or had questions to answer while watching animations. Participants
        scored highest when they had questions with the animations; giving these participants a choice
        amongst multiple examples also reduced learning time.

        Although computer scientists have developed many animation systems to demonstrate the operation of
algorithms, empirical evaluations of the pedagogical effectiveness of these animations have yielded mixed results
(Hundhausen, Stasko, & Douglas, 2002). These inconsistent results might be because the majority of the animation
systems have not utilized findings from research on learning. The goal of this study is to utilize two educational
concepts, learning scenario and learner involvement, to investigate how the presentation of the animations can be
altered in order to improve learning.

        Most algorithm animations       studies have used     a study scenario, in which  learners watch algorithm
animations for a set period of time, rather than a homework scenario, in which learners answer questions while they
view animations (Hundhausen et al., 2002; Kehoe, Stasko, & Taylor, 2001). Of the 24 algorithm animation studies
reviewed by Hundhausen et al. (2002), only two used a homework scenario. A study scenario might hinder learning
because novices have difficulty determining which aspects of an animation to focus on (Lawless & Brown, 1997;
Stasko, Domingue, Brown, & Price, 1998). In a homework scenario, however, questions can act as guides, allowing
students to focus on the critical parts of the animations.

        Learner involvement in example creation might encourage learners to develop and test inferences about the
algorithm (Hansen, Narayanan, & Schrimpsher, 2000). Giving learners the ability to input values into the algorithm
can increase students' interest and motivation (Stasko et al., 1998). Complete control of inputs is problematic,
however. First, novices have difficulty recognizing what input data will demonstrate interesting results (Stasko et al.,
1998). Second, complete control requires learners to decide what behavior they want to see and the inputs to
demonstrate that behavior; this mental workload could hinder their ability to understand the algorithm (Lawless &
Brown, 1997; Sweller, 1999). Therefore, we gave learners some freedom to choose examples, while eliminating the
need to create the examples from scratch. To do this, we used multiple animations that demonstrate the same
concept, but with different data. We hypothesized learners would gain understanding of the algorithm if, before
selecting an animation, they reasoned about which example would provide the information they needed to learn.

Method
        Ninety-three participants were recruited from the Georgia Institute of Technology and were compensated
with course credit. We used a 2 x 2 between-subject design to cross learning scenario (homework or study) with
learner involvement (high or low).     Participants were given questions to answer as they viewed the animations
(homework) or after they finished viewing the animations (study). Participants could choose which animations to
view and the order (high involvement), or they viewed a predetermined set of animations (low involvement).
Participants were randomly assigned to conditions.

        Twenty-one animations demonstrated how algorithms manipulated a binomial heap data structure. Nine of
the animations demonstrated INSERT,        EXTRACT-MIN,          and UNION   operations. The remaining  animations
illustrated low-level details about binomial heaps (e.g., number of nodes in a binomial tree). Narrated explanations
of the animations were simultaneously presented via headphones. The 21 animations consisted of six classes of
examples. Each class had either three or six animations that demonstrated key concepts and illustrated variations due
to differences in the data contained in the binomial heap. For instance, depending on the existing heap, the INSERT
operation will invoke a recombination of none, some, or all of its trees. Therefore, for the class of INSERT
examples, one example showed a binomial heap with no trees recombined, one showed some trees recombined, and
one showed all trees recombined. Participants in the high involvement condition could choose any combination of

                                                            922                                                ICLS 2006
the three INSERT examples to watch; participants in the low involvement condition could not choose which
animation to watch (i.e., only one was available to watch). For each example class we selected the animation that we
considered the most informative, and we made it available to participants in the low involvement condition.

         On the interface, animations were grouped by example class, yielding six groupings; each animation had a
short title that summarized the animation. In the low involvement condition, only one animation from each example
class was available (i.e., a total of six animations available); participants were able to watch each animation as many
times as they liked but could not return to previous examples. In contrast, learners in the high involvement condition
could choose any animation in any order (i.e., a total of 21 animations available). Because there was not enough
time to watch all 21 animations, participants in the high involvement condition had to choose a subset of animations.

         Participants in the study condition were given 35 minutes to watch the animations. The software then shut
down and they had 15 minutes to complete Posttest1. Participants in the homework condition had 50 minutes to
watch the animations and answer the Posttest1 questions; they were allowed to divide their time however they
wished. After completing Posttest1 all participants had 20 minutes to finish Posttest2. The posttests included
procedural, conceptual, near transfer, and far transfer questions. Posttest1 contained 14 questions and Posttest2
contained 19 questions.

Results
         Viewing behavior and test performance were analyzed. Our measure of viewing time is the time elapsed
between starting the first animation and stopping the last animation. This measure might overestimate the viewing
time for those in the homework condition because they could answer questions while the animations were running.
Given this caveat, there was a significant interaction between learning scenario and learner involvement on viewing
time, F(1, 89) = 5.98, MSE = 41.39, p = .02. Post-hoc contrasts were conducted to examine the simple effects of
learner involvement; although the following comparisons are not significant they remain informative. In the study
scenario, the low involvement condition yielded a shorter study period (M = 26.63 min, SD = 5.11 min) than the
high involvement condition (M = 29.96 min, SD       = 4.52 min), F(1, 89) = 3.15, p = .08. In the homework   scenario,
the low involvement condition  yielded   a   longer study period (M  = 33.75 min,    SD  = 6.86  min), than  the high
involvement condition (M = 30.56 min, SD = 8.54 min), F(1, 89) = 2.84, p = .10.

         Homework scenario participants scored higher on Posttest1 (M = 11.09, SD = 1.96) than study scenario
participants (M = 10.09, SD = 1.99), F(1, 89) = 5.90, MSE = 3.94, p = .02. There was not a significant effect of
learner involvement nor   a significant interaction.  A post-hoc  contrast examined    whether  learner involvement
affected scores in the homework scenario; there was not a reliable difference between the low involvement condition
(M = 10.78, SD = 2.35) and the high involvement condition (M = 11.39, SD = 1.47), F(1, 89) = 1.08, p = .30. For
Posttest2, there was not a significant effect of learning scenario or learner involvement, nor was there an interaction.

Discussion
         The homework scenario increased participants' performance on Posttest1. When a small, predetermined set
of animations was used, the homework scenario increased study time while improving performance. Furthermore,
the homework scenario, when paired with the high involvement condition, resulted in a lower study time and a
higher Postest1 score. Therefore, providing learners with questions, and the choice between multiple examples that
can answer those questions, appears to be an efficient method for teaching with animations. This combination of
questions with multiple examples reduced study time without lowering posttest performance.

References
Hansen, S. R., Narayanan, N. H., & Schrimpsher, D. (2000). Helping learners visualize and comprehend algorithms.
         Interactive Multimedia Electronic Journal of Computer-Enhanced Learning, 1.
Hundhausen, C. D., Douglas, S. A., & Stasko, J. T. (2002). A meta-study of algorithm visualization effectiveness.
         Journal of Visual Languages and Computing, 13, 259-290.
Kehoe, C., Stasko, J. T., Taylor, A. (2001). Rethinking the evaluation of algorithm animations as learning aids: An
         observational study. International Journal of Human-Computer Studies, 54, 265-284.
Lawless, K. A. & Brown, S. W. (1997). Multimedia learning environments: Issues of learner control and navigation.
         Instructional Science, 25, 117-131.
Stasko, J.T., Domingue, J., Brown, M.H., Price, B.A. (eds.). (1998). Software Visualization: Programming as a
         Multimedia Experience. Cambridge: MIT Press.
Sweller, J. (1999). Instructional design in technical areas. Melbourne, Australia: ACER Press.

                                                          923                                                ICLS 2006
