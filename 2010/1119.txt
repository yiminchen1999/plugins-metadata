                                                ICLS 2010   ·  Volume 1

                    Promoting Learning in Complex Systems:
       Effect of Question Prompts versus System Dynamics Model
 Progressions as a Cognitive-Regulation Scaffold in a Simulation-
                          Based Inquiry-Learning Environment
Deniz Eseryel & Victor Law, University of Oklahoma, 820 Van Vleet Oval Rm 321, Norman, OK 73019-2041
                                                        USA
                                       Email: eseryel@ou.edu, vlaw@ou.edu

         Abstract:   Designing   effective  technology-based      learning    environments   is  challenging.
         Designing   effective technology-based    learning    environments    to facilitate learning   about
         complex knowledge domains is more challenging. To a large extent, the key to the puzzle lies
         in identifying which scaffolding strategies are more effective; and under which conditions. In
         a simulation-based inquiry-learning environment, this controlled study investigated the effect
         of  two  promising    scaffolding  strategies; question   prompts    and   system dynamics     model
         progressions, on ninth-grade biology students' cognitive regulation and complex problem-
         solving  skills. For  simpler  complex    problems,     findings suggested   that both    scaffolding
         strategies were equally effective. However, as the problems increased in complexity, system
         dynamics model progressions were significantly more effective for facilitating both cognitive
         regulation and complex problem-solving skills.

Introduction
How  can     we effectively facilitate learning in complex       knowledge    domains  such  as  science,  technology,
engineering,  and  mathematics  (STEM)?     This question     led us  to  the study presented   in this paper.  Recent
research on problem solving in complex knowledge domains illuminates a number of learning challenges due to
the nature and structure of the real-life problems in these domains. For instance, ecology is a difficult subject
matter for many students. Learning ecology calls for learning about the complex ecology systems. In other
words, students   should    understand  the dynamic     interdependencies     among   different organisms    and  their
environment.    As such, a complex system involves a large number of interdependent variables and they are all
connected with one another dynamically (Dörner, 1996).         Prior research suggests that humans have significant
difficulties in  understanding  complex     systems,    due   to  the challenges    involved  in   building  a  mental
representation of complex systems, predicting dynamics outcomes, monitoring the cause and effects of complex
systems, and monitoring their own strategies of information processing. (Dörner, 1996; Dörner & Wearing,
1995; Funke, 1991).
         Mental model theory provides a framework to understand the learning processes of complex systems,
in which learners take the input from their learning environments and simulate the events in their mind to
accommodate revised mental models (Rumel, Smolensky, McClelland & Hinton, 1986; Seel, 2001).                    While
learning about complex systems, learners are required to construct and reconstruct their mental models, which
(1) guide learners' comprehension of the systems, (2) allow them to explain the system states, and (3) allow
them to predict the system behaviors (Greeno, 1989; Seel, 2006; Young, 1983).
         Therefore, it is argued that simulation-based inquiry learning environments would help students in the
inquiry processes, because they allow learners to visualize and investigate complex systems (de Jong & van
Joolingen, 1998; Linn, Davis & Bell, 2004).      In a simulated-based inquiry-learning environment, students are
expected to   discover dynamic   relationships  underlying     a  complex  system   through  an  iterative   process of
hypothesis generation, data collection, and data analysis. Indeed, simulation-based inquiry learning is gaining
visibility, especially in the area of science education, as a way to address the challenges of complex learning
(The   National   Research   Council,   2000).     Education      researchers  advocate    simulation-based    learning
environments to foster scientific inquiry skills (Wilensky & Reisman, 2006).          However, studies showed that
many students have difficulties with simulation-based inquiry learning (Hogan & Thomas, 2001).             In addition,
the effectiveness of inquiry-based learning is debatable (de Jong & van Joolingen, 1998; Linn et al., 2004).
Recent research suggests that low-level cognitive regulation skills are a major cause of failure in simulation-
based inquiry learning (Lavioe & Good, 1998; Simmons & Lunetta, 1993; Schauble et al., 1991; Shute & Glaser,
1990).
         Cognitive regulation, which refers to how individuals engage in a recursive process utilizing feedback
mechanisms to direct and adjust their learning and problem-solving processes, is crucial for constructing mental
model of the complex system to be learned (Azevedo, Guthrie & Seibert, 2004; Manlove, Lazonder & de Jong,
2006), especially when learners are lacking required domain knowledge (Pintrich, 2000). Thus, it is argued that
simulation-based   inquiry  learning   environments     should   include  scaffolds to  support    students' cognitive

                                                   1119   ·   © ISLS
                                                    ICLS 2010   ·  Volume 1

regulation.   This, in   turn, would    positively  support     students'  inquiry  processes   starting  from   hypothesis
generation to their interpretation of the findings.
         A review of the literature reveals two quite different scaffolding strategies for cognitive regulation that
can be ubiquitously embedded into simulation-based inquiry learning environments: (1) question prompts (Ge
&  Land,    2004);  and  (2) model    progressions   (White     &  Frederiksen,   1990).  Providing  question    prompts   to
students yielded promising results in some studies (see Ge & Land, 2003); however, others did not yield such
positive results (see Greene & Land, 2000; King, 1992). Similarly, model progression did not yield consistent
effects on learning performance (Alessi, 1995; Quinn and Alessi, 1994; Rieber and Parmley, 1995). We argue
that questions prompts and model progressions may enhance students' cognitive regulation skills, which, in turn,
foster their  learning  of complex    system;   however,    the   complexity   of the  learning domain    is a  main   factor
determining the effectiveness of question prompts and model progressions as cognitive regulation scaffolds.
         Despite    its importance,   however,   there  have    been  very  few   systematic  studies  on which     of these
strategies are more effective, under which conditions, in scaffolding students' cognitive regulation and learning
about complex systems in a simulation-based inquiry-learning environment (Kirschner, Sweller, & Clark, 2006).
Moreover, limited number of studies investigated the relationships between cognitive regulation and learning
performance in complex systems while existing studies on other technology-based learning environments point
out to mixed results.
         The purpose of this study is to examine the effect of two promising technology mediated scaffolding
strategies,  question   prompts     and model   progression,      on  ninth-grade   students' cognitive   regulation    skill
acquisition and learning about complex ecology systems. We hypothesize that task complexity is an important
determinant for whether or not certain cognitive regulation scaffolding strategies are effective. This study seeks
to replicate past findings and eliminate some confounding factors.
         The reminder of our paper is organized as follows. The next section presents the details of the research
study, the research questions, the design of the experiment and the data analysis methods.               Then, results are
presented.   Finally, we conclude with a discussion of the implications of our study and provide suggestions for
future research.

The Present Study
The purpose of this study was to investigate the effects of (1) question prompts (QP),and (2) system dynamics
model  progressions     (MP),  in   scaffolding ninth-grade     students' learning  of  a complex    ecology    system  in a
simulation-based inquiry-learning environment. In addition, this study aimed at examining the relationships
between    cognitive    regulation, learning  about  complex      systems,  and   task complexity.   Thus,   the  following
research questions were posed:
1.  Does scaffolding with question prompts and system dynamics model progressions affect students' cognitive
    regulation in the process of developing solutions to problem scenarios ranging in their level of complexity?
2.  How     do task  complexity     influence   the effectiveness    of  question  prompts   and system   dynamic      model
    progressions as cognitive regulation scaffolds to support students' learning about complex ecology system?

Participants and Design
A rural high school in the Midwest of the United States was used as a testbed for this study. 251 ninth-grade
biology students were randomly assigned to one of the ten classes. Out of these ten classes, five were randomly
assigned   to  experimental    (system  dynamics     model    progression   group)  condition   and  five were    randomly
assigned to control (question prompts group) condition. Of the 219 students, from whom we received both
consent  and   parental    assent   forms, 113   were   in  the   experimental    group   and   106  were    in the  control
group.  There were 50.6% males and 49.4% females.

Materials
Food Chain is a simulation-based inquiry-learning environment designed to support students while they carry
out a sequence of activities that correspond to the steps in the scientific method. Food Chain incorporated
always-available    domain-specific     knowledge,   scaffolded    activities, and  analytic tools  to help  students  learn
about  the  processes   of experimental    design   and data    analysis, the  nature  of scientific argument    and   proof.
Scaffolding strategies incorporated in Food Chain are aimed at supporting cognitive regulation, metacognitive,
and inquiry skills required for implementing scientific methods and effective complex problem-solving. Food
Chain was originally developed by isee Systems and modified for the purposes of this study.

Procedure
Participants   in  both  study  conditions    interacted   with   the   Food   Chain   simulation-based   inquiry-learning
environment for three weeks. In the first week, participants were introduced to the Food Chain simulation-based
inquiry-learning system. They were asked to design their own experiments and test their hypotheses to answer a
relatively  simple  complex    problem     (challenge#  1):   which   two   species (out  of eight)  can  survive   in Lake

                                                      1120    ·   © ISLS
                                                ICLS 2010   ·  Volume 1

Mirabile by themselves for 90 days? This first week was intended to familiarize students with the simulation
environment and to teach about scientific reasoning processes of hypothesis generation and testing. During this
initial run, students in both conditions had access to domain-specific knowledge related to the species in Lake
Mirabile, but they did not receive any scaffolding from the system. At the end of each inquiry-cycle, they only
received a text-based verification feedback of whether or not their hypotheses were correct.
        In the second week, participants tackled a problem scenario with medium complexity (challenge# 2), in
which they were asked to identify the smallest number of species that will enable Sunfish, one specie out of the
eight species in the simulated environment, to survive for 90 days in Lake Mirabile. Their entire hypotheses,
experimental designs, and elaborated reports of the findings were collected.
        During   the   third week,   participants tackled   a  very  complex   and  ill-structured problem    scenario
(challenge# 3), which called for environmental policy-making. In this problem scenario, students were asked to
play the role of an environmental scientist and evaluate the proposal to build 100 new houses on the shoreline at
Lake Mirabile from an environmental impact standpoint. All of their hypothesis, results of their experiments,
and their elaborated report of recommendations and assessment were collected.
        During the second and third weeks, the only difference between the two study conditions was the type
of scaffolding strategy provided by Food Chain at the end of each inquiry cycle. Following their experiment,
participants in the experimental (MP) condition were only provided with a text-based verification feedback (e.g.,
Nice try...but the species you chose didn't survive! View logic model and try again) and they were given access
to the system dynamic model progression, which included an annotated system dynamics model progression
(see Figure 1). On the other hand, participants in the control (QP) condition were only provided with a text-
based verification feedback    and  related  question-prompts    (see  Figure 2) to scaffold   their new   hypothesis
generation.

Measures and Data Analysis
In Food Chain, students were required to go through inquiry processes to solve three ecology problems in
increasing complexity.   For each problem, they were asked to go through the inquiry process at least four times.
Each  inquiry  cycle  in Food    Chain  simulation   involved   the  following  steps.  First, students  individually
developed   a hypothesis  using   their prior  knowledge    and  information  given  by  the   software.   Then,  they
observed the  results and analyzed the  charts generated  by the simulation environment for each variable.    Finally,
they explained the results.  A computer-generated question prompt or an annotated dynamic model progression
was provided to the students to interpret their findings. Students were expected to develop their subsequent
hypotheses  using  these  scaffolds.  Scoring  rubrics   were  used  to rate participants' cognitive  regulation  and
complex learning based on their protocols, which included, for each challenge (complex problem) in the Food
Chain simulation, four sets of hypotheses, hypotheses justification, results, and explanation of the results.
        Cognitive     regulation refers to  how   individuals   engage  in a  recursive process    utilizing feedback
mechanisms to direct and adjust their learning and problem-solving activities. Therefore, cognitive regulation
was  measured   by rating how    well   the previous inquiry   cycle  informed the  consecutive    inquiry cycle, and
whether or not the participant responded the cognitive regulation scaffold (question prompt or system model) in
generating and justifying the subsequent hypothesis (see Table 1).

 Figure 1. Model progression scaffold received by              Figure 2. Model progression scaffold received by
       participants in experiment condition                           participants in experiment condition

                                                    1121  ·   © ISLS
                                               ICLS 2010   ·  Volume 1

Table 1: Overview of the scoring rubric for cognitive regulation measure

Coding Category        Number of points given
Quality of the         2 points: strong evidences of using system feedback to develop the subsequent
hypothesis             hypothesis; 1 point: some evidences of using system feedback to develop the subsequent
                       hypothesis; 0 point otherwise
Quality of             8 points total: There are two questions per question prompt. 2 points for a clear and
justification of the   correct answer to each question, and 2 points for strong logical justification responding
hypothesis             to each question. Deduct 1 point per question if the response is not elaborated clearly.

          In addition, complex learning was measured by rating participants' hypothesis, hypothesis justification,
and explanation   of the results in the  inquiry cycles    to see if a participant  is developing   a comprehensive
understanding of the lake ecology system and the dynamic interrelationships among its variables. In other words,
whether students are able to (1) identify various variables affecting the lake ecosystem (for example, fungi,
bacteria, green algae,  diatoms, sun energy,    CO2, O2,   nutrients,  etc.), and (2)  identify the interrelationships
among these variables (for example, diatoms produce the oxygen that fuels bacterial respiratory processes,
while bacteria generates the C02 and nutrients that diatoms require for driving their photosynthetic processes.
Both diatoms and bacteria die, of natural causes, feeding the stock of detritus. That stock, in turn, nourishes
bacteria that then decompose detritus to create the nutrients that diatoms require for nourishment.) The scoring
rubric of the complex learning measured is shown in Table 2.
          Two raters coded all protocols based on the analytical rubric system developed by the research team.
The interrater reliability was 95% for cognitive regulation measure and 90% for complex learning measure. Any
discrepancies  of assigned  scores  were  discussed    among    the  raters   and the  adjudicated  score  was used.
Consequently, a high consensus was reached. Repeated measures ANOVA analysis was conducted to examine
change on cognitive regulation and complex learning measures both after challenge 1, after challenge 2, and
after challenge 3 in the Food Chain simulation.

Results
Table 3 summarizes the descriptive statistics for the two dependent variables (cognitive regulation and complex
learning) for both control (QP) and experimental (MP) groups along the three challenges: (T1) simpler complex
problem where no scaffolding was provided from the system; (T2) more complex problem scaffolded with
either QP or MP; and (T3) very complex, environmental policy analysis problem scaffolded with either QP or
MP. Below is the statistical analysis report in response to each of the hypotheses tested.

Table 2: Overview of the scoring rubric for complex learning measure

Coding Category        Number of points given
Correct variables      2 points: all correct variables are identified; 1 point: some correct variables are
                       identified; 0 otherwise.
Energy cycle           1 point: evidences of understanding that plants harness energy from the sun; 0 point
                       otherwise
CO2/O2 cycle           4 points: evidences of understanding that some species are providing CO2 and
                       consuming O2 in the ecosystem and other species are providing O2 and consuming CO2
                       in the ecosystem; deduct one point each if the students could not identify CO2 , O2,
                       consumption or production cycle.
Nutrient cycle         3 points: evidences of understanding that species are providing for other species for
                       nutritional needs; deduct one point each if (1) the student identify do not have the
                       complete food cycle, (2) cannot identify decomposer role in the system, and (3) do not
                       give a clear explanation.

                                                  1122   ·   © ISLS
                                                  ICLS 2010   ·  Volume 1

Table 3. Descriptive statistics for cognitive regulation and complex learning measures

               Control (QP) Group                                      Experimental (MP) Group
Measures       Mean                        Std. Dev.                   Mean                       Std. Dev.
               T1        T2        T3      T1       T2        T3       T1        T2     T3        T1      T2       T3
Cognitive      2.01      4.21      5.72    1.03     1.13      1.95     1.97      4.15   6.20      0.96    1.70     2.16
Regulation
Complex        1.84      3.45      4.33    0.89     0.49      1.16     1.64      3.75   6.92      1.04    0.89     1.30
Learning

Effects on Cognitive Regulation
The   first research  question  asked   whether  scaffolding    with  question  prompts  and system     dynamics  model
progressions affect student cognitive regulation in the process of developing solutions to complex problem
scenarios.
        As evident in the descriptive statistics shown in Table 2, participants in the MP condition had lower
mean scores than the participants in the QP condition in cognitive regulation after the first challenge in the Food
Chain, during which no cognitive regulation scaffolding was provided by the system. However, the repeated
measures    ANOVA      analyses did   not indicate  significant  differences   between  the  two  groups    in these two
dependent variables F (2, 209) = 0.47, p > .05, !2 = .025. The results indicated that the participants in both
conditions were comparably on equal basis after they completed the first challenge.
        However, the repeated measures ANOVA analyses revealed a significant main effect in participants'
cognitive regulation of inquiry learning in both QP and MP conditions between the first challenge, where no
scaffolding were provided by the system, and the second challenge, where participants received either scaffold
F (2, 209)   = 0.47,   p > .05, !2  =   .025, which  supported    the  hypothesis that  students, when   received  either
cognitive   regulation   scaffold, would  perform    significantly   better than  when  cognitive  regulation   was  not
scaffolded   during  inquiry  learning. Nevertheless,  after    completing  the  second  challenge   in the Food  Chain
simulation-based    inquiry-learning  environment,    the   repeated   measures   ANOVA     analyses    did not  indicate
significant differences between the two groups with respect to their cognitive regulation skills.
        After the third challenge, which included a very complex environmental policy making problem, the
repeated measures ANOVA analyses revealed a significant main effect in favor of the MP group, F (2, 209) =
0.47, p >   .05, !2  = .025,  which   supported  the hypothesis    that, in highly  complex  learning   tasks, providing
students with annotated system dynamic model progression was more effective than question prompting as a
cognitive regulation scaffold in simulation-based inquiry learning.

Effects on Complex Learning
The second research question asked whether scaffolding with question prompts and system dynamics model
progressions affect students' learning about complex systems, hence, complex problem-solving skill acquisition.
        As seen in Table 2, participants in the MP condition had lower mean scores than the participants in the
QP condition in complex learning after the first challenge in the Food Chain simulation-based inquiry-learning
environment,   during    which  no  cognitive  regulation   scaffolding   was  provided  by the   system.   However, the
repeated measures ANOVA analyses did not indicate significant differences between the two groups in these
two dependent variables F (2, 209) = 0.47, p > .05, !2 = .025. The results indicated that the participants in both
conditions were comparably on equal basis when they completed the first challenge.
        However, the repeated measures ANOVA analyses revealed a significant main effect in participants'
cognitive regulation of inquiry learning in both QP and MP conditions between the first challenge, where no
scaffolding were provided by the system, and the second challenge, where participants received either scaffold F
(2, 209) = 42.66, p <    .01, !2 = .53, which supported the hypothesis that students, when received either cognitive
regulation   scaffold, would    learn about   complex  systems     better   than when   cognitive  regulation   was  not
scaffolded   during  inquiry  learning. Nevertheless,  after    completing  the  second  challenge   in the Food  Chain
simulation-based    inquiry-learning  environment,    the   repeated   measures   ANOVA     analyses    did not  indicate
significant differences between the two groups with respect to complex learning.
        For the third challenge, which included a very complex environmental policy making problem, the
repeated measures ANOVA analyses revealed a significant main effect in favor of the MP group, F (2, 209) =
49.86, p <    .01, !2  = .57, which   supported  the hypothesis    that, in highly  complex  learning   tasks, providing
students with annotated system dynamic model progression was more effective than question prompting to
support students' learning of complex ecology systems during simulation-based inquiry learning.

                                                     1123   ·   © ISLS
                                                 ICLS 2010   ·  Volume 1

Discussion
The purpose    of this study   was  to investigate the  effects  of   two  promising    cognitive-regulation  scaffolding
strategies, question prompts and model progression, on ninth-grade biology students' (1) cognitive regulation of
inquiry learning; and (2) learning of a complex system (lake ecosystem) in a simulation-based inquiry-learning
environment. This study also addressed the effect of task complexity on the effectiveness of students' cognitive
regulation and their successful learning in complex systems.
        The results of this study pointed out to a number of important issues. First, the results showed that both
scaffolding  strategies were   effective   in providing    the  cognitive   regulation   support  for   students  using  a
simulation-based   inquiry-learning    system.   However,    as  the  complexity    of  the problem     increased,  model
progression  was   significantly more   effective  than  question-prompts.     This  is consistent   with earlier  studies,
which suggested    the  ineffectiveness of question   prompting   as   a  self-regulation   scaffold in  situations where
students have insufficient prior knowledge, which plays an important role in elaborated learning (Ge & Land,
2003; Greene & Land, 2000; King, 1992).
        This finding also explains some of the mixed results in earlier studies regarding the effect of model
progression on self-regulation. For instance, Quinn and Alessi (1994) performed a study, in which students has
access to a simulation of a spread of a disease within a simulation.      Students had to manipulate two to four input
variables to minimize the value of one of the output variables. Their data revealed that model progression had
no overall positive effect on performance. It should be noted, however, that the domain that was used by Quinn
and Alessi (1994) was quite simple: the variables in the model did not interact. In another study with simulation
of a more complex system (multimeter) Alessi (1995) found that model progression was beneficial for initial
learning and for transfer. Similar positive results were observed in Rieber and Parmley's (1995) study in the
area of Newtonian motion.
        Secondly, the results of this study confirmed the findings from the earlier studies, which suggested
using question-prompts    as a   self-regulation support   early in   the simulation.   For instance, Showalter     (1970)
recommended using question-prompts early in the simulation to focus learner attention to specific aspects of the
simulation. Similar studies by Zietsman and Hewson (1986), Tabak, Smith, Sandoval, and Reiser (1996), White
(1984; 1993) also pointed out to the effectiveness of such strategy in different domains for providing self-
regulation (especially for planning) support during simulation-based inquiry learning.
        Finally, this study provided empirical evidence for the link between cognitive-regulation skills and
learning in complex systems (i.e., complex problem-solving), which was suggested in earlier studies (see Butler
& Winne, 1995; Davis, 2000; Lin & Lehman, 1999; Schraw, 1998). In this regard, there was no significant
difference between    the impact   of  question-prompts    or  model  progression    when   the problem   was  a   simpler
complex  problem.     However,   as the  complexity   of the    problem    increased,  cognitive scaffolding   by   model
progression had significantly higher impact on learning than cognitive scaffolding by question-prompts.
        In future studies, it would be beneficial to investigate the effectiveness of both scaffolding strategies on
far-transfer complex problem-solving tasks. These studies should also factor in individual student differences
and investigate whether there are differences among low-achieving and high-achieving students. Students who
participated in   our study  had   not received   any prior    instruction on   lake  ecosystems;    the  domain-specific
knowledge required for inquiry learning was constantly accessible, on demand, in the Food Chain simulation
environment. Directions included in the system encouraged students to read the domain-specific information
prior to generating hypothesis. Therefore, in this study, an attempt was made to bring individual differences to a
minimum possible level in order to rule out confounding factors that might have affected the results of this
investigation.
        Overall, this study provided empirical evidence to the critical importance of embedding appropriate
feedback strategies in technology-based learning environments (see Cohen, 1985). This corresponds with the
current discussion about the failure of the popular movements of constructivist learning environments adopting
discovery,  problem-based,   experiential,  and   inquiry-based   teaching   approaches     and provide   support   to the
arguments   by  Kirschner   et al.  (2006) and   Mayer   (2004).  We      hope to see   more  studies    investigating the
effectiveness of technology-mediated scaffolding strategies so that an appropriate design theory for instructional
simulations  may   arise. Current   attempts,  though    interesting,  are  necessarily   fragmentary     and incomplete
(Thurman,    1993; de   Jong   &   van  Joolingen, 1998).     Coupled     with such   a theory,  inquiry   learning  with
simulations can fulfill its promise in learning and instruction.

References
Alessi, S. (2000). Building versus using simulations. In J. M. Spector & T. M. Anderson (Eds.), Integrated and
        holistic perspectives on learning, instruction, and technology: Understanding complexity (pp. 175-196).
        Dordrecht: Kluwer Academic Publishers.

                                                    1124   ·   © ISLS
                                              ICLS 2010   ·  Volume 1

Azevedo, R., Guthrie, J. T., & Seibert, D. (2004) The role of self-regulated learning in fostering students'
         conceptual understanding of complex systems with hypermedia. Journal of Educational Computing
         Research, 30, 87­111.
Butler, D. L. & Winne, P. H. (1995). Feedback and self-regulated learning: A theoretical synthesis. Review of
         Educational Research, 65, 245­281.
Cohen, V. B. (1985). A reexamination of feedback in computer-based instruction: Implications for instructional
         design. Educational Technology, 25(1), 33­37.
Davis, E.A., & Linn, M. (2000). Scaffolding students' knowledge integration: Prompts for reflection in KIE.
         International Journal of Science Education, 22(8), 819­837.
De Jong, T. (2006) Scaffolds for computer simulation based scientific discovery learning. In J. Elen & R. E.
         Clark (Eds.), Dealing with complexity in learning environments. London: Elsevier Science Publishers.
de Jong,  T. &   van  Joolingen,  W.  R. (1998).  Scientific  discovery  learning with computer    simulations of
         conceptual domains. Review of Educational Research, 68(2), 179-202.
Dörner,  D. (1996).  The logic of failure: Why  things  go  wrong   and what  can we do to make    them right  (R.
         Kimber & R. Kimber, Trans.). New York: Metropolitan Books.
Dörner, D., & Wearing, A. J. (1995). Complex problem solving: Toward a (computer-simulated) theory. In P. A.
         Frensch & J. Funke (Eds.), Complex problem solving: The European perspective (pp. 65-99). Hillsdale,
         NJ: Lawrence Erlbaum Associates, Inc.
Funke, J. (1991). Solving complex problems: Exploration and control of complex systems. In R. J. Sternberg &
         P. A. Frensch (Eds.), Complex problem solving: Principles and mechanisms (pp. 185-222). Hillsdale,
         NJ: Lawrence Erlbaum Associates, Inc.
Ge, X., & Land, S. M. (2003). Scaffolding students' problem-solving processes in an ill-structured task using
         question prompts and peer interactions. Educational Technology Research and Development, 51(1),
         21­38.
Ge, X., & Land, S. M. (2004). A conceptual framework for scaffolding ill-structured problem-solving processes
         using question  prompts  and  peer interactions.   Educational Technology  Research  and   Development,
         52(2), 5-22.
Greene, B. A., & Land, S. M. (2000). A qualitative analysis of scaffolding use in a resource-based learning
         environment involving with the World Wide Web. Journal of Educational Computing Research, 23(2),
         151­180.
Greeno, J. G. (1989). Situations, mental models, and generative knowledge. In D. Klahr & K. Kotovsky (Eds.),
         Complex information processing (pp. 285-318). Hillsdale, NJ: Erlbaum.
Hogan, K., & Thomas, D. (2001). Cognitive Comparisons of Students' Systems Modeling in Ecology. Journal of
         Science Education and Technology, 10(4), 319-345.
Jonassen, D.  H. (1997).  Instructional  design models    for well-structured and  ill-structured problem-solving
         learning outcomes. Educational Technology: Research and Development, 45(1), 65-95.
King, A. (1992). Facilitating elaborative learning through guided student-generated questioning. Educational
         Psychologist, 27(1), 111­126.
Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during instruction does not work:
         An analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based
         teaching. Educational Psychologist, 41(2), 75-86.
Lavoie, D.R., & Good, R. (1988). The nature and use of predictions skills in a biological computer simulation.
         Journal of Research in Science Teaching, 25, 335-360.
Lin, X. & Lehman, J. (1999). Supporting learning of variable control in a computer based biology environment:
         Effects of prompting college students to reflect on their own thinking. Journal of Research in Science
         Teaching, 36, 837-858.
Linn M. C., Davis E. A., & Bell, P. (2004) Inquiry and technology. In M.C. Linn, E. A. Davis, & P. Bell (Eds.),
         Internet environments for science education (pp. 3­28). Mahwah: Lawrence Erlbaum Associates.
Manlove, S., Lazonder, A. W., & de Jong, T. (2007). Regulative support for collaborative scientific inquiry
         learning. Journal of Computer Assisted Learning, 22, 87-98.
Mayer, R. E. (2004). Should there be a three-strikes rule against pure discovery learning? The case for guided
         methods of instruction. American Psychologist, 59(1), 14-19.
National Research Council (2000). How people learn: Brain, mind, experience, and school. Washington, DC.:
         National Academy Press.
Quinn, J., & Alessi, S. (1994). The effects of simulation complexity and hypothesis generation strategy on
         learning. Journal of Research on Computing in Education, 27, 75-91.
Pintrich P. (2000)  The  role of goal orientation in  self-regulated  learning. M. Boekaerts, P.   Pintrich &  M.
         Zeidner (Eds.), In Handbook of self-regulation (pp. 451­502). Academic Press, London.

                                                 1125   ·   © ISLS
                                                ICLS 2010   ·  Volume 1

Rieber, L.P.,  &   Parmley, M.W.   (1995).   To  teach  or  not  to  teach? Comparing     the use   of computer-based
        simulations in deductive versus inductive approaches to learning with adults in science. Journal of
        Educational Computing Research, 14, 359-374.
Rumelhart, D. E., Smolensky, J. L., McClelland, J. L., & Hinton, G. E. (1986). Schemata and sequential thought
        processes in PDP models. In D. E. Rumelhart, J. L. McClelland & the PDP Research Group (Eds.),
        Parallel distributed processing: Explorations in the microstructure of cognition (pp. 7-57). Cambridge,
        MA: The MIT Press.
Sabelli, N. H. (2006). Complexity, technology, science, and education. Journal of the Learning Sciences, 15, 5­
        9.
Schauble, L., Glaser, R., Raghavan, K., & Reiner, M. (1991). Causal models and experimentation strategies in
        scientific reasoning. The Journal of the Learning Sciences, 1, 201-239.
Schraw G. (1998) Promoting general metacognitive awareness. Instructional Science 26, 113­125.
Seel, N. M. (2001). Epistemology, situated cognition, and mental models: Like a bridge over troubled water.
        Instructional Science, 29(4-5), 403-427.
Seel, N. M. (2006). Mental models and complex problem solving. In J. Elen & R. E. Clark (Eds.), Handling
        complexity in learning environments: Theory and research (pp. 43-66). Elseiver, Ltd.
Showalter, V.M. (1970). Conducting science investigations using computer simulated experiments. The Science
        Teacher, 37, 46-50.
Shute,  V. J., &   Glaser, R.  (1990). A large-scale   evaluation    of an  intelligent discovery   world: Smithtown.
        Interactive Learning Environments, 1, 51-77.
Simmons, P. E., & Lunetta, V. N. (1993). Problem-solving behaviors during a genetics computer simulation:
        beyond the expert/novice dichotomy. Journal of Research in Science Teaching, 30, 153-173.
Tabak, I., Smith, B. K., Sandoval, W. A., & Reiser, B. J. (1996). Combining general and domain-specific
        strategic support for biological inquiry. In C. Frasson, G. Gauthier & A. Lesgold (Eds.), Intelligent
        Tutoring Systems (pp. 288-297). Berlin, Germany: Springer-Verlag.
Thurman,   R.   A.  (1993).   Instructional simulation    from   a   cognitive psychology     viewpoint.   Educational
        Technology Research & Development, 41, 75-89.
Van Joolingen W. R., De Jong T., Lazonder A.W., Savelsbergh, E. & Manlove, S. (2005) Co-Lab: Research and
        development    of   an on-line  learning  environment      for  collaborative   scientific discovery learning.
        Computers in Human Behavior, 21, 671­688.
White, B. Y. (1984). Designing computer games to help physics students understand Newton's laws of motion.
        Cognition and Instruction, 1, 69-108.
White, B.Y., & Frederiksen, J.R. (1990). Causal model progressions as a foundation for intelligent learning
        environments. Artificial Intelligence, 42, 99-157.
White, B.Y. (1993). ThinkerTools: Causal models, conceptual change, and science education. Cognition and
        Instruction, 10, 1-100.
Wilensky, U., & Reisman, K. (2006). Thinking Like a Wolf, a Sheep, or a Firefly: Learning Biology Through
        Constructing and Testing Computational Theories-An Embodied Modeling Approach. Cognition &
        Instruction, 24(2), 171-209.
Young, R. (1983). Surrogates and mappings: Two kinds of conceptual models for interactive devices. In: D.
        Gentner, & A. L. Stevens (Eds), Mental models (pp. 35-52). Hillsdale, NJ: Erlbaum.
Zietsman,  A.  I., &  Hewson,   P. W.    (1986). Effect     of instruction  using microcomputers       simulations and
        conceptual change strategies on science learning. Journal of Research in Science Teaching, 23, 27-39.

                                                   1126   ·   © ISLS
