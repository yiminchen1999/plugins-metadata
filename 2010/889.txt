                                                    ICLS 2010     Volume 1

          Using collaborative activity as a means to explore student
                                   performance and understanding
                                             Marcela Borge, John M. Carroll
Pennsylvania State University, College of Information Sciences and Technology, University Park, PA 16802-6823
                                    Email: mborge@psu.edu, jmcarroll@ist.psu.edu

          Abstract:    This  paper  presents data   from  the  beginning    process of  restructuring   a usability-
          engineering   course to  incorporate aspects   of  collaborative-process    theory  with   required course
          content. Students' collaborative processes were evaluated at the end of the semester in order to
          identify key areas that instructors would need to support better in future iterations of the course.
          Overall students were skilled communicators, but lacked ability in areas of planning, productivity
          and critical  evaluation. A  closer  examination     was   made   of difficulties that students had    with
          critical evaluation and trade-off analysis processes since these were key aspects of the course.
          Findings from analyses were used to suggest ways that the course could be modified in the future
          to better prepare students to address these important concepts in usability engineering.

Introduction
As more emphasis in technology and science classes gets placed on getting students to do well on product-based
outcomes, less emphasis is placed on getting students to understand and engage in important learning processes.
This is  extremely     problematic as researchers   and  field practitioners   in science   education  have   emphasized   the
growing need for and increasing lack of students' abilities to understand and engage in critical processes associated
with collaborative intellectual-activity: the ability to communicate effectively, listen and build on ideas, negotiate
ideas, and engage in scientific argumentation (Barron, 2003; Driver, Newton, & Osborne, 2000; Felder, Woods,
Stice, & Rugarcia, 2000). Such processes, though identified as critical in educational and work contexts, have been
identified as a weakness among American students (SCANS, 1991). The attention focused on collaborative skills
has  led many     schools to increase  the  number   of  team-based    projects,  and collaborative   activities  they require
students to participate in. A fundamental problem with this response is that it assumes that students already possess
the cognitive and social skills necessary to engage in effective collaborative endeavors and that these competencies
will naturally emerge and develop through exposure to collaborative learning environments; this is wrong. Research
has shown that individuals are often frustrated by group interactions, waste time (Salomon & Globerson, 1989), or
fall victim to a variety of other problems that lead to dysfunctional group-processes (Webb & Palincsar, 1996). This
suggests that students need more explicit guidance with collaborative endeavors than is currently typical.
          Gaps in knowledge about what constitutes effective collaborative interactions and/or the ability to apply
these concepts in practice are particularly problematic at the college level where individual grades may depend on a
team's performance. In The College of Information Sciences and Technology (IST), where our participants have
spent the last four years, students are expected to work in teams for much of their academic careers, depending on
each other    for grades  based  on   their collaborative   efforts. Given   that dysfunctional    interactions   among  team
members   can     lead to many   negative   outcomes    such   as lower  performance,   alienation,   failure,  and even   the
likelihood that students will drop out of the group, the course, even the major (Barron, 2003; Hogan, 1999; Light
1990; Rosser, 1998), it stands to reason that over a four-year time span such trends could lead to considerable
problems for these students, and many faculty regularly see these problems play out day-to-day. In order to diminish
these problems it is necessary to develop students' collaborative competence. Toward this aim, the authors are
currently undertaking     a restructuring   process for a   usability-engineering   course   in  the College   of Information
Sciences and Technology (IST). The goal for the course is to help students understand and apply important concepts
of  usability engineering    and   collaborative-competence    theory   simultaneously.     However,   time   constraints, the
breadth of domain knowledge, and student push back make this an uphill battle. This paper represents the beginning
of our journey down Dillenbourg's long road (Dillenbourg & Traum, 1999): moving beyond distribution of the task
to a shared understanding of it. The work presented here is a specific report based on one small strand of our
research that encompasses a much broader data set.

The usability-engineering course
Usability engineering is defined as the concepts, processes, and practices engaged in the process of developing
software systems and applications to ensure that they serve their intended users effectively. To a considerable extent,
it mirrors and complements software engineering. Our approach to teaching usability engineering relies on user

                                                        889     ISLS
                                                    ICLS 2010     Volume 1

interaction  scenarios  as  the primary   context  for  analyzing   requirements,   describing  specifications, envisioning
designs, developing rationales, creating various sorts of prototypes, and evaluating systems and applications. Our
usability-engineering course is organized around a semester-long system development project. The course works its
way through a curriculum organized by the flows of the system development process, and student teams define,
implement, and test their projects respectively throughout the course. This course has been previously successful
with computer and information science students. In 2002, a textbook based on the course was published (Rosson &
Carroll, 2002; see http://ist413.ist.psu.edu/ for most recent syllabus and class-by-class activities).
      When we designed the usability-engineering course, we carefully devised a list of six main learning goals for
our students and came up with various ways that we could implement and assess them (Carroll & Borge, 2007). In
this paper we will evaluate students with regards to one of these goals: the ability to build knowledge base through
collaborative discussions and refine necessary skills in collaborative groups.

Four core collaborative capacities
Four core capacities within the realm of collaborative learning were identified from educational research literature
(Borge, 2007) and unpacked for students. These capacities were intended to embody an organized theory of goals
and objectives   students  would  need    to meet  in  order  to  work  in an effective  collaborative environment.   These
capacities were introduced and incorporated into the students' collaborative work. These capacities were originally
created as an attempt to structure group processes to address the many factors that can contribute to group-process
problems (Borge, 2007; White & Frederiksen, 2000).          Originally presented as roles, they were also a useful way to
simplify a complex endeavor, such as collaborative interaction, into more manageable sub-skills that students could
learn about, demonstrate to each other, and improve upon through practice (Brown & Palincsar, 1985).
         The four collaborative capacities are planning manager, communication manager, critical evaluation &
negotiation manager, and productivity manager. Physical tools and activities were developed to support the learning
and use of the roles/capacities. One of the primary tools was a guide for students that detailed each capacity: the
major    objectives, problems    students    may  need   to   prevent/correct,   and strategies  that  they   could  use  to
prevent/correct problems (for examples of the guides see http://ist413.ist.psu.edu/, week-to-week activities, calendar,
2/4, under collaborative roles). These Guides were originally developed for children ages 10-14 years of age, but
extensively  modified   for use  in  this course  for   use with  usability contexts  and college   age learners.   We  used
feedback  from   instructors and  students    to tailor the   language, problems,   and  strategies to specifically  address
problems faced by this population of students (Borge & White, 2009). Examples of common problems for this
population include: improper use of time during team meetings, lack of critical evaluation, and lack of accessibility
to collaborative products    in real-time.   The updated    guides  included  information  to prevent  and/or   correct such
problems.

Research methods, objectives, and class context
The research objectives for this project were to fundamentally improve how team activities were implemented,
supported and structured in a usability-engineering course so as to improve students' ability to both understand and
apply important concepts. Since application of concepts is a critical aspect of the research we will focus on the
analyses  of  an in-class   activity requiring   the  application  of  usability engineering  concepts  through     particular
collaborative interactions. We used assessments of students' collaborative processes from these videos and compared
them  to  their  final project  grades  to   see whether    students'   process  performance  was   correlated  to  students'
performance on their team project.
         Participants were students enrolled in a senior level usability-engineering course (IST413). Students were
divided  into eight  teams   of five-to-six  students:  two   teams  composed    of  two females and    four males   and six
composed of all males. The teams worked together throughout the semester on collaborative in-class activities,
collaborative homeworks, and a semester-long usability-engineering project. All students were required to learn
about effective collaborative-interactions, and practice applying goals and strategies during these activities. Students
learned about the four capacities, their objectives, and ways of promoting effective collaborative interactions (i.e.,
setting process goals, and using strategies to prevent or correct team-problems). Goals and activities related to the
four capacities were incorporated with 13 in-class activities during which students were expected to work together
and turn in a deliverable (an activity worksheet). The activities were either tied to the content covered that week, or
to the students semester long project. The roles were a required part of the course since activities, collaborative
homework assignments, and even some quizzes required students to practice and understand the roles, their goals
and strategies. Students' overall collaborative capacities were assessed from video of teams engaged in an in-class
activity in week 13 of the course (for details of the activity see http://ist413.ist.psu.edu/, week-to-week, calendar,
activity 13  on  2/28). This    videotaped   activity was   a   brainstorming and   decision-making   session  that required

                                                        890      ISLS
                                                  ICLS 2010    Volume 1

students to use  and apply concepts  they had     learned   throughout   the semester: write design  scenarios, identify
potential user problems, suggest ways to resolve problems with documentation/help design, evaluate trade-offs of
proposed designs, etc. We assessed students on their collaborative interactions by using a scoring rubric developed
by Borge, 2009 (See Table 1 for the four capacities and objectives that students were assessed on). This rubric is
directly connected with the collaborative-capacity guides as the objectives they assess are those proposed in the
guides. Out of eight teams seven consented to be included in our research; these teams were used in analyses. There
were five objectives for each of four capacities, for a total of 40 possible points. However, one objective in the
planning capacity, and one objective in the productivity capacity were not applicable to this activity (those appear
shaded in Table 1). Therefore, there was a total of 38 possible points on this measure.

         Table 1: The four capacities and their objectives. Objectives not assessed appear shaded.

         Students were familiar with the rubrics, as they had used them to assess themselves at the beggining of the
semester. When students completed a video-taped activity, at the begining of the semester, they were asked to watch
their videos, score their team with these rubrics, and identify areas that seemed particularly problematic for their
team. The scoring system was as follows: 0 = does not fuilfill objective to any degree, 1= has some ability to fulfill
objective, but still somewhat problematic, 2= fulfills objective perfectly, could be used as ideal example for other
students. Once they identified problem areas they were asked to look through the guides and select strategies to
prevent/correct the problems they identified. This activity was used as a means to introduce the roles, their guides,
and ways to apply the concepts they presented. It was also a means to familiarize students with the rubric we would
use to assess their interactions at the end of the semester. At the end of the semester, students were told that we
would assess them with the same rubrics they used at the beginning of the semester and we repeated the same
procedures as during the first recorded activity.

Reliability of the scoring rubric
The results we report are derived from overall scores determined by the first author. however, we assessed the
overall reliability of the scoring rubric by training a graduate student to be a second rater. The second rater was
presented with the same materials and guides the students were given. Twenty percent of the data set was doubly
coded with the following results: Kappa=.69, and r=0.88, n=140, p< .001. Transcripts were not available at this
point in the analysis, so the coding was based directly on video clips. The two coders matched exactly on 78.8 % of
the items. Most of disagreement regarded teams for which the first author identified modest critical evaluation (score
of 1), but the graduate-student coder felt there was no critical evaluation demonstrated (score -9, n/a).

Transcription of video
Each of the seven 30-minute videos was transcribed following a similar format. Each new speaker utterance and or
behavior   was numbered,  denoting a new   "turn".    A   "turn"  ended  when   a different  speaker introduced  a new
utterance. The  students were given  pseudo-names     in    the transcript.  Parentheses were used   to label nonverbal
gestures and events (i.e., leaving the group, making faces, using hands, etc). Brackets were used for codes, time

                                                     891      ISLS
                                                  ICLS 2010    Volume 1

stamps, and notes relevant to the analyses, but not found in the video itself. These transcripts were utilized when the
author scored each team's collaborative interactions with the given rubric, and for microanalysis of processes.

Results

Teams' collaborative interactions: overall findings
Out of 38 possible points in the assessment, the average score across all seven teams was 50% and the median score
was 47.3%. This means that all     teams scored in the "average" range of performance, getting mainly scores of "1" on
the different  objectives in  the  assessment. Scores of    "2" were   far less common,    except in  the communication
category. Collaborative performance was not significantly correlated to measures of individual performance (i.e.,
exam scores, class participation, or overall course grade). However, the collaborative assessment was correlated to
performance on measures assessed at the team level: where students worked together on one product and shared the
same  grade   (r=.77, p<.05).  This   suggests that improving     collaborative    performance  could  lead to  improved
performance on deliverables created and assessed at the team level.
        When    we  examined   how    teams performed     on each  individual   capacity,  we found  that they  performed
significantly better on one of them. There was a significant difference between how students performed on the
communication capacity (M=.70, SD=.18) and how they performed on the assessment as a whole (M=.50, SD=.13),
t(6)=6.8, p<.001). Communication was the students' strongest area reflecting strengths in listening for understanding
and building on each other's ideas. In fact we saw very little evidence of parallel talk, where one utterance, or turn, is
followed by a different and unrelated utterance.

              Table 2. Descriptive statistics of teams' overall performance on capacities:
              crit = critical evaluation, com = communication, pro = productivity, and plan = planning.

        There   were  no  significant differences between    the  rest of  the  capacities and overall performance  with
students scoring in the average range for all three capacities. Particular aspects of the planning capacity were not
evident in student behavior. For example, students largely accomplished the activity problem by problem, with no
thought as to what previous information might be helpful, what the goals of the activity were, or how they would
ensure that they achieved those goals. Some aspects of       productivity were also missing from students' interactions.
Even though they were quite good at staying on task and progressing through items quickly, they still did not
perform above average in this capacity. Most of their low scores were due to the fact that they 1) did not show
evidence that they had set methods for determining and recording progess and 2) failed to evaluate work quality.
Many aspects of this activity depended on students pulling on previous assignments and, for the most part, team
members  were    unaware   of who   was responsible   for which    tasks.  Failure to evaluate work   quality  was also a
problematic and reaccuring theme. Most teams only cared that the answer was "good enough", not that it was the
best possible answer. In fact, student teams actually used the term, "good enough" an average of two times during
the activity.
        The last capacity we would like to discuss is the critical evaluation/ negotiation capacity. Given that many
parts of the activity explicitly asked students to engage in evaluation processes, we would have expected to see
higher than average scores in this area, but were disappointed to find that only three teams were able to score any 2s
in this area, and only one team fulfilled the trade-off analysis objective, demonstrating the ability to effectively
apply the concept of trade-off analysis (a process we will discuss later). All other teams scored 0s on this objective.

Specific problems with the capacity for critical evaluation/ negotiation
Thre was no    need for   conflict managment   or negotiation   of differing   points of  view as students  did not seem
particularly invested in critiquing or evaluating ideas. Students were very respectful to fellow teammates and there
were no instances of intimidation or personal attacks during collaborative activities. Unfortunately, students seemed
to be crossing off mental checklists rather than engaging in deeper forms of analyses or argumentation. In fact,
students accepted or built upon ideas without stopping to evaluate or challenge those ideas or ask team members to
provide rationale for suggestions. Questions, such as, "why do you think we should that" simply were not evident in

                                                      892     ISLS
                                                          ICLS 2010     Volume 1

the team interactions. For the most part, students seemed to lack the ability to step back from problems and evaluate
their suggestions as objects of thought. One of the best examples of students' inability to take a step back and think
about a problem and its solutions critically came from one of the teams presented below. This particular team was
creating a website for a photo club. In this example, the team is working on the second question in the activity.
       Question 2. Discuss the design scenarios you have prototyped. Choose 2 that you think are most
       likely to raise problems for learning or use, i.e. where help or other documentation might be
       required for at least some users. Summarize each scenario briefly, and indicate why you think it
       might be a candidate for help or training.

The team (M1, M2, M3, M4, and M5) tries to come up with problems that a user could experience when trying to
use their website. Since their website is for a photo club, M3, suggests that they might have problems uploading a
picture file. M4 asks M1 to log into the website in order to check for restrictions. M1 tries to log on, but does not
remember the password. The rest of the team continues to talk about the potential problem (see Table 3).

Table 3: Team talk as they work to find a solution to a problem in the activity.

         1.     00: 20: 24.22 M3: How many picture files can you use?
         2.     00: 20: 25.14 M2: It was on the website...
         3.     00: 20: 25.25 M1: Yeah where's that at...
         4.     00: 20: 33.10 M2: Yeah go to the homepage... yeah, there you go...
         5.     00: 20: 35.28 M1: What's the password, anybody...
         6.     00: 20: 40.28 M3: Um the other Mike has that, but he's not here... I don't know the password of---
         7.     00: 20: 47.13 M4: Why couldn't he make it something easy like one two three four...
         8.     00: 20: 49.15 M3: He made it something easy but it was just like---
         9.     00: 20: 51.14 M1: Penn State?
         10.    00: 20: 52.19 M2: Try it.
         11.    00: 20: 54.29 M4: Did he actually send you the---
         12.    00: 20: 56.12 M1: No, I just remembered it was something stupid.
         13.    00: 20: 57.22 M2: He said it out loud to us.
         14.    00: 20: 58.02 M4: Oh... I can't remember!
         15.    00: 21: 01.19 M2: Security question?
         16.    00: 21: 06.23 M3: Um?... Calendar help training solution, we could just provide like a document that showed you
                              how to do stuff.
         17.    00: 21: 26.06 M1: Yeah I'm not sure this is the right, its asking for yahoo ID.
         18.    00: 21: 32.05 M3: Even if just [inaudible] the document that shows them where the help is for Flickr.
         19.    00: 21: 36.17 M2: That's gonna have a [Inaudible].
         20.    00: 21: 37.25 M4: Flickr for beginners?
         21.    00: 21: 41.11 M2: Or what happens if you go to the picture page and click on a picture?
         22.    00: 21: 50.22 M1: It's asking to, ah, open up.
         23.    00: 21: 58.24 M2: [Inaudible] pictures so...
         24.    00: 22: 03.05 M1: Um.
         25.    00: 22: 07.29 M2: Oh wait.
         26.    00: 22: 11.21 M3: What is it?!

The team continues discussing possible problems with uploading pictures in turns 8-11. Meanwhile, M1 still cannot
remember    the password and asks         the  team  for     help  (turn 12). In    the next     11 turns the      team tries to guess the
password to no avail. Nonetheless, in turns 25-26, the team marches on, trying to come up with another idea for a
problem that a user could experience when trying to use their website, while M1 continues to try to log on. The team
proceeded in this manner for two minutes. Unable to resolve the problem and log on, one of the students suggested
they simply summarize their previous scenario about a user experiencing problems uploading pictures, but this time
add an FAQ or search page to help the user resolve the problem.
       What happened here?    It seems these students were so engaged in trying to verify a proposed solution to the
problem, that they completely ignored the most obvious one: the problem they were experiencing at the moment of
not being able to login because no one could remember the password. Forgetting the password is a common problem
that many sites simply resolve by allowing you to reset the password. However, for this particular website that
solution would not work because all the users share the same password. This was a trade-off that came up in their
design, they wanted all the users to be able to link Google Calendars and other aspects of the website to Facebook,
but the only way that could work was for all of them to share the same password. This solution gave them want they
wanted in a feature without them having to write programming code. However, it had drawbacks as their transcript
demonstrated. Identifying these types of trade-offs and thinking ahead for possible solutions was the main goal of
the activity, but sadly, not one this team could accomplish. This team was not alone.

                                                               893     ISLS
                                                      ICLS 2010        Volume 1

Trade-off analysis
Trade-off   analysis is a  fairly complicated      critical-evaluation        process        that is  also a   core   concept  of   usability
engineering. It involves weighing design goals (i.e., the needs of your users, ease of use, added features, etc.) with
constraints  and resources   of   the design     team     (i.e., cost   to build   and       maintain,   programming      demands,    client
demands, etc.) in order to make the most reasonable decisions. Trade-offs, by definition imply that there is no
correct answer, just the best one given design goals and resources; it is a balancing of factors that cannot all be
attained simultaneously.   Given   its centrality   to     this  course    it was  one       of   the critical evaluation  objectives   that
students were assessed for as part of the collaborative assessment. This was the only objective in the assessment in
which teams received zeroes across the board. Only one team, Team 6, was able to effectively apply the concepts
inherent to  trade-off  analysis. This   team    will   be  used     to illustrate       the difference  between      how the  majority  of
students engaged in the trade-off analysis process, with what was expected.
         In question three of the activity, the students were asked to "Analyze trade-offs associated with your design
idea. Consider a variety of issues, e.g. cost to build or maintain, generalization to other scenarios, reactions by users,
and so on". Six out of seven teams simply responded by coming up with a list of pros and cons. The teams did not
justify their design or why it was the best possible solution. They also did not weigh factors involved in their
decision making. Instead, team discussions were more akin to identifying possible problems. The following example
is representative of the pattern of interactions that stemmed from teams trying to engage in the process of trade-off
analysis. In this team, members (M1, M2, M3, M4, and M5) have just completed question two (presented with the
example in Table 3) and just now reading question three (see Table 4).
Table 4. Team talk representative of the majority of teams as they worked to resolve Question 3.
         1.    00:18:34.05  M1: can you read the question
         2.    00:18:34.07  M2: It just says analyze the trade offs, consider a variety of situations
         3.    00:18:43.29  M1: [inaudible] ... you can say there is almost no way... if it happens it's going to come down to the
                                officers of DDF [Dance Dance Fanatics] to do something
         4.    00:18:59.02  M3: Yeah
         5.    00:18:59.03  M4: Yeah
         6.    00:19:03.00  M3: Like  when you sign-up you should    be required to like... accept---
         7.    00:19:08.13  M1: Like one of those... agreements [inaudible]
         8.    00:19:10.29  M3: yeah, that you always accept and just push ok.
         9.    00:19:17.18  M4: Something with the calendar page
         10.   00:19:22.09  M1: Um
         11.   00:19:45.16  M2: Members using calendar may f[**] it up.
         12.   00:19:52.00  M1: Ha, yes!
         13.   00:19:54.21  M3: He stole the words outta my mouth!
         14.   00:20:01.21  M2: They... what's a better word than that?!
         15.   00:20:07.22  M1: May   not fill in all the field, or something like that.
         16.   00:20:08.05  M2: May   hinder---
         17.   00:20:10.12  M1: --- So their, maybe events on there but they may not have like time or place or combination of
                                something or description just be like meeting
         18.   00:20:21.17  M2: Like the date and time.
         19.   00:20:22.05  M1: Might just be ambiguous as to what the person    was talking about.
         20.   00:20:26.10  M4: The date is like required.
         21.   00:20:28.12  M1: Yeah its like, you'll know the day, but its like it might just be completely ambiguous and just say
                                meeting... [inaudible].
         22.   00:20:50.28  M2: I think we're good with three.

         In turns 3, 9, and 11, Students present two problems with their design: 1) there is no simple way to monitor
content, and 2) users may leave important fields in the calendar page blank. In turns four through nine, students
unpack the first problem, and in turns 10-21 students unpack and discuss the second. As students speak a member
writes down these problems on their team paper. There is no discussion about alternative designs or justification for
why they have chosen this design even though these problems are present. In turn 22 they simply state that they have
sufficiently addressed question three and move on to question 4. This was the predominant pattern of talk that
ensued from 6 teams during this trade-off analysis portion of the activity: identify problem, agree/build on idea,
identify another problem, agree/build on idea, write ideas down and move on. Only one team demonstrated any
ability to engage in trade-off analysis, Team 6 (see Table 5 below).

Table 5: Team talk of students demonstrating ability to apply trade-off analysis concepts.

         1.    00: 26: 27.22 M2: Some others, trade-offs.
         2.    00: 26: 32.11 M4: Cost to build or maintain, photo gallery is like it takes time for people to like

                                                            894       ISLS
                                                       ICLS 2010        Volume 1

          3.     00: 26: 41.18 M3: Update their galleries because they have to send them to the administrator and then the
                                administrator has to go and
          4.     00: 26: 44.13 M1: Actually (mumble)
          5.     00: 26: 47.28 M3: Yeah
          6.     00: 26: 47.20 M4: (Mumble) You got a hundred members and all these photos just gonna be taking up space and all
                                that stuff.
          7.     00: 26: 55.07 M1: All, well if we get a hundred members [inaudible---all talking at once].
          8.     00: 26: 59.24 M4: I don't know how restrictive the university is with all that though.

          Conversation continues...
          14.    00: 27: 29.03 M4: That's why it has to be each user can just login and upload their own photos
          15.    00: 27: 36.09 M2: Cost to build and maintain.
          16.    00: 27: 38.08 M4: So that's one of our trade offs I guess, wouldn't it be? Cause ours is like that, and after--- and not
                                so much functional I guess, because of our own time to build something like that with our limited
                                resources.

          The patterns of talk are quite different in this team. They begin by identifying a relevant trade-off, cost to
build and maintain (turn 22) and then unpack what the trade-off means as it applies to their project (turns2-8). This
type of discussion continues as students build on each other's ideas. Until finally, in turn 14, an alternative design
idea is proposed that would address their design problems. At which a member reminds them of the trade-off, cost to
build and maintain (turn 15) and another team member summarizes their discussion by stating that this is in fact
their trade-off: they are settling for a website with vey little functionality because they lack the time and resources to
produce anything more complicated. The majority of teams started by identifying problems with their design and
never worked their way up to how they fit into a bigger category of trade-offs or thought about how a trade-off
category would play out with their project. Team 6 started with a trade-off, worked their way down by unpacking
the trade-off as it applied to their system, identified related problems, alternative designs, and then weighed these
designs in order to decide which would work best for their team. They did this by summarizing their discussion and
defending their original design idea with concrete rationale.

Discussion
Findings from this study have helped us to identify problematic concepts for students and provided us with insights
as to  how    to modify the   course    in  future   iterations    to address  these issues. This           course contained    a         group of
seemingly engaged students, who appeared to be collaborating well on the surface. However, when their interactions
were closely examined we were able to see that students were falling considerably short of what could be attained in
terms  of collaborative  competence.        Students  in  this     course  were very   competent          communicators,     but          were still
lacking abilities to critically evaluate their ideas and work quality as well as plan their work and interactions. We
also discovered that students' understanding and practice of the trade-off analysis process is not in keeping with the
major goals of the course. Our novice usability-engineering students seem to be making mistakes analogous to
Schoenfeld's     (1989) novice mathematicians; they were picking solutions to problems and not stopping to evaluate if
they were the best ones. Thus we should employ teaching methods similar to those used by Schoenfeld to improve
students' critical   thinking skills    as  well as   their    ability  to step   back from  a           problem   and think    about          their
performance as an object of thought. Our findings also suggest that we can improve team deliverables (the project
score) by improving process learning (their collaborative score).
          We are currently developing process-learning activities that can better meet the needs of our students for
the next iteration of this course. For example, expert thinking will be made more visible for our students (Collins,
Brown, & Newman, 1991) in three ways: 1) the instructor will model trade-off analysis for students with the help of
colleagues and focus more on explaining his rationale behind usability decisions by weighing options, 2) students
will be shown contrasting examples (such as those presented in Tables 4 and 5) of students engaging in trade-off
analysis, and 3) students and instructors will work together on collaborative problem solving activities; the aim
being to move to a more discussion centered methodology where evaluation of processes are common topics. We
are also trying to minimize the "good enough" tendencies in our students by forcing teams to defend their design
ideas during presentations to their peers, and also require peer evaluations of project deliverables throughout the
semester. Thus motivating them to focus more on their work-quality and their decision-making processes.

Closing thoughts: the importance of professional collaboration and course evaluation
It is essential  for educators  to ensure     that   the collaborative     learning  opportunities          we  provide    for our         students
consistently accomplish what we intended them to do: give students opportunities to learn from and challenge the
ideas of others, and give them an authentic context to practice applying the core concepts and techniques of a
domain. Collaborations between domain experts and experts in the science of learning, such as that between the

                                                               895     ISLS
                                                 ICLS 2010      Volume 1

authors of  this paper, are  crucial to accomplishing    this goal.  This  type of  collaboration  is necessary   help set
appropriate learning goals for students, identify learning goals that present particular problems for students, and
implement changes to a curriculum to address these difficulties. Only through these types of collaborations and
course evaluations we can fully develop students' learning potential and provide them with richer, more meaningful
courses that prepare them to function in an increasingly team-oriented workplace. We do not pretend that these types
of domain and process learning fusions do not come with extreme challenges, as we faced many. Among them was a
huge push back from students who genuinely felt as though they already knew all there was to know about effective
collaborative practice; our finding demonstrate that this is not the case. We contend that the only way to get past
students' misconceptions of collaborative practice are to make collaborative goals and processes part of an ongoing
conversation between instructors and students that begins in elementary school and continues on throughout their
educational and professional careers.

References
Barron, B. (2003). When Smart Groups Fail. Journal of the Learning Sciences, 12(3), 307-359.
Borge,  M.  (2007). Regulating   social interactions: developing    a functional theory    of collaboration. Dissertation
        Abstracts International, 241.
Borge, M., & White, B. Y. (2009). Scaffolding collaborative processes with managerial roles. Paper presented at the
        American Educational Research Association, San Diego, CA.
Brown, A. L., & Palincsar, A. M. (1985). Reciprocal teaching of comprehension strategies : a natural history of one
        program    for  enhancing learning.  Champaign,     Ill. Cambridge,    Mass.: University  of  Illinois at Urbana-
        Champaign; Bolt Beranek and Newman Inc.
Carroll, J., & Borge, M. (2007). Articulating case-based learning outcomes
        and assessment. International Journal of Teaching and Case Studies, 1, 33-49.
Collins, A., Brown, J. S., & Newman, S. E. (1991). Cognitive Apprenticeship: Making Things Visible. American
        Educator: The Professional Journal of the American Federation of Teachers, 15(3), 6-11,38-46.
Dillenbourg, P.,  & Traum,    D. (1999). The   long road    from  a  shared  screen   to a shared  understanding.   Paper
        presented   at  the  Proceedings  of   the  Computer     Support   for  Collaborative   Learning  (CSCL)     1999
        Conference, Palo Alto, California.
Driver, R., Newton, P., & Osborne, J. (2000). Establishing the norms of scientific argumentation in classrooms.
        Science Education, 84, 287-312.
Felder, R., Woods, D., Stice, J., & Rugarcia, A. (2000). The future of engineering education II. Teaching methods
        that work. Chem. Engr. Education, 34(1), 26-39.
Hogan, K. (1999). Sociocognitive Roles in Science Group Discourse. International Journal of Science Education,
        21(8), 855-882.
Light, R. (1990). Explorations with students and faculty about teaching, learning, and student life.
        Cambridge, Mass.: Harvard University Press.
Rosson, M. B., & Carroll, J. M. (2002). Usability engineering: scenario-based development of human-computer
        interaction. San Francisco, CA: Morgan Kaufmann Publishers Inc.
Rosser, S. (1998). Group work in science, engineering, and mathematics: Consequences of ignoring
        gender and race. College Teaching, 46(3), 82-88.
SCANS    (1991).  What  work   requires  of schools:  A  SCANS     report  for  America    2000. Washington,    DC:  U.S.
        Department of Labor.
Schoenfeld, A. H. (1987). What's all the fuss about metacognition. In A. H. Schoenfeld (Ed.), Cognitive science and
        mathematics education (pp. 189-215). Hillsdale, NJ: Lawrence Erlbaum Associates.
Salomon, G., & Globerson, T. (1989). When teams do not function the way they ought to. International
        Journal of Educational Research, 13, 89-99.
Webb, N., & Palincsar, A. S. (1996). Group processes in the classroom. In D. C. Berliner & R. C. Calfee
        (Eds.), Handbook of Educational Psychology (pp. 841-873). New York: Simon & Schuster
         Macmillan.
White,  B., Shimoda,    T., &  Frederiksen, J. (2000).   Facilitating  Students' Inquiry   Learning   and Metacognitive
        Development Through Modifiable Software Advisers. In S. Lajoie (Ed.), Computers as Cognitive Tools,
        Volume Two: No More Walls (pp. 97-132). Mahwah, NJ: Erlbaum.!!

Acknowledgments
The work reported here was partially supported by The National Science Foundation (IIS/REC 0735440) and by
the Edward Frymoyer Chaired Professorship.!!

                                                     896      ISLS
