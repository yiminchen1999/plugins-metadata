                                                  ICLS 2010   ·  Volume 1

                         An Overview of CSCL Methodologies
              Heisawn Jeong, Hallym University, Chuncheon, Korea (South), heis@hallym.ac.kr
      Cindy E. Hmelo-Silver, Rutgers University, New Brunswick, US, cindy.hmelo-silver@gse.rutgers.edu

         Abstract: This study surveyed empirical CSCL papers published in seven leading journals of
         the field during 2005-2007 and analyzed methodologies of a random sample of 33 studies.
         The dominant features of CSCL research methodologies were descriptive studies in classroom
         settings with  self-report/questionnaire     that were   quantitatively  analyzed.   Yet,   studies   were
         quite eclectic in their research method, adopting a variety of research design, data sources and
         analysis  methods,  leading   to cross-over    or  hybrid methodologies.       Studies   of collaborative
         learning were carried out frequently both in experimental and descriptive studies in lab and
         classroom settings. Diverse data sources were collected and analyzed both quantitatively and
         qualitatively. New techniques for analyzing collaborative processes were also emerging. Still,
         it appears that  the  field  needs  to    be more    principled  in  its applications    of design-based
         research. In  addition, the  field is in  need    of more diverse    and  systematic    ways    to analyze
         qualitative data in order to take a full advantage of the rich data afforded in CSCL research.

Introduction
There has been an increasing interest in collaborative knowledge construction for some time. With the advent of
information  technology,    collaboration   is also   increasingly    mediated    by various     technologies.    Computer-
Supported Collaborative Learning (CSCL) is an interdisciplinary research field that are concerned with how
people can learn together with the help of computers (Myake, 2006; Stahl, Koschmann, & Suthers, 2006). In
any field including    CSCL,  methodology      is of  utmost  importance     in advancing    the  field. Methodologies   are
constrained by the current level of understanding and available analysis tools in the field, and yet advancements
in methodology    can  also open up   new   conceptual     spaces and  allow    researchers  to  examine    phenomena   and
questions that were not previously possible. Reflecting its interdisciplinary mission, diverse methodologies co-
exist in  CSCL.    Traditional   methods    initially  developed    in    the   context of   studying     individuals  (e.g.,
questionnaires, study of individual problem solving protocols) have been actively used by CSCL researchers.
CSCL researchers also have reached out to methodologies in fields such as linguistics and anthropologies. The
infusion of these methodologies has helped to advance the field a great deal. Currently, however, it is difficult to
understand  what  the  current  state of  research    methodologies    is in  CSCL.  Numerous      methodologies    co-exist
without a clear understanding of what they are and how they are related to each other.
         Recently, several meta-analyses have been conducted in related fields of CSCL. Hrastinski and Keller
(2007)  examined   research  approaches     (e.g., empirical    versus conceptual    studies,  quantitative    or qualitative
methods, etc.) of papers published in four leading journals of educational technology during 2000-2004 period.
They found that about two thirds of the studies were empirical investigations, about half of which (51%) used
quantitative methods, 25% used qualitative methods, and 24% used mixed methodologies (Hrastinski & Keller,
2007). Hew, Kale, and Kim (2007) also conducted similar meta-analysis on research topics and methodologies
in the field of instructional technology based on publications in three journals also during 2000-2004. They
focused on empirical articles and found that descriptive and/or correlational studies were the dominant research
methods in educational technology, being used in more than half of the studies published. Methodologies varied
somewhat depending on research topics; experimental method was more commonly used in studying the topic
of psychology of learning and teaching, while correlational method was most frequently used with the topic of
research and evaluation methodology. In addition, the studies most commonly collected survey/questionnaire
data in higher education and K-12 settings (Hew et al., 2007).
         Distinctions  such  as  quantitative  and    qualitative methodologies      have    long existed   in many   fields.
Quantitative methods were typically associated with experimental and survey studies where numerical data are
collected. Analysis aimed to uncover `general' trends that are true to the population at large, which lead to the
focus on large sample sizes and quantitative analyses using inferential statistics. Qualitative methods, on the
other hand, were associated with descriptive research such as case or ethnographic studies where qualitative data
such as video-tapes, verbal transcripts, and artifacts are collected. Analysis aimed at uncovering `meaningful'
patterns and in-depth look at a small set of data was emphasized. While specific methodological traditions in
qualitative research may focus on different aspects of the data and may take different routes to arrive at their
interpretations,  data were   mostly   analyzed    qualitatively.  Such    a    distinction, however,     no   longer seems
appropriate. As qualitative methods become more popular, more attempts are made to triangulate data collection
and analyses. Traditional descriptions of qualitative or quantitative research no longer adequately describe some
of the research being carried out in the field. There is a need to develop a more sophisticated understanding of
what quantitative and qualitative methods mean and how they can be better integrated to inform CSCL research
at large. In addition, we need to examine and evaluate how the current CSCL methodology is addressing some

                                                      921   ·  © ISLS
                                                   ICLS 2010  ·    Volume 1

of the unique challenges that CSCL faces. With its emphasis on social construction of knowledge mediated
through technological tools, CSCL has been in need of methodologies to study learning as it happens in the
process of interacting with learning partners and tools. Interaction is a complex phenomenon where traditional
methodologies    developed     to study   individual    learning   are  not  necessarily  readily  applicable.   Moreover,
interaction in CSCL environments is unique in that there exist diverse forms of interaction such as distributed as
well as   face-to-face  interaction   and  that  interaction    is  often mediated    by  technological   tools  (Lehtinen,
Hakkarainen,   Lipponen,  Rahikainen,      &   Muukkonen,     1999).    Because    thinking often develops   and   is made
observable through dialogues and discourse, research efforts have been directed to the analyses of dialogue and
discourse  both  in face-to-face   distributed  interaction.  Analysis    methods    such as  content  analysis, discourse
analysis,  and  conversation      analysis have    thus    become      commonplace     in   CSCL  literature.   Combining
methodologies from different research traditions has been useful for revealing ambiguities and contradictions,
which in turn have led to new conceptual developments. However, it have increased the risk of confusion in
different epistemological perspectives (Naidu & Jarvela, 2006). Individual researchers are confused about how
different methodologies can be used to inform their research. It is equally unclear whether CSCL methodologies
have been adequately addressing the key issues of CSCL.
          In response to these concerns, this study attempted to provide an overview of current methodologies
used in empirical investigations of CSCL. We first carried out a comprehensive examination of empirical CSCL
studies, analyzing research methodology at the following four dimensions: (1) research designs (e.g., descriptive
or experimental design), (2) study settings (e.g., classroom or laboratory), (3) data collected (e.g., survey data,
log data, or synchronous or asynchronous text data), and (4) analyses carried out with the data (e.g., content
analysis, social  network    analysis,  multi-level    analyses).     Based  on this  analysis,  we also   examined    how
collaboration is studied in CSCL research.

Methods

Journal Selection
International Journal of Computer Supported Collaborative Learning (ijCSCL) is the flagship journal of the
CSCL community. In addition to ijCSCL, there are numerous other research journals where CSCL research is
published.   We  asked   the leaders  of   CSCL    community,      CSCL     committee  of   ICLS  and  the  editorial board
members of ijCSCL to nominate five leading journals of the field other than ijCSCL. Based on the responses
from 16 community leaders, the following seven journals were selected: (1) ijCSCL, (2) Journal of the Learning
Sciences,  (3) Learning   and     Instruction, (4)  Computers      and  Education,   (5)  Journal of   Computer    Assisted
Learning,  (6)  International  Journal     of  Artificial Intelligence    in Education,   and (7) Computers     in  Human
Behavior. They are all peer-reviewed journals published by well-known publishers with international authorship
and readership.

Paper Selection
From the papers published in the seven journals, we identified empirical CSCL papers to be used for the content
meta-analysis.  By  empirical,    we  mean    that the  study   collected   and analyzed  primary   data.  Secondary   data
analysis, simulated results, theoretical papers and meta-analyses were not included. The data may have been
collected as part of a larger project, but the analysis and finding had to be new. Papers that described the design
process were included if they reported on empirical data. By CSCL research, we mean studies where students
learned collaboratively using computers or other technological tools. Learning needed to be collaborative, but it
does not mean that all phases of learning needed to be collaborative. As long as parts of the learning process
involve interaction (e.g., collaborative discussion after individual study), it was considered collaborative. We
focused on small group peer collaboration, that is, collaboration among learners who are similar in knowledge
and status. This means that studies that examined student-teacher interaction or whole class discussions were
not included unless they included small group peer collaboration. The applied technologies do not necessarily
have to be collaboration technology such as e-mails or discussion boards, but need to be specific. Interaction
with computerized    agents  were   included   if  it involved   learning.   Studies about   motivation  or attitudes  were
included  if they   were studied   in  relation to    learning.  In   addition  to empirical  CSCL   papers,  we   included
methodological papers that addressed various methodological issues related to CSCL (e.g., introduction of new
methods such as Social Network Analysis, development of specific rating schemes).
          Excluding non-research articles (e.g., editorials, book reviews, or obituaries), 868 articles published in
the 2005-2007 period were screened, which means three years of publication (two years of publication in the
case of ijCSCL).    The  total numbers     of  selected   papers   is currently 175,  indicating  that 20%  of  the   papers
published in the seven journals during 2005-2007 were empirical or methodological investigations of CSCL.

                                                       922  ·   © ISLS
                                                  ICLS 2010    ·  Volume 1

Content Analyses
The following aspects were coded for each paper: (1) Design, (2) Study setting, (3) Data, (4) Analysis, and (5)
Interaction. In this paper, we report on the analyses carried out on a random sample of 33 papers. This set
constituted 21% of the total empirical CSCL paper (and 19% of the total CSCL papers including methodology
papers;  percentages     did not come    out even due   to the   ongoing   nature of  paper selection   process)  and  were
marked with * in the reference list. A combination of inductive and deductive approaches was used to develop
coding categories for each code. Coding categories were initially generated top-down (e.g., using categories
drawn from the submission descriptors of the 2005 CSCL conference) and then later refined bottom-up through
multiple iterations of coding. Jeong and Hmelo-Silver (2010) reported on a different set of analyses (with the
exception of the collaboration coding) carried out on the same sample. Specific codes for each category are
described below.

Research design
Research design refers to whether the study is (1) Experimental, (2) Descriptive, or (3) Design-based method.
Experimental    design   refers  to  studies  where  variables    are  manipulated    and   was  further  divided   into (a)
randomized, (b) quasi-experimental, and (c) pre-post design. Descriptive studies do not manipulate variable and
assign  study  participants   to different conditions.  They     study the variables   and  phenomena    as  is and include
studies such as case studies, surveys, and ethnographic investigations. Design-based method refers to research
strategy  where  CSCL       designs  and  interventions   are theoretically-driven    and are   refined progressively  over
several  iterations. In  order   to be  coded as  design-based    method,   the study   not only  needs   to  design CSCL
systems or environments, but also the design needs to be theoretically grounded, instantiated in specific contexts,
studied and refined iteratively as part of a bigger design-based research (Barab & Squire, 2004; Brown, 1992;
Collins, Joseph, & Bielaczyc, 2004). Once a study was coded as design-based method, the design of individual
iterations which can be either experimental and/or descriptive was not coded separately.

Study settings
Study  setting  refers   to the  contexts in which  the   research  was    conducted,  that is, whether   the research   was
carried  out in (1)  Laboratory,    (2) Classroom,  or    (3) Other  settings  (e.g., informal  learning  environments   or
workplaces).   Laboratory     means  that  learning occurred     in lab or  lab-like  controlled settings   where  the task
occurred isolated by itself outside the context of classroom or other authentic learning situations. Classroom
setting means that learning occurred as part of classroom/curricular activities, which could involve not only
physical classroom but also other settings (e.g., field trips).

Data
Data refer to the types of data collected in the study and was coded into (1) Process, (2) Outcome, and (3)
Miscellaneous data types. Process data were further divided into (a) text-asynchronous, (b) text-synchronous, (c)
video, (d) audio, (e) log data, and (f) other. When papers reported on both video and audio data (e.g., Li et al.,
2006), they were coded only as the video data unless they were analyzed separately. Outcome data were further
divided into (a) multiple-choice questions, (b) open-ended questions, (c) expert ratings (e.g., experts' rating of
students' work), (d) artifacts (e.g., contents of multi-media whiteboard), and (e) other (e.g., final course grades).
Outcome data can be collected at the beginning of a study (e.g., pre-test) as well as at the end of a course.
Miscellaneous   data   include   (a) self-reports/questionnaires    (e.g., demographic    information,   affects, perceived
acceptance, etc.), (b) interview and/or focus groups, (c) field notes or observations, (d) individual difference
measures (e.g., IQ, learning styles), and (3) other (e.g., course registration information).

Analysis methods
Analysis methods refer to the kinds of analyses carried out on the data. There were two general categories: (1)
Quantitative  and    (2) Qualitative.  Quantitative analyses     were  further coded   into the  following    sub-types: (a)
simple descriptive or quantitative (e.g., simple frequencies and means, as well as simple quantitative analysis of
qualitative data such as coding numbers of words in messages or scoring an open-ended answers), (b) code and
count, (c) inferential statistics, (d) causal or multi-level modeling, (e) sequential analyses, (f) social network
analysis, and   (h)  other   miscellaneous   quantitative analysis  (e.g., comparison   with    simulated data).  Code   and
count refers to what is often called "quantitative content analyses" (de Wever, Schellens, Valcke, & van Keer,
2006). Qualitative analysis refers to analysis that analyzes data in a qualitative manner. Qualitative analyses
were further coded into: (a) (qualitative) content analysis, (b) conversational analysis, (c) grounded theory, (d)
interaction analysis, (e) loosely defined.

Collaboration
Collaboration refers to the types of student-to-student interaction and consisted of four categories: (1) Face-to-
face, (2) Distributed synchronous, (3) Distributed asynchronous, and (4) Other. Face-to-face collaboration refers

                                                      923   ·   © ISLS
                                                 ICLS 2010   ·  Volume 1

to co-located and synchronous interaction. Distributed collaboration refers to collaboration among distributed
learners and   can    be either synchronous   (e.g., through    chat)  or   asynchronous     (e.g., through e-mails   and
discussion boards). Other collaboration refers to miscellaneous interaction such as non-verbal interaction (e.g.,
interaction  through     argumentation map),     interaction   with computer     agents,  or indirect  interaction   (e.g.,
collaborative filtering or data sharing).

Methodological features of the studies were generally coded according to the authors' definitions. If authors call
their design as `experiment' then we coded them as such. In unclear cases where the authors did not explicitly
state information relevant to the coding categories (e.g., no mention of whether the study was carried out in lab
or in classrooms), we relied on contextual information presented in the paper. For example when a study did not
specify  its setting, if `recruited' students performed    a   stand-alone  activity with no  connection    to classroom
activities, its setting was coded as laboratory (e.g., Mercier & Frederiksen, 2007). Similarly, when a study did
not specify data and only stated that the number of words in asynchronous notes was analyzed, it was assumed
that asynchronous text messages were collected as data (e.g., Hewitt & Brett, 2007). In a few controversial cases,
we followed a more conventional approach so that `near synchronous' interaction was coded as asynchronous
interaction and that an `experiment' without any mention of treatment conditions were coded as a descriptive
study. Multiple coding was allowed when more than one coding categories were applicable. For example, if the
study compared     different  versions  of  a course   that    was  implemented    in  face-to-face,  synchronous,    and
asynchronous learning situations (e.g., Kitsantas & Chow, 2007), all different types of collaboration were coded.
Reliability check was completed in all code (except for the collaboration code, which is still in progress) by
having two coders independently code the set while discussing unclear cases. Cohen's kappa values between the
two coders were all above .75 in all coding categories.

Results

Methodological Features of CSCL Research
The analyses indicated that the most prevalent CSCL research design is descriptive design: 58% of the studies
were descriptive, 33% were experimental, and 9% were design-based. Of the experimental studies, 45% used
randomized experiments, 27% quasi-experimental method, and 27% single-group pre-post design.
         As for the settings of CSCL research, studies were mostly carried out in the classroom: 82% of the
studies were carried out in the classroom, 12% were in the laboratory, and 6% in other settings (e.g., workplaces
and informal settings). Classroom studies are typically associated with descriptive design, whereas laboratory
studies  are typically   associated  with experimental   design.    While   this indeed  was  a case   with most   of the
descriptive  studies  being  carried out  either in classrooms     or other settings,  there were   a sizable  number  of
studies that `crossed over'. The majority (70%) of the classroom studies was either descriptive or design-based
studies, 30% were experimental studies. Classroom experiments were typically quasi-experiments using intact
classrooms or single group pre- and post-test comparisons (e.g., Cho, Gay, Davidson, & Ingraffea, 2007), but
there were a few cases of randomized classroom experiments as well. Random assignments in classroom setting
were carried out by assigning different treatment groups to separate laboratory sessions (e.g., Vizcaino, 2005) or
by online groupings (e.g., Cho & Schunn, 2007).
         As for data collection, 73% of the CSCL studies collected process data, 61% of the studies collected
outcome data, and 70% collected other data. The majority of the studies collected more than one data types:
Only 15% of the studies collected a single data type, and the rest of the studies collected multiple types of data.
Studies on average collected 3.0 data in a given study with some studies collecting up to six different data. Of
the process data, log data were the most frequently collected (42%), asynchronous text-messages were the next
(30%), followed by synchronous text (15%), video-(12%) and audio-data (9%). Of the outcome data, and other
types was most popular (24%), followed by artifacts (21%), open-ended (18%), multiple-choice questions (12%)
and open-ended     questions  (12%),  and  experts'  rating  (9%).    Other outcome    types include  data  such  as final
grades,  students' self-ratings and   evaluations.  Of the   miscellaneous   data  types, self-report/questionnaire   was
most popular (61%), followed by interviews (21%), field notes (12%), individual difference (6%), and other
(3%). A quite diverse set of data is being collected in CSCL research. Additional sources of data can provide
more  information     about the questions. It appears  that  CSCL     researchers  are actively attempting    to examine
learning from multiple perspectives.
         As for the analysis method, some form of quantitative analyses was carried in all studies. The most
common forms were simple descriptive and inferential statistics (58% each), followed by code and count (52%);
Causal or multi-level modeling was used in 6% of the articles, and social network analysis and other analysis
were used in 3% of the studies each. A notable trend was a development of new quantitative methodologies
such as social network analyses (Cho et al., 2007) and multi-level analyses (de Wever et al., 2006) that aimed to
analyze collaborative processes quantitatively. Qualitative analyses were also used quite frequently, being used
in 42% of the studies. However, the methods in general were not well described or attributed to one of the more

                                                     924  ·  © ISLS
                                                  ICLS 2010   ·  Volume 1

specific genre  of    qualitative  research such  as   grounded    theory  (Glaser   &  Straus,   1967)    or conversational
analysis (Goodwin & Heritage, 1990). The majority of the studies (93%) adopted the "loosely defined" method
in which data was qualitatively described with some examples.

Studying Collaboration in CSCL Research
How is collaboration studied in CSCL research? In terms of the types of collaboration investigated in CSCL
research, 36% of the CSCL studies examined face-to-face collaboration, 36% synchronous, 48% asynchronous,
and 12% other types of collaboration. The majority of the studies (70%) implemented and studied only one type
of  collaboration, but   there were  studies that    implemented   either  two   (24%)  or  three (6%)     different types of
collaboration. It was due to the fact that CSCL environments often allow students to interact with each other not
only asynchronously through e-mails, but also synchronously through chats (e.g., Yang & Liu, 2007) and also
because studies compared different types of collaboration in a given study (e.g., Kitsantas & Chow, 2007).
         How were these various forms of collaboration studied in CSCL research? Comparisons of the studies
that examined   different  collaboration    types (see  Table   1) indicated    that while  face-to-face   and synchronous
collaboration are studied equally likely with experimental and descriptive design, asynchronous collaboration
were mostly studied with descriptive design. Distributed collaborations, both synchronous and asynchronous,
were  studied  exclusively   in classroom   settings,  less likely  to collect   outcome    data and carry    out qualitative
analyses. Other types of collaboration were more likely to be studied in experiments in laboratory setting with
quantitative  analysis.   Further  analyses  on   the sub-type   of data   showed     that, not  surprisingly,   face-to-face
collaboration studies collected either video or audio data, whereas synchronous and asynchronous collaboration
studies  collected synchronous     and asynchronous     text  data, respectively.    Studies of  other collaboration    types
mainly collected log data, but also collected video or other types of process data along with the log data. While
quantitative analyses were used across all collaboration types, qualitative analyses were more frequently used to
study  face-to-face    collaboration  than  distributed    collaboration.  A    number   of  factors  such    as  theoretical
commitments, research questions, and available study settings influenced the choice of research methodologies.
More analyses are needed, but it appears that the choice of CSCL research methodology is also dependent on
the type of collaboration examined.

Table 1: Studying     collaboration in CSCL research
                Research Method                       Settings                          Data                     Analyses
              Exp         Des       DB       Lab       Class       Other     Proc       Out       Misc        Quant     Qual
   F2F        38%        50%       13%       25%        75%         0%       88%        100%      63%         100%      63%
 Synch        40%         40%      20%       0%        100%         0%       80%        40%       80%         100%      40%
Asynch        14%         71%      14%       0%        100%         0%       86%        71%       57%         100%      43%
 Other        100%        0%        0%       67%        33%         0%       67%        67%       33%         100%        0%
Notes. F2F refers to face-to-face, Synch refers synchronous, and asynch refers to asynchronous collaboration. Exp refers to
experimental, Des refers to descriptive, and DB refers to design method. Proc refers to process, Out refers to outcome data.
Quant refers to quantitative and Qual refers to qualitative analyses. Note that there were studies that investigated more than
one type of collaboration. In order to isolate research methodologies uniquely associated with different collaboration types,
studies that investigated only one form of collaboration were included in the comparisons: There were eight studies that
exclusively   studied face-to-face collaboration, five  such  studies  for synchronous      collaboration, seven  studies  for
asynchronous collaboration, and three studies for other types of collaboration.

         Study of collaborative interaction in CSCL research is often accompanied by an accumulation of large
amount   of   qualitative data  such as video-data     and  discussion   board   messages.    Such  qualitative   data  were
traditionally analyzed qualitatively most of the time using methods such as discourse analysis (Brown & Yule,
1983;  Gee,   2005)   or  grounded  theory  (Straus   & Glaser,    1967).  How     does recent   CSCL   empirical    research
approach the analyses of these qualitative data? With this question in mind, we examined how the six types of
qualitative data coded in this study--video audio, text-synchronous, text-asynchronous, open-ended outcome,
and artifacts--were analyzed in recent CSCL research (see Table 2). Additionally, we also examined how log
data were analyzed in CSCL research. An ever increasing amount of log data is collected in CSCL research, and
yet  there is only a   scant understanding   as   to how   they can  be  used   in the  context  of studying   collaborative
learning. The results indicated that the majority of the qualitative data was quantified. The quantification was
mostly done using procedures such as code and count or scoring, but "simple descriptive" measures were also
directly calculated in some cases, using quantitative indices of the qualitative data (e.g., word counts in notes).
While most of the process data were quantified with code and count procedure (i.e., coding the presence of
certain code and counting its frequency), open-ended outcome data was analyzed with scoring procedure (i.e.,
assigning a score using a criterion). In addition, qualitative data, especially video-data and synchronous and

                                                       925  ·  © ISLS
                                                ICLS 2010   ·  Volume 1

asynchronous   text  data were   also often  analyzed   `qualitatively', but specific  kinds   of  qualitative analysis
methods  were  seldom     used. The   qualitative analyses    were mostly   `loosely defined'   in that they   provided
qualitative descriptions of the data along with examples without committing to specific qualitative methods. As
for the log data, not surprisingly, they were analyzed quantitatively, but the range of log data used for analysis
was quite diverse. Studies examined time to post messages, length or duration of given threads, pages read,
number of files in the folders, or participation rates. Lastly, the results also indicated that CSCL research may be
good at collecting multiple types of data, but are not always good at analyzing them. Of the 50 pieces of data
examined, one fifth (19%) was either unanalyzed or lacked sufficient detail as to how they were analyzed.

Table 2: Analyses of qualitative (& log) data in CSCL research
                        SimpledescriptiveScoring          Code &count    QualitativeanalysisUnanalyzed     Unspecified
     Video                0%                0%              75%             75%                25%             0%
     Audio                0%                0%              67%              0%                33%             0%
     Text-synchronous     0%                0%              60%             40%                20%             20%
     Text-asynchronous    10%               0%              70%             30%                0%              10%
 Open-ended               29%             71%                0%              0%                14%             0%
    Artifacts             14%               0%              29%             14%                29%             14%
    Log data              93%               0%               0%              0%                7%              0%

Discussion
The  analyses of this   study showed   that the   typical methodology    of CSCL     research  was descriptive  studies
carried out in classroom settings with the most frequently collected data being self-report/questionnaire data
analyzed quantitatively. While these emerge as dominant features of CSCL methodologies, CSCL methodology
is far from monolithic and can be characterized as multi-method. Studies often collected multitudes of data and
also used both quantitative and qualitative analyses. As a result, research methodologies of a given study often
do not fit the traditional quantitative and qualitative divide and/or experimental and descriptive divide. This
seems to  be  driven both  by   the need to  understand   CSCL     from multiple perspectives   (Hmelo-Silver,   2003;
Suthers, 2006)  and  also  with  the  infusion from  different  methodological     traditions. Learning, especially in
CSCL   contexts, is   a   complex   phenomenon,     understanding    of  which   requires  multiple   approaches   and
perspectives  ranging from    individual learner, interactive  processes, tool  mediation, teacher   roles, to general
cultures. In order to achieve a richer and integrated understanding of learning, CSCL research is increasingly
required to examine learning from multiple perspectives, using multiple data sources and analyses methods in
the contexts of specific research questions and settings.
         In spite of the dynamic developments in the field, it appears that there are a number of areas where
more sophisticated methodological understanding and practices are needed. One is with what researchers called
as design-based research. Although we did not carry out systematic analyses, it was clear in the coding process
that researchers often did not distinguish design as a research goal from design as research methodology. Even
in the case where design research was used to refer to the research method or strategies, its applications were
often name in only. While exceptions exist, studies that reported on multiple iterations and progressive design
refinements were rare. The field needs to be more principled in its applications of design-based research. In
addition, there is a need for the field to develop a larger repertoire of analytical tools to deal with the large
amount   of qualitative data  collected  in CSCL   research.   Large amounts    of online and   offline messages  and
dialogues can be and are collected in many CSCL investigations. The advancement of the field greatly depends
on how well these rich sources of data can be utilized. While existing qualitative methods have their values,
they are not well suited to a large-scale analysis. In addition, epistemological commitments of some of these
methodologies are often in conflict with those from more quantitative traditions emphasizing objectivity and
generality of findings. Although qualitative analyses were used in close to half of the studies examined in this
study, they rarely adopted a specific qualitative method and just described the data with no reference as to how
the conclusions  were   derived. It  might  be the  case  that the  researchers failed to attribute  their analyses to
specific methods even though their analyses belong to that category, but it might also be the case that existing
methods may not adequately describe what researchers are currently doing with qualitative analysis. Qualitative
analyses tend to be less explicit in its procedures and more challenging to learn and execute. Efforts are needed
to systematize existing qualitative analyses procedures so that they are easy to understand and carry out and also

                                                    926   ·  © ISLS
                                              ICLS 2010  ·  Volume 1

to develop new analytical procedures to analyze them.
        Given the small sample size of this study, the results and conclusions from this study should be taken
as tentative at the moment. We aim to obtain more robust findings with additional analyses, but hope that an
overview of CSCL research methodology even in this small scale is informative to researchers both in terms of
understanding current CSCL research methodologies and guiding their future research. We also hope that the
methodological issues raised in this study contribute to establishing a common conceptual framework for CSCL
methodologies and integrating diverse findings from different research traditions.

References
Barab, S. A., & Squire, K. D. (2004). Design-based research: Putting a stake in the ground. Journal of the
        Learning Sciences, 13(1), 1-14.
Berge, O., & Fjuk, A. (2006). Understanding the roles of online meetings in a net-based course. Journal of
        Computer Assisted Learning, 22, 13-23*.
Brown,  A.  L. (1992). Design   experiments:   Theoretical and   methodological  challenges  in  creating  complex
        interventions in classroom settings. The Journal of The Learning Science, 2(2), 141-178.
Brown, G., & Yule, G. (1983). Discourse analysis. Cambridge: Cambridge University press.
Chen,  Z.-H.,  Chou,  C.-Y., Deng,   Y.-C., &    Chan,   T.-W.  (2007).  Active  open   learner models  as  animal
        companions: Motivating children to learn through interacting with My-Pet and Our-Pet. International
        Journal of Artificial Intelligence in Education, 17, 145-167*.
Cho, H., Gay, G., Davidson, B., & Ingraffea, A. (2007). Social networks, communication styles, and learning
        performance in a CSCL community. Computers and Education, 49, 309-329*.
Cho, K., & Schunn, C. (2007). Scaffolded writing and rewriting in the discipline: A web-based reciprocal peer
        review system. Computers and Education, 48, 409-426*.
Collins, A., Joseph, D., & Bielaczyc, K. (2004). Design research: theoretical and methodological issues. Journal
        of the Learning Sciences, 13(1), 15-42.
Cress,  U. (2005). Ambivalent    effect of member     portraits in virtual groups. Journal  of  Computer   Assisted
        Learning, 21, 281-191*.
de Wever, B., Schellens, T., Valcke, M., & van Keer, H. (2006). Content analysis schemes to analyze transcripts
        of online asynchronous discussion groups: A review. Computers and Education, 46, 6-28.
Ellis, R. A., Goodyear, P., Prosser, M., & O'Hara, A. (2006). How and what university students learn through
        online  and  face-to-face discussion:  conceptions,  intentions,   and approaches.   Journal of Computer
        Assisted Learning, 22, 244-256*.
Fuks, H.,  Pimentel, M., &   de Lucena,  C. J. P. (2006).  R-U-Typing-2-Me?     Evolving   a chat tool  to increase
        understanding    in learning activities.  International  Journal   of  Computer-Supported    Collaborative
        Learning, 1, 117-142*.
Gee, J. P. (2005). An introduction to discourse analysis: Theory and method (2nd ed.). New York: Routledge.
Glaser B.G. & Strauss A. (1967). Discovery of trounded theory: Strategies for qualitative research. Hawthorne,
        NJ: Aldine Transaction.
Goodwin, C., & Heritage, J. (1990). Conversation analysis. Annual Review of Anthropology, 19, 283-307.
Hakkinen,  P., &   Jarvela,  S. (2006).  Sharing  and    constructing   perspectives in web-based    conferencing.
        Computers and Education, 47, 433-447*.
Hew, K. F., Kale, U., & Kim, N. (2007). Past research in instructional technology: Results of a content analysis
        of empirical studies published in three prominent instructional technology journals from the year 2000
        through 2004. Journal of Educational Computing Research, 36(3), 269-300.
Hewitt, J. (2005). Toward   an  understanding  of how   threads  die in asynchronous   computer  conferences. The
        journal of the Leaning Sciences, 14(4), 567-589*.
Hewitt, J., & Brett, C. (2007). The relationship between class size and online activity patterns in asynchronous
        computer conferencing environments. Computers and Education, 49, 1258-1271*.
Hmelo-Silver, C. E. (2003). Analyzing collaborative knowledge construction: Multiple methods for integrated
        understanding. Computers and Education, 41, 397-420.
Holliman, R., & Scanlon, E. (2006). Investigating cooperation and collaboration in near synchronous computer
        mediated conference. Computers and Education, 46, 322-335*.
Hrastinski, S., & Keller, C. (2007). An examination of research approaches that underlie research on educational
        technology: A review from 2000 to 2004. Journal of Educational Computing Research, 36(2), 175-190.
Hwang,  W.-Y.,  Chen,  N.-S.,   & Hsu,  R.-L.  (2006).  Development     and evaluation  of multimedia   whiteboard
        system for improving mathematical problem solving. Computers and Education, 46, 105-121*.
Jacobs, N., & McFarlane, A. (2005). Conferences as learning communities: some early lessons in using 'back-
        channel' technologies at an academic conference-distributed intelligence or divided attention? Journal
        of Computer Assisted Learning, 21, 317-329*.
Janssen, J., Erkens, G., Kanselaar, G., & Jaspers, J. (2007). Visualization of participation: does it contribute to

                                                  927  ·  © ISLS
                                               ICLS 2010   ·  Volume 1

         successful computer-supported collaborative learning? Computers and Education, 49, 1037-1065*.
Jeong, H., & Hmelo-Silver, C. E. (2010). Technology use in CSCL: A content meta-analysis. Paper presented at
         the Hawaiian International Conference on System Science.
Kitsantas, A.,  &   Chow,  A. (2007).  College   students'   perceived threat and  preference  for seeking  help in
         traditional, distributed, and distance learning environments. Computers and Education, 48, 383-395*.
Lai, C.-Y., &  Wu,  C.-C.  (2006).  Using  handhelds    in a  jigsaw cooperative  learning  environment. Journal of
         Computer Assisted Learning, 22, 284-297*.
Lee, Y.-J. (2005). VisSearch: A collaborative Web searching environment. Computers and Education, 44, 423-
         439*.
Lehtinen, E., Hakkarainen, K., Lipponen, L., Rahikainen, M., & Muukkonen, H. (1999). Computer-supported
         collaborative learning: A review. Unpublished manuscript.
Li, S. C., Law, N., & Lui, K. F. A. (2006). Cognitive perturbation through dynamic modelling: a pedagogical
         approach to conceptual change in science. Journal of Computer Assisted Learning, 22, 405-422*.
Lim, C. P., & Barnes, S. (2005). A collective case study of the use of ICT in economics courses: A sociocultural
         approach. The Journal of the Learning Sciences, 14(4), 489-526*.
Lopez-Morteo, G., & Lopez, G. (2007). Computer support for learning mathematics: A learning environment
         based on recreational learning objects. Computers and Education, 48, 618-641*.
Mercier, J., & Frederiksen, C. H. (2007). Individual differences in graduate students' help-seeking process in
         using a computer coach in problem-based learning. Learning and Instruction, 17, 184-203*.
Miyake, N. (2006). Computer supported collaborative learning. In R. Andrew & C. Haythornwaite (Eds.), Sage
         handbook of e-learning research. London, UK: Sage.
Naidu, S., & Jarvela, S. (2006). Analyzing CMC for what? Computers and Education, 46, 96-103.
Nicol, D. J., & MacLeod, I. A. (2005). Using a shared workplace and wireless laptops to improve collaborative
         project learning in an engineering design class. Computers and Education, 44, 459-475*.
Redondo, M. A., Bravo, C., Ortega, M., & Verdejo, M. F. (2007). Providing adaptation and guidance for design
         learning   by problem    solving:   The  design    planning   approach  in DomoSim-TPC        environment.
         Computers and Education, 48, 642-657*.
Schellens, T., Van Keer, H., De Wever, B., & Valcke, M. (2007). Scripting by assigning roles: Does it improve
         knowledge    construction  in asynchronous     discussion   groups?  International  Journal of  Computer-
         Supported Collaborative Learning, 2, 225-246*.
Schwabe, G., & Goth, C. (2005). Mobile learning with a mobile game: design and motivational effects. Journal
         of Computer Assisted Learning, 21, 204-216*.
Stahl, G., Koschmann, T., & Suthers, D. D. (2006). Computer-supported collaborative learning: A historical
         perspective.  In R.  K.  Sawyer   (Ed.), Cambridge     handbook   of  the  learning sciences.  New  York:
         Cambridge University Press.
Sung, Y.-T., Chang, K.-E., Chiou, S.-K., & Hou, H.-T. (2005). The design and application of a web-based self-
         and peer-assessment system. Computers and Education, 45, 187-202*.
Suthers, D. D.  (2006).   Technology   affordances  for  intersubjective meaning   making:   A research  agenda  for
         CSCL. Computer-Supported Collaborative Learning, 1, 315-337.
Tseng, S.-C., & Tsai, C.-C. (2007). On-line peer assessment and the role of the peer feedback: A study of high
         school computer course. Computers and Education, 49, 1161-1174*.
Veermans,   V., &   Cesareni, D.   (2005).   The  nature   of the  discourse  in web-based   collaborative  learning
         environments: Case studies from four different countries. Computers and Education, 45, 316-336*.
Vizcaino, A. (2005). A simulated student can improve collaborative learning. International Journal of Artificial
         Intelligence in Education, 15, 3-40*.
Wecker, C., Kohnle, C., & Fischer, F. (2007). Computer literacy and inquiry learning: when geeks learn less.
         Journal of Computer Assisted Learning, 23, 133-144*.
Weinberger,  A.,  Stegmann,   K.,  &   Fischer,  F. (2007).   Knowledge    convergence   in  collaborative learning:
         concepts and assessment. Learning and Instruction, 17, 416-426.
White,  T.  (2006). Code   talk: Student discourse   and   participation with networked    handhelds.  International
         Journal of Computer-Supported Collaborative Learning, 1, 359-382*.
Yang, Z., & Liu, Q. (2007). Research and development of web-based virtual online classroom. Computers and
         Education, 48, 171-184*.

Acknowledgments
This research  was  in part  funded by   the National   Research   Foundation  of  Korea   (NRF) (Grant  No. 2009-
0068919) awarded to the first author and a Rutgers University Faculty Research Council grant to the second
author.

                                                    928  ·  © ISLS
