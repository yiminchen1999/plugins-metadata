     Exploration of Scaffolding in Teachers' Dialogue Analysis
                 Moegi Saito, CoREF, University of Tokyo, saitomoegi@coref.u-tokyo.ac.jp
                   Shinya Iikubo, CoREF, University of Tokyo, iikubo@coref.u-tokyo.ac.jp
                Hajime Shirouzu, CoREF, University of Tokyo, shirouzu@coref.u-tokyo.ac.jp

         Abstract: This  paper explored  how to scaffold    teachers' analysis of  dialogue in  order to
         improve their formative assessments. We implemented a half-day teacher workshop wherein
         the teachers were asked to collaboratively analyze a transcribed dialogue using both a manual
         approach and dialogue analysis tool. An analysis of the teachers' writings and dialogues in the
         workshop revealed that the teachers were able to identify the students' lack of understanding
         and consider its reasons usable in formative assessments.

Introduction
In order to design successful collaborative learning, teachers need to conduct formative assessments. Formative
assessment is often considered to be an assessment during the learning process in a unit in order to modify
teaching to improve student attainment. However in collaborative learning situations students learn on their own,
while the quality of their learning depends much on the design of the lesson. Thus teachers first should focus on
the students' learning process in a lesson from the beginning to the end of the lesson (i.e., "What and how did the
students learn or fail to learn?"). Second, regardless of whether the lesson itself goes well or not, teachers should
generate hypotheses about why   students learn or   fail from the  findings of the learning process   (i.e., "What
elements of the lesson affected learning and how?"). For this paper we designed a half-day workshop in which
we asked teachers the above questions to tie their dialogue analyses to improvement of their formative assessments.
          Why do we need dialogue analysis? Japanese "lesson study" is known to be an effective form of
formative assessment in which teachers observe live classroom lessons and discuss after the observation (Lewis
et al., 2006). Yet, there remains a persistent problem: many teachers lack the practice of focusing on the cognitive
processes. Although the live lesson itself provides rich information about student learning, some teachers tend to
focus on the superficial activities of the students or overly focus on only one student, because of their varying
degree of expertise and intentions. Dialogue analysis is able to help solve this problem as tangible transcribed
dialogue allows digging, surveying   and revisiting from   multiple perspectives. Thus, we  introduced   use   of a
dialogue analyses and support tool.

Method
Table 1 shows the structure of the half-day teacher workshop. All 54 participants came from Japanese schools or
boards of education with differences in the subjects matter and their expertise. The participants were randomly
divided into 14 groups comprising three or four members. In Stage 1, we provided a hard copy of the student
dialogue data of one group and presented three questions: (1) What are the issues the students are focusing on
during their discussions? (2) What do students seem to understand and what don't they? (3) How do students
learn? These questions compose the first question above, supporting participants to carefully comprehend the
whole story of the learning process. In Stage 2, participants freely analyzed the dialogues using a technological
tool named the "Conversation Analyzer" (hereafter CA: Shirouzu et al., 2018). The CA supports the visualization
and analysis of student learning by presenting transcripts which are electronically searchable using keywords. By
providing the CA, we aimed at supporting participants to compare detailed analysis of particular groups with an
overall pattern in the class. Through the teacher workshop program, we expected that the participants would reflect
on the lesson design (the second question above), while considering why students learn or fail.
          The analysis targeted the dialogue data of 15-minute small group discussions in a junior high school
science lesson, "The Mechanics of Exercise." The goal of the lesson was to understand the mechanics of body
movement which includes three points, the sensory nervous system, commands from the brain, and the motor
portion of the nervous system. The participants analyzed the data of three out of eight groups, because of the ease
with which it would be possible to assess student learning and find problems in the lesson design. According to
an analysis of the students' post-test results, none of them were able to write "complete" answers which included
the three points mentioned above. Detecting this lack of understanding became a key issue in the dialogue analysis.

Table 1: Program of the teacher workshop on dialogue analysis

 z   Dialogue Analysis: Stage 1 (Using a hard copy/ Focusing on one group)                       55 min.
 z   Dialogue Analysis: Stage 2 (Using the CA/ Focusing on three groups)                         30 min.

CSCL 2019 Proceedings                                  873                                                   © ISLS
Data analysis and results
We analyzed the participants' worksheets written at the beginning and at the end of the workshop and their
dialogue during the workshop from two perspectives: 1) Did the participants focus not only on the superficial
activities but also on the cognitive processes? 2) Did the participants reflect on the lesson design by connecting
the findings from the analysis with the lesson design? Accordingly, we analyzed the data in two steps:
   Analysis 1: How many groups were able to find the specific points of the students' lack of understanding?
Analysis 2: How many groups were able to infer the reason for the lack of understanding?
           We recorded the conversations of the groups during the entire workshop using IC recorders for the
above analyses. The total number of teachers' utterances was about 23,508, the average of which was 1,679 per
group. Figure 1 shows the results of Analysis 1. All 14 groups made a reference to some specific lack of student
understanding: 10 groups identified the lack of all three targeted points of the lesson, two groups identified two
points and the other two groups identified only one. It seemed more difficult to notice that the students had missed
the element of "Command," indicating that not all of the groups had picked up on the complete difficulty of the
lesson. However, the participants found other various points, indicating that they were able to find an unexpected
insufficiency of understanding. Figure 2 shows the results of Analysis 2. Twelve out of 14 groups referred to the
reason for the lack of understanding, implying     that most  participants considered the reason and     generated
hypotheses. Ten groups referred to more than two reasons. Even though each group referred to a small number of
reasons, we were able to see a rich variety as a whole (see "reasons" written in the bars in Figure 2).

    Figure 1. Lack of understanding referred (n=14)          Figure 2. Reasons for lack of understanding (n=14)

Discussion
By designing a workshop in which teachers collaboratively analyze the dialogue of students both manually and
using a technological tool (CA), we demonstrated that the participants were able to uncover the students' lack of
understanding and its precise points (Analysis 1), and many of them considered the reasons for that lack of
understanding from various perspectives and generated hypotheses (Analysis 2).  Pointing out the problems leads
to generating a hypothesis about the next lesson.
           On the other hand, when focusing on the qualitative aspects of the teachers' discourse, there were still
some remaining issues. For example, the participants were not able to effectively use the CA. By using searchable
dialogue data, some teachers had a tendency to look at trivial matters (i.e., "This student uses the wrong term.")
and lost sight of the big picture of student learning processes, through which we observed that the participants'
lack of sufficient understanding of the subject had a negative influence. In order to support this, we plan to design
a new workshop which includes experiencing the lesson as learners before analysis.

References
Lewis, C., Perry, R., & Murata, A. (2006). How should research contribute to instructional improvement? The
        case of lesson study. Educational Researcher, 35(3), 3-14.
Shirouzu, H., Saito, M., Iikubo, S., Nakayama, T., & Hori, K. (2018). Renovating assessment for the future:
        Design-based implementation research for a learning-in-class monitoring system based on the learning
        sciences. In J. Kay, & R. Luckin (Eds.). Rethinking Learning in the Digital Age: Making the Learning
        Sciences Count, 13th International Conference of the Learning Sciences (ICLS) 2018, Volume 3. London,
        UK: International Society of the Learning Sciences, 1807-1814.

Acknowledgments
This work was supported by JSPS KAKENHI JP17H06107, New Learning Project, and Taketa City BOE.

CSCL 2019 Proceedings                                   874                                                  © ISLS
