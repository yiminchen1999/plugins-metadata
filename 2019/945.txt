 Exploring Students' Self-Assessment on Collaborative Process,
         Calibration, and Metacognition in an Online Discussion
                                             Environment
                                    Yu Xia, Hye Yeon Lee, and Marcela Borge
                               yzx64@psu.edu, hxl90@psu.edu, mbs15@psu.edu
                                        The Pennsylvania State University

         Abstract: Students need to accurately assess their performance on collaborative sense-making
         before employing    strategies and  making   changes   to improve    their collaborative   learning
         processes.  In this study, one  group of  three undergraduate  students    who  took     part in five
         synchronous discussion in an online text-based learning environment was focused on as a case.
         The  patterns  of self-assessment,  their calibration of  understanding    of learning   goals,  and
         inaccurate understandings were analyzed and discussed.

Collaborative skills are gaining prominence as the workplace becomes increasingly dependent on groups to solve
complex, cross-disciplinary problems. According to Stahl (2017), group practices provide group members with
opportunities that individual work cannot provide, where they can share knowledge and develop intersubjective
sense-making. One critical competence that learners need to have to achieve better learning outcome is regulation.
Learners with better self-regulated learning skills perform better academically (Azevedo, 2009). In collaborative
learning activities, groups  with higher  regulation  show  less   conflict and more    productive     processes. Self-
assessment on both content knowledge and collaborative sense-making processes is critical to group success. In
this study, we investigate the co-occurrence of students' self-assessment and their metacognitive reflection and
aim to investigate the interplay between accuracy in individual self-assessments and group calibration as teams
engage in metacognitive reflections over the five sessions in one semester.
         Self-assessment is critical to students' self-regulated learning and also their lifelong learning (Harris &
Brown,   2013). To    improve  accuracy   of self-assessment,   students'   self-calibration of   their   decisions on
performance over time and over different learning goals is needed. However, students' calibration is affected
again by various factors, including students' actual performance level, feedback provided by instructors, and the
degree of their sophistication in epistemological beliefs (Dunning, Heath, & Suls, 2004). We see calibration as a
process  of change   that happens   when students   are engaged   in metacognitive     activities and   adjusting their
understanding of the assessing criteria, and argue that the process of calibration is enhanced when students are
actively engaged in metacognitive activities both individually and as a group. To better understand the nested
relation between self-assessment, calibration, and metacognition, we ask the following questions: 1. How does
the group's average scoring accuracy of collaborative process change over time? 2. How does individual students
calibrate their understanding of criteria over time as they engage in metacognitive activities? 3. What are the most
commonly mistaken criteria?

Methods
The study took place in a 15-week university level introductory online course in College of Information Sciences
and Technology in a university in the Northeast United States in Spring 2016. We selected one team (Team 12)
of three as a case. Same with other teams, team 12 answered questions about course materials before participating
in online synchronous discussions in weeks four, six, eight, ten, and twelve. The online text-based discussion
environment  is called  CREATE      (Collaborative  Regulation,  Enhanced    Analysis,  and  Thinking     Environment
Prototype). After answering reading questions, teams first plan their discussion. When they meet online, they
discuss the course materials for around 90 minutes, move to the self-reflect session where they self-assess the
discussion quality, and close with a group discussion on their individual reflections. Students use the same rubric
for self-assessment with the one used by expert to assess group discussion quality. The rubric is a 5-point rating
scales on six items (i.e. verbal equity, developing joint understanding, joint idea building, exploring alternative
perspective, quality of claims, and constructive discourse; see more in Table 1 in Borge, Ong, & Rosé, 2018).
         To answer the first question, we examined the students' group average scores as compared to expert
scores. For expert scores, six items were averaged to produce an overall quality of collaborative communication
competence.  For  students'  group  average  score,  after averaging  each   individual's  quality     of collaborative
communication competence score, the quality scores from each student were then averaged to present group's
overall average scores. To answer the second and third research questions, we analyzed the data of individual
students self-reflections where they gave self-assessing scores and justification score on the item. Code "Match"

CSCL 2019 Proceedings                                   945                                                      © ISLS
was assigned to students' justification that included direct or indirect explanation of item. Cases when students
provided description of any other item, indicating misunderstanding or confusion about assessment criteria, were
coded "Mismatch." "Other" refers to cases when students provided unrelated or general justification, or the
justification was missing.

Findings
Figure 1 of the radar plots of score changes (Blue: Expert score, Red: Groups' average score) shows an increased
accuracy in terms of the group's self-assessment. However, we need to examine students' self-reflections about
their score assignment to see if team 12's metacognitive reflections show a similar trend.

        Figure 1. Quality of discussion assessed by expert and team 12 from session 1 through session 5.

Analysis of the individual  reflections across five time points,   the percentage  of  Match   justification tend to
improve, from 50% at session 1 to 61.54% at session 5. Percentage for students' Mismatch justification identified
noticeably decreased as time goes by, from 27.78% at session 1 to 7.69% at session 5. However, the percentage
for Other category varied from 21.88% to 36.36%.
         A further focus on Mismatch reflections revealed that when students were asked to assess joint idea
building, students frequently included the quality of claim (19.4%); when assessing alternative perspective, the
most frequently confused criteria was identified to be joint idea building (18.2%); and when assessing verbal
equity, quality of claims were identified in students' justification (11.8%).

Discussion and conclusion
The present study examined    students' self-assessments on   collaborative   processes as   they engaged  in group
discussions, and how the nested calibration and metacognition is related to the accuracy of self-assessment. As
shown in the findings, students improved in accuracy of assessing their discussion quality as their scores were
better aligned with expert scores by session 5. Team 12 also showed calibration in their understanding of the
rubric. The shown calibration in metacognitive activities matches the improved accuracy of the group's self-
assessments, indicating that having students to reflect on their discussions for the self-assessment scores directly
engages students in metacognitive processes, which contributes to both students' metacognitive competence and
self-assessment accuracy.
         Nevertheless, further work  is   needed to  reflect all 12   teams'   calibration  accuracy  over   time on
collaborative sense-making. With data from more groups, we would be in a better position to understand the
complex interplay between calibration and metacognition, and how this interplay is related to self-assessment.
Also, quality of claims are the most frequently confused item, meaning when asked to assess other items, students
included criteria of quality of claims in their justification. This indicates the necessity of follow-up analyses about
students' understanding in association with criteria, which may be expected to give an implication for the design
of learning systems. Providing more explanations or visualizations as scaffolding in the activity that fade as
students are more familiar with the environment is one way to address this problem.

References
Azevedo,   R.  (2009). Theoretical,  conceptual,    methodological,    and    instructional issues in research    on
         metacognition and self-regulated learning: A discussion. Metacognition and Learning, 4, 87-95.
Borge,  M., Ong,  Y.   S.,  &  Rosé,  C.  P. (2018).  Learning     to  monitor  and regulate    collective   thinking
         processes. International Journal of Computer-Supported Collaborative Learning, 13(1), 61-92.
Dunning, D., Heath, C., & Suls, J. M. (2004). Flawed self-assessment: Implications for health, education, and the
         workplace. Psychological Science in the Public Interest, 5(3), 69-106.
Harris, L. R., & Brown,    G.  T. (2013). Opportunities  and  obstacles  to   consider  when   using peer-and  self-
         assessment to improve    student learning: Case   studies into teachers'  implementation.    Teaching    and
         Teacher Education, 36, 101-111.
Stahl, G. (2017). Group practices: A new way of viewing CSCL. International Journal of Computer-Supported
         Collaborative Learning, 12(1), 113-126.

CSCL 2019 Proceedings                                  946                                                    © ISLS
