       The CUBE: A Tangible for Embodied Learning, Balanced
                     Engagement, and Classroom Orchestration
                      Pantelis M. Papadopoulos, Aarhus University, pmpapad@tdm.au.dk

        Abstract:    This classic  interactive demo  paper    presents the   CUBE, a  recently  developed
        Arduino-based tangible that can be used in collocated collaboration for a multitude of research
        and teaching purposes. The CUBE was designed with three instructional needs in mind: (a)
        offer opportunities for embodied learning to the students, (b) monitor and enhance balanced
        engagement in a group, and (c) allow the teacher to monitor and orchestrate a classroom.

Improving collaboration in collocated settings
The CUBE presented in this paper focuses on monitoring, analyzing, and supporting collocated collaboration by
offering opportunities for embodied learning and providing immediate and summative individual/group feedback
on the collaboration   activity and    on each   person's contribution,   to motivate  students towards   balanced
engagement. Via Wi-Fi connectivity and a dashboard, the CUBE can inform the teacher on the state and progress
of collaborating groups, allowing real-time classroom orchestration.
        Embodied learning has recently been established as an important field (Lindgren & Johnson-Glenbegr,
2013). Incorporating movement of the entire body or embodied phenomena as gestures and hand movements, the
embodied learning paradigm suggests that body and environment are linked to cognitive processes (Shapiro,
2010). Tangible interfaces, often with cubic shapes, have been used in embodied learning in several ways (e.g.,
Sapounidis, Demetriadis, Papadopoulos, & Stamovlasis, 2018; Terrenghi, Kranz, Holleis, & Schmidt, 2006), for
example for making interface elements directly available to all participants or to enhance group self-organization
around the manipulation of a single tangible.
        Balanced engagement in collocated collaboration settings is crucial as lower participation is often linked
to poor academic performance (Bachour, Kaplan & Dillenbourg, 2010). By combining microphones and visual
representations of   participants' speaking  time, the Reflect   table  (ibid.) provided  immediate  feedback  to
discussants. Similarly, the Conversation Clock (Bergstrom & Karahalios, 2007) provided visualizations of audio
patterns in a spiral timeline, offering a representation of how much and when each discussant talked during
collaboration.
        Finally, tangibles such as the Lantern (Alavi & Dillenbourg, 2012) and FireFlies2 (Verweij, Bakker, &
Eggen, 2017)    have been  used    for classroom orchestration, focusing   on enhancing  communication    between
students, teachers, and groups.
        These characteristics along with additional ones can be found in the CUBE tangible presented next.

CUBE description
This section presents the basic technical characteristics of the CUBE, along with its main functionalities. Since
the artifact was developed recently, there is no empirical evidence at this point to validate which combination of
the offered affordances would be more effective/efficient in a learning scenario. A series of research activities
involving the CUBE will commence in Spring 2019.

Technical characteristics
Figure 1 presents the first, fully functional, version of the CUBE. The CUBE is an Arduino-based system that
includes screens, monitors, microphones, and a set of sensors and modules. It has a cubic shape, with 11 cm long
edges and weights a bit less than 1 kg (including the batteries). Each side has a square 2.8" in. TFT screen (the
actual screen is bigger, but covered for symmetry). In addition, the screens offer touch capabilities, but this
characteristic has not been used in this prototype. The model (i.e., skeleton and sides) was produced in a standard
FDM  3D  printer,  using  PLA   material.  Improved models    printed  in a  high-end SLS printer have  also been
produced, but not used yet.

CSCL 2019 Proceedings                                    1001                                                © ISLS
                                       Figure 1. The CUBE prototype.

Functionalities
The list below describes the functionalities of the current prototype, while additional functionalities could be
added in near future, either by extending or by combining the current ones. Arguably, it is not expected that all
the affordances of the CUBE will be necessary for the envisioned learning scenarios. For example, rotation
detection might not be important in situations where the CUBE is used as ambient technology.

Display information
The cubic shape of the artifact and the sitting arrangement of a group of students around a table create two
distinctive spaces (Figure 2). Each side screen is visible by only one student (private space), while the top screen
is visible by all, including the teacher (public space). This split between private and public could be useful in a
learning design. For example, feedback can be directed to a specific student or to the whole group. In addition,
the side screens can be used for detailed information, while the top one could be used to denote the state of the
group. It should be mentioned that information targeted to all students should be presented in the side screens,
since the top screen is flat and (text) orientation will be an issue for three of the students. The analysis of the
(visible) screen is 320x320 pixels, which means that it is suitable for short messages and graphs. Longer blocks
of information could be divided into several screen pages (e.g., 1/2, 2/2, etc.). Finally, due to the processing power
of the Arduino, the screens are updated sequentially, while the frame rate is very low. This means that the screens
are not suitable for playing videos, while there is a small lag between screens (30-300 ms, depending on the size
of information presented). However, this lag does not pose an issue, since students can only see the screen in front
of them (and the top) and such lag is not critical in learning scenarios.

                      Top screen (public)

                        Side screen (private)

                          Figure 2. Sitting arrangement and private/public spaces.

Rotation detection
The CUBE can detect rotation on the x- and y-axis. Rotation on the z-axis is considered turning and cannot be
detected accurately by the sensors used. At any given moment, the CUBE can identify the side that is on top and
as such, rotation can be used as a selection action. For example, the screens may show different options, asking
students to make a choice. The students could rotate the CUBE to have their selection on the top. As it is expected
that the students may move around the artifact or hold them in their hands during a selection task, the final option
is confirmed when the CUBE detects that it has stopped moving for three seconds. Then, the side that is on top is
considered the students' choice.

Shaking detection
Because of the physical characteristics of the CUBE, shaking requires both hands. As such, shaking is a more
powerful move than rotation and can be used during significant moments of the activity. For example, shaking
can be used to tell the CUBE to wake up (and start an activity) or to interrupt the ongoing activity. Similarly, it
can be used as a celebratory gesture denoting the successful completion of a task, a strong disagreement between
group members, or the need for teacher's intervention.

CSCL 2019 Proceedings                                 1002                                                    © ISLS
Vibration
The CUBE can vibrate to provide haptic feedback to the students. This functionality can be used to confirm a
selection, draw the attention of the person holding it, or accompany audio/visual feedback. The intensity and
duration of vibration can be modified according to the learning scenario needs to denote different types of haptic
feedback. Vibration, along with rotation and shaking can be used in embodied learning settings.

Audio play
A small speaker inside the CUBE is capable of playing tone sounds and midi files loud enough for a surrounding
group of people to hear clearly. Playing voice files or sounds of higher quality would require a larger and heavier
speaker, along with the need for audio controls (i.e., pause, stop, rewind, etc.). As such, audio is used primarily
as auditory feedback to confirm an action, or draw attention.

Audio recording
The system can record the discussion that occurs around it and saves it in an SD card. The recording could cover
the whole activity or start/stop according to an event triggered by the students (e.g., rotation), the teacher (e.g.,
remote control through a dashboard), the system itself (e.g., pre-defined script conditions). The recording can be
a single file, or several, and can be used after the activity for post hoc analysis of the interaction or become
available to the students as feedback. Functionalities such as rotation and shaking can be used in parallel to the
audio recording to allow the students to self-annotate. For example, during an argumentation task, students can
rotate the CUBE in different directions to mark when a fact or a warrant has been mentioned. Self-annotation
information could later be used along with the audio file of the discussion.

Audio source monitoring
Each side of the CUBE is equipped with a microphone. By comparing the sound received by each microphone,
the system can infer in which side a speaker is sitting. The system cannot understand what a student is saying,
since the analysis and monitoring of the audio source are based only the volume of the sound. This function is
similar to the two tables cited earlier (Bachour, Kaplan, & Dillenbourg, 2010; Bergstrom & Karahalios, 2007),
with an added benefit that the CUBE is portable and can be used in more settings. The ability to monitor the audio
source can be used to support balance engagement during a discussion.

    (a) Feedback during discussion    (b) Feedback after discussion

             Figure 3. Feedback that appears in the side screens during and after peer discussion.

       The system can provide both immediate and summative feedback, focusing on the length of a person's
talk and not on its content. Figure 3 shows some of the feedback metrics the prototype can offer during and after
the discussion. So, during the discussion (Figure 3, a), the students can see how much each student talked (%).
Each color represents a different student, with the top screen colored in the color of the person that spoke the
most. This graph is updated every five seconds. After the discussion (Figure 3, b), additional metadata on students'
engagement are presented. First, the timeline shows the order and length of each person's speaking time, while
gaps in the timeline denote periods of silence. In the same screen, the students can also see how many times each
person talked. In the second post-discussion feedback screen, the students can see aggregated feedback on their
participation in terms of actual time and percentage. The Balanced/Discontinuous/Unbalanced indication is based
on the coefficient of variation (Lovie, 2005) for the group and it is an example of additional metrics that could be
used as feedback.
       After a CUBE rotation, the audio source monitoring will start a new session, unless provisions are taken
in the learning scenario so that the rotated CUBE can link previous and current sitting arrangements.

CSCL 2019 Proceedings                                 1003                                                  © ISLS
Connectivity
As mentioned earlier, the CUBE is equipped with a Wi-Fi module allowing it to communicate with the server,
and through that with other CUBEs and with the teacher's dashboard, thus supporting classroom orchestration.
Because of the technical limitations of the Arduino, the wireless connection is established periodically every few
seconds and data from and to the CUBE are transmitted in short bursts. This is the reason why the CUBE cannot
transmit the audio of the discussion in real-time to the server, acting also as a fly-on-the-wall for the teacher.
However, this limitation is not crucial for the envisioned learning scenarios and the few seconds (<2") delay in
updating the teacher's dashboard is not expected to raise significant issues for the participants.

Script editor
During the testing period of the CUBE, collaboration scripts are coded directly in the Arduino IDE. However, an
online script editor is also under development to allow non-technical teacher/researchers to design their own
learning scenarios with the CUBE.

Activity monitor
Currently, monitoring a test activity with the CUBE occurs by observing the server's database. The activity
monitor, which is also under development, refers to the dashboard the teacher/researcher will be able to use in the
classroom during the runtime of a CUBE activity. Apart from monitoring, the dashboard will allow the teacher to
intervene. For example, the teacher may choose to pause an activity in one CUBE or send a message to the screen
of the least engaged student in another CUBE.

Conclusions
The functionalities and robustness of the CUBE have to be evaluated in research activities. Its design is based on
three main pillars: embodied learning, balanced engagement, and classroom orchestration. However, additional
uses and theoretical underpinnings may be identified by the reader. This paper serves as a first introduction of the
CUBE to the CSCL audience, in search of constructive feedback and future collaborations.

References
Alavi, H. S., & Dillenbourg, P. (2012). An ambient awareness tool for supporting supervised collaborative
         problem solving. IEEE Transactions on Learning Technologies, 5(3), 264-274.
Bachour, K., Kaplan, F., & Dillenbourg, P. (2010). An Interactive Table for Supporting Participation Balance in
         Face-to-Face Collaborative Learning. IEEE Transactions on Learning Technologies, 3(3), 203-213.
Bergstrom, T., & Karahalios, K. (2007). Conversation Clock: Visualizing audio patterns in co-located groups. In
         Proceedings of the 40th Annual Hawaii International Conference on System Sciences (HICSS '07). IEEE
         Computer Society, Washington, DC, USA, 78-78.
Dillenbourg, P., & Jermann, P. (2007). Designing integrative scripts. In F. Fischer, H. Mandl, J. Haake & I. Kollar
         (Eds.),  Scripting computer-supported communication    of knowledge    -  cognitive,    computational   and
         educational perspectives (pp. 275-301). New York: Springer.
Lindgren, R.,  &   Johnson-Glenberg,  M. (2013).  Emboldened    by embodiment:     Six precepts     for research on
         embodied learning and mixed reality. Educational Researcher, 42, 445-452.
Lovie, P. (2005). Coefficient of variation. Encyclopedia of statistics in behavioral science.
Sapounidis,   T., Demetriadis, S., Papadopoulos,  P. M.,    & Stamovlasis,  D. (2018).  Tangible     and  graphical
         programming with experienced children: A mixed methods analysis. International Journal of Child-
         Computer Interaction. https://doi.org/10.1016/j.ijcci.2018.12.001.
Shapiro, L. (2010). Embodied cognition. New York, NY: Routledge.
Terrenghi, L., Kranz, M., Holleis, P., & Schmidt, A. (2006). A cube to learn: a tangible user interface for the
         design of a learning appliance. Personal and Ubiquitous Computing, 10(2-3), 153-158.
Verweij,  D., Bakker,  S.,  & Eggen, B.  (2017). FireFlies2:  Interactive Tangible Pixels     to enable  Distributed
         Cognition in Classroom Technologies. In Proceedings of the 2017 ACM International Conference on
         Interactive Surfaces and Spaces (ISS '17). ACM, New York, NY, USA, 260-269.

Acknowledgments
This work has been partially funded by a starting grant from Aarhus University Research Fund, titled "Innovative
and Emerging Technologies in Education". The author would also like to thank Artur Mantaluta and Antonis
Natis for their contributions in producing the CUBE.

CSCL 2019 Proceedings                                  1004                                                   © ISLS
