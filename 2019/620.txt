A Systematic Review of the Quantification of Qualitative Data in the
 Proceedings of International Conferences of CSCL from 2005 to
                                                      2017
                                            Yu Xia and Marcela Borge
                                          yzx64@psu.edu, mbs15@psu.edu
                                        The Pennsylvania State University

        Abstract:   In  computer-supported    collaborative    learning  (CSCL),   verbal  and   non-verbal
        behaviors among group members are often examined to help investigate how people in groups
        interact and learn. Analytic methods used on language data that involve a certain degree of
        quantification  are not  rare. This study    examines  the quantification  of qualitative data  that
        involves some form of language use in articles included in the proceedings of international
        conferences on CSCL from 2005 to 2017. The goal of this systematic review is to identify gaps
        in methodological effort of quantifying human interactions and communication in terms of
        research contexts, levels of analysis, and time points of assessment. In synthesizing information
        on these three aspects, we hope to identify trends over the years and encourage more effort in
        less researched areas.

Introduction
The interaction and communication between individuals who work in collaborative groups are key aspects in
CSCL and is often times under investigation by researchers in the field. As CSCL grows and attracts effort from
multidisciplinary fields, it presents the community with promising developments and tensions (Borge & Mercier,
2018). Previous effort has been made to review some key aspects of CSCL field to provide insights into the
characters of CSCL and further into future paths. Those studies helped to reveal CSCL's membership (Hoadley,
2005), methodological approaches and theoretical frameworks (Jeong, Hmelo-Silver, & Yu, 2013), and similar
issues in fields related to CSCL such as educational and instructional technology (Hrastinski & Keller, 2007).
Research in CSCL produces large amount of qualitative data and has used methods of quantification to identify
patterns and themes in interviews, student speech in naturalistic settings, online discourse, and other qualitative
data forms, as well  as to  understand    relationships in data and  make   predictions on  learning   behaviors  and
outcomes. Chi (1997) proposed a method of content analysis to quantify quantitative data; from then on, there
have been other methods used for the same purpose, such as thematic analysis, open coding, social network
analysis, as well as automatic analysis based on Natural Language Processing. However, we do not yet know how
researchers in the community of CSCL, as identified by having work published in international conferences of
CSCL   proceedings, apply   methods    to quantify   human  interactions over  the years,  and  how  prevalent   these
different types of methods are in the CSCL conference community, what learning contexts we tend to prioritize,
and to what extent we examine collaborative learning processes over time.
        In this paper, we begin the process of examining our community's existing practices and trends by
examining  where   the data being  quantified  was   collected, which   indicates  where   the researchers examined
phenomena associated with learning; at what level(s) researchers analyzed language use; and how many time
points assessments were administered. We believe an understanding of the contexts and participants is critical to
a comprehensive view on the types of learning that has been under investigation and to reveling gaps in research.
Thus, we  ask  the following research     questions: (RQ1)  In  what contexts  have   data of  group   processes been
quantified and how have they changed from 2005 to 2017; (RQ2) What are the levels of analysis that have been
examined in when quantifying group process data and how they changed from 2005 to 2017? 3; (RQ3) How many
times were assessments carried out and how has this aspect changed from 2005 to 2017?

Methods

Selection criteria
We examined CSCL conference proceedings from 2005 to 2017. Inclusion criteria were as follows: (1) articles
had to be five or more pages long; (2) the data sources included some form of language use, i.e., text-based
language, spoken   language,  or  body    language;  (3) the analysis   of the data should     involve some  type  of
quantification, from frequency counts to statistical comparisons or mathematically aided machine automation,
e.g. natural language processing.

CSCL 2019 Proceedings                                    620                                                   © ISLS
Data collection and coding
The first author read through the methods section and findings of all conference papers that were over five pages
to check for inclusion or exclusion.   In total 30.51 % of all papers (180/599) met our criteria; 29/65 articles in
2017, 25/57 in 2015, 31/69 in 2013, 32/71 in 2011, 24/93 in 2009, 19/144 in 2007, and 20/100 in 2005. Information
from methods sections was extracted on (1) research contexts, (2) level of analysis, and (3) number of assessment
time-points.
          Research contexts refer to the settings where the selected research was carried out; due to its variety, the
first round of analysis focused on extracting keywords on these three dimensions with minimal interpretation; in
the second   round  of analysis, categorization was  based   on  the following  codes. Classroom:   studies  in K-16
classrooms, including face-to-face courses or hybrid courses where only part of the learning activity was carried
out online;  these classrooms    may or  may  not   be technology-enhanced,     depending  on  the  research design.
Technology implemented in classroom: studies with focus on technologies such as online systems incorporated in
K-16 classrooms. Lab: studies were carried out in computer laboratories. Technology implemented in informal
learning (online): studies that extracted data from educational online technologies that were used outside of
classrooms. Informal learning (offline): studies in physical informal learning environments such as after school
clubs,   museums,  and  summer    camps.  Informal  learning    (online): studies that extracted data  from   online
communities that are not designed for educational purposes, such as forums where interest groups interact online.
Online classroom: studies were carried out in completely online classes, including both those offered within
degree programs and massive online open courses. Classroom and lab: studies were carried out in both classrooms
and labs. Company: studies were carried out in company settings, such as corporate training sessions. Email list:
studies that extracted data from emails in email lists. N/A refers to unspecified cases.
          Level of analysis indicates whether the analysis was conducted at the level of the individual, dyad, group,
or community. At individual level, data was extracted from individuals; research addressed the question(s) at the
individual level; please note that even in cases where students worked in groups, but if the researchers used
individual data to answer questions concerning individual learners, those studies analyzed data at the individual
level and fell under this category. At dyad level, data was extracted from dyads; research addressed the question(s)
at the group level. At group level, data was extracted from groups with three people or more; research addressed
the question(s) at the group level; in some studies where students worked in groups, but if the researchers analyzed
for individual learning, those studies were not included in this category. At community level, data was extracted
from   a community   of learners  both online   and offline  from  cases  where   researchers considered the    whole
classroom as community to online communities; research addressed the question(s) at the community level. At
multiple  levels, data was extracted from  two   or more    levels from   above categories; research addressed   the
question(s) at multiple levels. N/A refers to unspecified cases.
          Assessment and time point refers to how many times the researchers administered assessment and an
approximate description of whether the interval is long or short to see whether they are looking at learning over
time. Between the first and last time points, short intervals are less than half semester and long intervals are more
than half semester. Cases where there was no information concerning assessment were coded as N/A. The possible
codes for our database of papers included  "1TP," "2TP (long)," "2TP (short)," "5TP," or "N/A."

Results

Context
The five most common contexts were classrooms (47.31%), technology implemented in classrooms (12.90%),
technology implemented in informal learning (online) (7.53%), informal learning (offline) (6.99%), and labs
(11.29%). We examined the frequency of these contexts as settings for studies over time to look for trends. Figure
1 shows the frequencies of each category from 2005 to 2017. Classroom setting shows an upward trend, increasing
from 6 cases in 2005 and 2006 to around 18 in 2013, 2015, and 2017. Technology implemented in informal
learning (online) does not appear in 2015 and 2017, and the laboratory setting shows a slight upward trend.

Level of analysis
As would be expected, the articles that studied groups and dyads account for the most, 81(45%) and 42(23%)
respectively, followed by 20 papers (11%) studying individuals and 16 (9%) examining communities. Only 10
studies  (6%) examined     collaborative learning   at multiple  levels.  Eleven  studies  did  not report   relevant
information. We also examined trends of different categories over the years from 2005 to 2017 (Figure 2). While
there are fluctuations in number of studies on groups, dyads, and communities, the general representations remain
stable, with groups being the most represented, then dyads, and communities being the least represented of the

CSCL 2019 Proceedings                                    621                                                  © ISLS
three. With the exception of 2017, the number of studies examining at the level of individual shows an increasing
trend. Overall, the number of articles that examined multiple levels of analysis is the least represented of all the
categories over time, with the exception of 2017.

  Figure 1. Changes of the most frequent five categories in contexts over the years from 2005 to 2017 (left).
      Figure 2. Changes in the number of papers for different levels of analysis from 2005 to 2017 (right).

Assessment time point
The majority of papers in our sample (116/64.09%) did not provide an analysis of collaborative processes at
different time points. Of those that provided an analysis of collaborative processes at different time points, 44
papers (24.31%) included only two time points; these were primarily pre- and post- tests administered at the
beginning  and end  of the study  over   a short  period. Articles  with two time  points of  assessments  show  an
increasing trend from 2013 to 2017. Followed afterwards are those with one time point of assessment (15/8.29%),
which usually  had  pretests. Notably,   the number   of  assessments administered  to measure    change  over  half
semester is extremely low. Five studies had pre- and post- tests to measure change over half semester from 2009
to 2017, and only one study administered more than two assessments; they measured collaborative processes at
five time points that spanned over 11 weeks.

Discussion
This study examines   the studies  that  quantified  language  that occurs during  collaboration  in the conference
proceedings of CSCL from 2005 to 2017, as seen representative of the overall effort made in CSCL community.
Being a first step to illustrate the effort, this study  focuses on  the research contexts,  level of analysis, and
assessment time points in the selection of articles.
        In terms of research contexts, findings showed a dominance of research conducted in classroom settings
over other types of learning environments. Researchers in learning have emphasized the importance of conducting
research in naturalistic settings (Barab & Squrie, 2004), which offers a possible explanation for the majority of
studies that quantifies human interaction and communication in real classrooms. Nonetheless, our findings show
that there is a context gap since there are other types of naturalistic learning environments that show potential in
offering various learning opportunities which learners might not be able to get from formal classrooms. According
to Enyedy and Stevens (2014, p. 207), the collaborative learning in informal learning sites could be as much
different as they differ from formal schooling; to them, the difference is "striking". Though there are collaborative
learning activities in those formal learning environments and most of the studies were conducted there, those
activities would have less impact on the collaborative learning than other informal, collaborative learning contexts
where learners work as groups Thus, to have a fuller and more in-depth understanding of collaborative learning,
more research is needed in underrepresented contexts (i.e., informal and professional work spaces)       as well as
between contexts (i.e., between classroom and informal contexts, between classroom and work contexts, etc.).
Increasing representation  of  different learning    contexts will  help learning scientists gain  a  richer, deeper
understanding of how people enact collaborative learning and learn to collaborate.
        Findings on levels of analysis shows that the majority of selected articles are focused on dyad or group
level, which causes little surprise. However, researchers in the CSCL community have called for effort to address
collaborative learning at multiple levels (Borge & Mercier, 2018; Stahl, 2013; Strijbos, 2011; Zhao & Frank,

CSCL 2019 Proceedings                                    622                                                   © ISLS
2003). The findings indicate more effort needs to be exerted to address this multi-level analytic needs to further
investigate collaborative learning as it happens across individuals, dyads, small groups, and communities. The
need for studies on individuals and small groups is inherent in learning from sociocultural perspective (see more
on "internalization" from Vygotsky, 1978, p. 56), and the research on learning communities not only offers
opportunities to examine learning phenomena and knowledge dissemination, retention, and transfer at the larger
scale. The  multi-level  research   could   also benefit other effort    in education   research, such as design-based
implementation research (Fishman, Penuel, Allen, & Cheng, 2013).
          Equally   important  is the  need to include   more  studies   conducted   at the community   level. The  low
representation   of such  studies   is problematic, given   that  people    organize their  learning around   the social
communities to which they belong and the process of learning and membership in a community of practice are
inseparable (Lave & Wenger, 1991). Thus, more research in CSCL is needed that operationalizes interactions
within a community or at multiple levels that include the community. Such research would has the potential to
generate innovative insights and contribute to the field.
          Lastly, findings on the assessment of collaborative processes highlights how little research has measured
learning of content or changes in processes over time. While our community values the close examination of
language that occurs during collaboration, the measurement of learning and process changes over time gives
critical information on learning progressions and provides more in-depth analysis of the effectiveness of some
interventions. However, our findings show that in most cases, no assessment was administered, and even for those
pre- and post-tests, most of them were measuring learning in a short time and thus did not consider knowledge
retention or  the actual  development    of competence.   The    lack of    assessments might  be  a result of practical
complexities, restrictions in conducting research, or a lack of measurements that are of high reliability and validity,
but this current status nevertheless creates an urgent need to develop measurements in the field of CSCL. The
quantification on human interactions unlocks potential for learning analytics to exploit process data, which could
be used for measurement in connecting the interpretation of process and outcome data and thus help fill this gap.

References
Barab, S., & Squire, K. (2004). Design-based research: Putting a stake in the ground. The Journal of the Learning
          Sciences, 13(1), 1-14.
Borge,  M.,   &  Mercier,   E. (2018).  Towards   a cognitive  ecological    framework    in CSCL.   In Proceedings  of
          International Conference of the Learning Sciences, ICLS (Vol. 1, pp. 336-343). International Society of
          the Learning Sciences.
Chi, M. T. (1997). Quantifying qualitative analyses of verbal data: A practical guide. The Journal of the Learning
          Sciences, 6(3), 271-315.
Enyedy,   N., &   Stevens,  R.  (2014).  Analyzing  collaboration.    In The  Cambridge     Handbook   of the  Learning
          Sciences, Second Edition. Cambridge University Press.
Fishman,   B. J., Penuel,   W.  R., Allen,  A.-R., &   Cheng,  B.  H.  (Eds.). (2013).   Design-based  implementation
          research: Theories, methods, and exemplars. National Society for the Study of Education Yearbook. New
          York, NY: Teachers College Press.
Hoadley,   C. M.    (2005). The   shape  of the   elephant: Scope   and     membership   of  the  CSCL  community.   In
          Proceedings of the 2005 conference on Computer Support for Collaborative Learning: learning 2005:
          the next 10 years! (pp. 205-210). International Society of the Learning Sciences.
Hrastinski, S., & Keller, C. (2007). An examination of research approaches that underlie research on educational
          technology: A review from 2000 to 2004. Journal of Educational Computing Research, 36(2), 175-190.
Jeong, H., Hmelo-Silver, C. E., & Yu, Y. (2014). An examination of CSCL methodological practices and the
          influence  of   theoretical   frameworks   2005­2009. International        Journal  of   Computer-Supported
          Collaborative Learning, 9(3), 305-334.
Lave, J., & Wenger, E. (1991). Situated learning: Legitimate peripheral participation. Cambridge university
          press.
Stahl,  G.  (2013).   Learning    across    levels. International  Journal    of Computer-Supported       Collaborative
          Learning, 8(1), 1-12.
Strijbos, J. W. (2011). Assessment of (computer-supported) collaborative learning. IEEE transactions on learning
          technologies, 4(1), 59-73.
Vygotsky, L. S. (1978). Mind in society. Cambridge, MA: Harvard University Press.
Zhao,   Y.,   &   Frank,    K.  A.  (2003).    Factors   affecting  technology    uses    in  schools:  An   ecological
          perspective. American educational research journal, 40(4), 807-840.

CSCL 2019 Proceedings                                     623                                                     © ISLS
