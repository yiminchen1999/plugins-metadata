Unpacking Socio-Metacognitive Sense-Making Patterns to Support
                                     Collaborative Discourse
                                   Marcela Borge, Tugce Aldemir, and Yu Xia
                              mborge@psu.edu, tfa5065@psu.edu, yzx64@psu.edu
                                        The Pennsylvania State University

        Abstract:   This   study explores   the dynamics      between   socio-metacognitive    communication
        patterns   and collaborative processes,   as  students  engage   in collaborative   discussions  about
        course   concepts. Building   upon   a  series  of studies  that aimed    to  design   and validate  an
        intervention to help students develop collaborative competencies at the group level, the study
        aims    to map    how   socio-metacognitive     sense-making     patterns  are associated    with   the
        collaboration quality, by comparing the patterns for low, medium, and high performing teams.
        Discussion    and  after-discussion  reflection transcripts of   12 teams    over  five sessions were
        analyzed and assessed, using previously developed collaborative discourse rubric and sense-
        making coding construct. The results showed a significant correlation between frequency of
        sense-making acts and the quality of the collaborative discourse.

Introduction
Collaborative  competencies   are  essential sets of   skills for society   and it is vital  to identify strategies and
approaches to nurture them in learners. Unfortunately, many learners do not have the opportunities to develop
collaborative competencies, which can lead to poor performance outcomes and undesirable group conflicts (Borge
& Carroll., 2014; Fischer et al., 2013; Kozlowski & Ilgen, 2006). In addition to individual problems associated
with underdeveloped collaborative skills, in CSCL environments, group-level issues such as social loafing, sucker
effect (Salomon & Globerson, 1989), lack of non-verbal expressions, and time lag between the interactions can
contribute to lower quality collaboration (Kreijns, Kirschner, & Jochems, 2003). This is why helping learners
engage in higher quality collaborative processes has been a central concern for CSCL. As such, many CSCL
researchers have attempted to help students improve collaborative skills, but few have recognized the complex
and nested nature of collaboration (Borge & White, 2016; Borge, Ong Shiou, & Rosé, 2018; Baker et al., 2007;
Stahl, 2006).
        Prior research has indicated that without any guidance, support, or training, students tend to demonstrate
dysfunctional group processes (Borge at al., 2018; Barron, 2003; Hogan, 1999; Webb & Palincsar, 1996). To
address this  problem, existing  studies have   designed   and  evaluated   interventions   to support  group   function,
including  (1) scripting  collaborative interactions,  (2) helping  students  improve     metacognitive  sense-making
through reflecting upon individual and group performances and comparing them to the models of competence,
(3) supporting students to develop self- and group-regulatory behaviors, and so forth. However, there still are
issues that need further investigation, such as what kind of scripting is needed, how to balance the level of
scripting, when to fade the external support, and how to help groups internalize the external support (Borge &
White, 2016; Hogan, 1999; Kozlowski, Watola, Jensen, Kim, & Botero, 2009).
        Building upon the critical role that self-regulation plays in individual's learning (Zimmerman & Schunk,
2001), Borge et al.'s (2018) suggested that metacognitive guidance and regulation at the group level can serve to
help students optimize their collaborative experiences. They referred to collective awareness of and the collective
ability to monitor and regulate the collaborative process as socio-metacognition. Borge et al.'s (2018) findings
were promising, but also highlighted the need for further research on the development of socio-metacognitive
expertise. Similarly,  the scarcity  of  research  on   the   interplay  between   socio-metacognitive      process and
collaborative  discussion necessitates  further exploration   of  their dynamics   in  CSCL     context (Kwon,  Liu,  &
Johnson, 2014; Rogat & Adams-Wiggins, 2015). This paper builds upon this existing literature and aims to extend
what is known about collective regulation by identifying critical socio-metacognitive sense-making patterns in
process-related dialogue   acts in real-world  collaborative   learning  contexts.   Thus,  we  explored  the dynamics
between    socio-metacognitive   sense-making   patterns   and    collaborative processes,     as  students engaged   in
collaborative discussions about course concepts.

Theoretical framework
Our work is influenced by the theory of group cognition and thus recognizes collaboration as a form of nested
cognition that entails intersubjective knowledge construction and collective sense-making that are situated at
different levels, including individual, group, and community levels (Stahl, 2006). Collaborative activities offer

CSCL 2019 Proceedings                                    320                                                      © ISLS
sophisticated learning opportunities that might not be available in individual learning activities, which are likely
to help learners meet the need in our society for higher collaborative competencies and to succeed in collaborative
teams. However, problems often arise in collaborative activities, especially in terms of the collective sense-making
processes (Barron, 2003). The collaborative discourse that occur during group interactions has a large influence
on the group  performance   and  outcome (Kozlowski     &   Ilgen, 2006; Stahl, 2006). Thus, the    examination  of
communication patterns can help evaluate the quality of collaborative discourse.

Challenges for group regulation
Literature on self-regulation in individual learning   has identified problems  that pose  barriers for learners to
successfully regulate their own learning, such as not being aware of learning problems, misdiagnosing them, etc.
(Winne & Nesbit, 2009). In collaborative contexts, these individual regulation problems arise alongside group
regulation problems, where may teams fail to identify or accurately assess problematic processes and devise
remediation strategies. There are also social and emotional difficulties involved in communicating problems and
regulating group behaviors (Järvelä, & Hadwin, 2013). In each stage of regulating collaborative process, there is
a requirement on building shared understanding through synthesizing information and idea negotiation (Stahl,
2006). We argue that in order for teams to engage in high-quality collaborative activities, they need to develop
socio-metacognitive expertise: the knowledge of and ability to monitor and regulate collective cognitive processes.

Socio-metacognitive sense-making
As collective literature argues, engaging in high quality collaborative discussion is a hard but crucial skill to
develop. Despite multiple complexities and interrelated variables to consider, research also suggested that learners
can improve the quality of their collaborative activities over time, if they learn how to regulate their collaborative
process (Borge et al., 2018; Kozlowski et al., 2009).
        Collaborative interactions  are the  externalized   forms  of collective thinking, and  thus,   how teams
collectively make sense of, monitor, and regulate these interactions play a central role in collaborative process.
Recognizing the role of group regulation in collaborative activities, recent research efforts have been directed
toward developing technological tools to support group regulation. As one of the pioneers of these efforts, Järvelä
and Hadwin (2013) developed a technological tool to help learners develop awareness and planning strategies for
their collaborative activities. However, developing awareness and planning is not sufficient for the regulation
process; learners need to develop an understanding for how a high-quality collaborative discussion should be like,
compare their process to a model, identify problems in their process, collectively develop or choose appropriate
remedial strategies to solve problems, and take action (Nesbit, 2012; Winnie, & Nesbit, 2009). In group contexts,
these steps demand both individual and collective efforts, attention, and time, which might be the reason why
teams tend to neglect their collaborative process while reserving all the attention to make sense of the content
(Kerr & Tindale, 2004). Collective sense-making of the collaborative process requires teams to monitor and reflect
on their collaborative activity (Nesbit, 2012). However, research suggested that individuals do not perform well
at asking and addressing these questions, and situation gets even intensified when sense-making moves from
individual to group cognition (Gabelica et al., 2014).
    In our  previous  work,   we addressed  problems    associated  with collective  regulation and   developed  a
theoretically informed technological intervention to help students develop their socio-metacognitive expertise.
We identified the communication patterns associated with high quality collaborative discussion, proposed two
core capacities for collaborative sense-making, and listed concrete patterns of communication associated with
more or less optimal collaborative sense-making processes. We then helped students monitor and regulate their
collaborative processes by guiding and constraining how they get prepared for discussion and how they engaged
in socio-metacognitive sense-making and regulation after the discussion. The intervention succeeded in getting
teams to improve the quality of their collaborative activity over time. However, we did not fully examine the
dynamics between socio-metacognitive competence, collective sense-making, and the quality of the collaborative
discussion. The collection of research on socio-metacognitive sense-making suggests that it is critical to identify
how teams engage in socio-metacognitive sense-making and regulation of both course content and team process
discourse, and how these interconnected processes may impact each other. To address this need, we aim to identify
socio-metacognitive sense-making patterns in collaborative discussions and thus examined how these patterns are
associated with team collaborative performance. Our research questions were:
    (RQ1) What patterns of socio-metacognitive sense-making (SMS) talk do teams engage in when unpacking
    course content and thinking about their own discussion processes?
    (RQ2) What are the differences in these patterns between low, medium, and high performing teams?

Methods

CSCL 2019 Proceedings                                   321                                                  © ISLS
Course context and participants
The study was conducted in an online 16-week undergraduate course designed to introduce students to information
science concepts. As part of the course, students were expected to engage in collaborative reasoning practices and
discussion activities. Developing collaborative discussion skills was one main goal of the course. Participants
were 34 online students who enrolled in the course (11 females, 33.3%; 22 males ,66.7%). Students' ages ranged
from 25 to 44, and the majority of students were part-time students with full-time jobs.

Procedure
Students were assigned to 12 teams of three based on when they were available to meet. Due to two students
dropping the class, two teams ended up as dyads. These teams were required to meet synchronously for five
sessions to collectively make sense of course concepts. They met every other week for ten weeks. As a pre-
discussion activity, the students were required to read the weekly readings and write an individual reflection in
response to four higher-order questions about the readings. Then, they were asked to set a meeting time with their
teammates to synchronously discuss the questions and readings. Each discussion session was about 90 minutes:
60-minute  main    discussion, 15-minute     individual  assessment  of  team   discussion,    and 15-minute   collective
planning discussion. These activities counted towards 25% of students' grades. For the 15-minute individual
assessment of the team discussion that followed the 60-minute main discussion, students were provided with a
collaborative process rubric detailing how to assess discussion quality, guides containing goals for collaboration,
problems   that interfered with      good collaboration, and  strategies for improving    collaborative   processes. The
discussions were held on a computer supported collaborative discussion environment, and saved automatically in
the system. After each individual scored their team's collective processes, the entire team was responsible for
completing a collective planning session, where they discussed their scores and process weaknesses they identified
during individual reflection for the purpose of collectively diagnosing problems and planning out strategies the
team could use to improve in future sessions.

Research design and analysis
We implemented explanatory mixed methods (Creswell, 2015). The main discussions, individual reflections, and
collective planning sessions were collected and analyzed for 12 teams across five sessions, following the verbal
analyses guidelines offered by Chi (1997). We coded a total of 12,755 utterances, 10,764 from content-based
discussions and 1,991from team planning sessions. All ethical guidelines were followed in collecting, analyzing,
and reporting the study.

Evaluating teams' discourse quality
Building upon previous theoretical frameworks (Borge et al., 2018), teams' discourse quality, when teams work
to collectively understand course content, is defined as the extent to which team members provide evidence of
engaging   in   communication   patterns    associated   with   high-quality  information      synthesis  and knowledge
negotiation (see Table 1).  The quality of teams' discussions was assessed by a research assistant with two years
of communication analysis training, using a rubric developed by Borge et al., (2018), which measures two core
capacities each with three categories of behavior (see Table 1). To score each item, the entire transcript is

Table 1: Summary of core capacities and categories of behavior associated with high quality collaborative
communication behaviors from Borge et al. (2018).*

 Core Capacities     Categories of Behavior                   What is Examined in the Transcript
 Information         Verbal Equity                            To what extent team members contributed equally.
 Synthesis           Developing Joint Understanding           To what extent team members make an effort to ensure that
                                                              they fully understand the ideas.
                     Joint Idea Building                      To  what   extent  team   members    elaborate  on  others'
                                                              contributions.
 Knowledge           Exploration of Different Perspectives    To  what   extent teams   present   and discuss  alternative
 Negotiation                                                  perspectives.
                     Quality of Claims                        To  what   extent teams   provide   logical and  fact-based
                                                              evidence to their claim.
                     Norms of Evaluation                      To what extent teams adhere to social norms.
Note: * Each score ranging from 1 to 5 outlines a set of guidelines indicating what each score means for each category. For
example, in Quality of claims, a score of five means "There are at least two examples where claims are supported by references
to course readings or online content    AND   at least one example  of weighing   of   options or examination  of different
perspectives." (Borge et al., 2018).

CSCL 2019 Proceedings                                       322                                                     © ISLS
examined for specific discourse quality markers (see top of figure 1 for example) and these markers are used as a
means to provide evidence for a score from 1 to 5 (see bottom of Figure 1). Scores for the six categories were
summed to a single collaborative discussion quality score for each discussion. Once all the five main discussion
scores were identified, they were averaged to produce the average collaboration performance of each team. The
quality of the collaborative discussion was assessed using only the main discussion transcripts. 20% of the data
were   double  coded    by two  trained students with extensive   communication       analysis    experience.     Significant
agreement was reached, r = .86, p < .001; Kappa = .64, p < .001.

 Session      Turn      User ID   Entry                                                                               Codes
 1            29        86        He tries to balance his views with a pro for privacy violations, asserting that it  DJU-1,
                                  may be used for national security. Do you agree with this implication?              JIB- 1
 1            31        85        I apologize for the delay, I've been rereading your post, I'm not sure I            DJU- 1
                                  understand what you mean. The author is pro privacy violation?

 Criteria               Score   Evidence
 Joint Idea Building    4       I could not locate more than one instance [of JIB] where the participants extended or
                                supported the original claim with additional information. The participants seemed to either
                                skip to another subject very quickly or discuss an opinion over one or two turns.
 Developing Joint       5       There are at least two instances where one participant made an effort to ensure s/he
 Understanding                  understood what the previous participant said. They used the following strategies: (DJU-1)
                                Rewords another member's ideas to make sure s/he understands it; (DJU-2) Asks another
                                member to explain an idea by elaborating further.
    Figure 2. Two screenshots depicting how lines of transcript are examined for specific markers for developing joint
understanding (DJU-1 represents first instance of DJU) and joint idea building (JIB-1 represents first instance of
           JIB) (top) and how these markers are used as evidence for discourse quality scores (bottom).

Analysis of socio-metacognitive sense-making (SMS) talk
The transcripts of main discussions and after-discussion collective reflection and planning sessions were analyzed
for 12 teams across five sessions, for a total of 60 analyzed discussions. Socio-metacognitive sense-making is a
specific type of process talk where students think about their collaborative processes to try to understand or modify
them. To identify SMS talk, main discussions and reflection discussions were segmented into chat turns; then
each turn was coded as process (P), content (C), or other (O). Inter-rater reliability for 20% initial coding was
Kappa = .79, p < .001. The total frequency of P acts varied for each team. To compare teams' SMS behaviors, we
calculated percentages of SMS talk out of P acts.
         We used two versions of a socio-metacognitive sense-making coding scheme originally developed by
Borge et al. (2018): the original version was used for reflections about the processes that occurred during their
content-based discussion and a second version for the content-based discussion itself. Both original and modified
rubrics are presented in Table 2. The original version was designed to code talk that occurred during reflection
sessions to identify the extent to which teams engaged in socio-metacognitive sense-making activities during the
reflective activity. It included process reporting, monitoring, reflecting, planning, and revising. Two trained coders

Table 2: Examples of SMS patterns by the participants
   SMS           Rubric      Example
   Pattern       Version
   Reporting     Both        "We sure covered a lot." (MD)
   (RP)          rubrics     A student reports her/his opinion about the collaboration quality without referring to concrete
                             events or patterns from discussion or justifying her/his judgment.)
   Process       Both        "and we also evaluated trade-offs for some while comparing implications." (MD)
   Monitoring    rubrics     (A student demonstrates evidence of paying attention to the ongoing collective process by
   (MO)                      pointing out a specific activity/requirement that the team has done or needs to do.)
   Process       Both        "I think the problem is that we read 2 different things." (MD)
   Reflection    rubrics     (A student demonstrates evidence of reflecting on their ongoing collaborative discussion by
   (RF)                      pointing out a reason for why s/he thinks that particular incident happened.)
   Process       Original    "To begin, we definitely need to work on time management. Our communication skills are
   Planning      rubric      sufficient when it comes to the subject matter, but we definitely need to get tasks done with a
   (PL)                      sense of urgency." (AR) (A student demonstrates evidence of planning by unpacking the
                             problem and proposing new goals.)
   Regulation    Modified    "Let me play devil's advocate, since we need to think about the other side of the coin." (MD)
   (R)           rubric

CSCL 2019 Proceedings                                      323                                                          © ISLS
                          (A student demonstrates evidence of regulating ongoing activity reminding teammates about
                          previously set process goals.)
Note: MD=Main discussion, AR=After-discussion reflection.

coded 23% of the total reflective discussion data using the original construct, with Kappa = .806; p < .001. The
coders discussed and resolved disagreements and then one coder re-coded all the reflection data. A second version
of the coding construct was created to code SMS talk that occurred during the content-based discussion. This new
version included all the previous forms of talk plus a new category: regulation talk. Regulation talk identified
socio-metacognitive strategies, including moves such as proposing or using a discussion strategy from guides we
provided. Inter-rater reliability was checked on this new version on 24% data with Kappa = .725; p < .001. Two
coders discussed and resolved disagreements and then one coder re-coded all the main discussion data.

Selection of high, medium, and low performing teams
Average collaborative discussion quality scores were used to assess performance for each team (see Table 3).
High, medium, and low performing categories each have four teams.

Table 3: Mean discussion quality scores of the teams (Lowest to Highest)
        Low Quality Discourse Teams        Medium Quality Discourse Teams          High Quality Discourse Teams
        T9      T7     T1       T4         T3        T6        T8       T12        T5       T10       T11     T2
  M     19.80   20.20  21.40    21.80      23.40     23.40     23.40    23.80      24.00    24.40     24.60   25.40
  SD    3.56    4.15   3.36     3.42       2.88      2.51      3.97     2.59       3.32     1.52      3.65    3.21

Comparing team process interactions
We conducted one-way ANOVA to determine whether there was a significant difference in the average means of
the amount of socio-metacognitive sense-making talk that occurred in low, medium, and high performing teams.
For that, Levene's test for homogeneity of variances was found to be protected for the total percentage of SMS
acts (F(2,9)= 1.23, p=.34). Shapiro-Wilk test was performed to ensure normality, and no significant value was
identified; therefore, the normality assumption was met to perform ANOVA.

Findings
(RQ1) Patterns of socio-metacognitive sense-making (SMS) talk
On average, each team created 263.58 process-related turns over five discussion sessions, SD=120.75, Min=133,
Max=521. Over half of these process-related turns, 54.54% (SD=13.03), were coded as SMS talk. Those that were
not coded as SMS focused on sharing information about how they were using technology or other forms of social
-off-task talk. All teams engaged in RP, MO, RF, and PL/R acts in at least one discussion session, but only 7 out
of 12 teams demonstrated RV act (Table 4).

Table 4: The mean percentages of teams' SMS patterns
                                          Main Discussions     After-Discussion Reflections    Total
                                          M (%) *     SD       M (%) **       SD               M (%) ***    SD
    Reporting (RP)                        2.65        4.96     18.37          14.65            11.52        5.06
    Process Monitoring (MO)               12.27       4.77     8.45           7.34             10.39        4.76
    Process Reflection (RF)               0.19        0.45     5.44           2.63             3.45         1.66
    Process Planning/Regulation (PL/R)    43.93       11.05    15.14          11.96            28.64        9.71
    Process Revising (RV)                 0.58        1.15     0.45           0.67             0.54         0.62
Notes: *  Mean  frequency   percentage out of   the total  P  frequency in   main  discussions for   each  SMS   pattern
       ** Mean frequency percentage out of the total P frequency in after-discussion reflections for each SMS pattern
       *** Mean frequency percentage out of the total P frequency for each SMS pattern

         The findings suggested that the teams engaged in PL/R act the most. In the main discussion these acts
occur as regulation acts focused on proposing a discussion strategy or new direction for conversation, whereas in
the after-discussion reflections these acts are planning acts focused on identifying a strength or weakness, or
proposing and evaluating goals and strategies for future discussions. As shown in table 4, the high frequency of
PL/R acts are due to the high number of regulation acts that occurred in the main discussions. For example, during
the second discussion, a team was discussing whether all members understood the course content. One team
member, Bill, stated that the content was new to them so they did not have full understanding of it. Upon hearing
that Bill had specific questions, another member, Juan, said: "... we had some questions there, Bill, you can start"
(R act in Main Discussion; Team 2, Session 2).

CSCL 2019 Proceedings                                     324                                                    © ISLS
        Figuring out how to discuss topics deeply, while keeping to an agreed upon time limit was a common
topic discussed during the after-discussion reflection  sessions. Team          2 discussed  this topic as part of  their
reflection. Bill said, "Although we got very in depth, I feel like we could get just as in depth if we focus the
conversation more and have strict time framing." Another member, Jill, added, "We need to figure out how to
keep to the timeframe without cutting off something important" (PL Act, After-Discussion Reflection; Team 2,
Session 2,). Juan then proposed a suggestion for improvement, "We can do anything! we will just have to weigh
our questions maybe before we start the conversation and start there as a base." Bill responded by saying. "Right
maybe ask the longer ones first" (PL Act, After-Discussion Reflection).

(RQ2) Differences between low, medium, and high-quality discourse teams
Looking at the relationship between SMS talks and team performance, when ranking teams by SMS acts, we saw
that the frequency of SMS acts was closely tied to the quality of collaborative discourse (see table 5).

Table 5: Frequencies of teams' collaboration patterns (SMS Lowest to Highest for Each Performance Category)
          Low Quality Discourse Teams    Medium Quality Discourse Teams                High Quality Discourse Teams
          T4     T7     T9      T1       T6       T8        T12     T3                 T11     T5       T10      T2
 P        348    365    196     217      142      133       247     200                216     521      166      412
 SMS*     25.57  41.10  43.88   50.23    47.89    57.89     61.54   67.00              60.19   65.45    66.27    67.48
Notes: * Total frequency percentage out of the total P frequency for each team
P=Process-related talk, SMS=Socio-metacognitive sense-making talk, T=Teams

        Comparing values on Table 5 with those on Table 3 (teams' mean discussion quality scores), we observed
that low quality discourse teams tended to engage in least SMS acts while high quality discourse teams tended to
engage in most SMS acts. However, as illustrated in Table 5, there were some exceptions. A Pearson correlation
analysis on SMS percentage means and mean performances showed a strong positive correlation (r = .749, n =
12, p = .005). Analysis of variance showed a significant difference in the SMS percentage means between different
quality discourse teams, F(2,9)=10.66, p=.00. Tukey HSD indicated a significant difference in SMS percentage
means between low (M=40.20, SD=10.47) and high-quality discourse teams (M=64.85, SD=3.22), and between
low (M=40.20, SD=10.47) and medium quality discourse teams (M=58.58, SD=8.05). No significant difference
was found between medium and high-quality discourse teams.
        We conducted one-way ANOVA to determine whether there is a significant difference in the five SMS
patterns (n=5) between the low, medium, and high-quality discourse teams; no significant differences were
found. However, descriptive analysis of the percentage values between teams yielded interesting trends. The
average percentage of PL/R act increased from low (M=19.76, SD=5.12) to medium performing teams
(M=32.09, SD=9.92), and from medium to high performing teams (M=34.09, SD=7.77). In RV act: low-
performing teams engaged less (M=0.27, SD=0.31) than medium-performing teams (M=0.52, SD=0.37), who
engaged less than high-performing teams (M=0.83, SD=0.98). The average percentage of RF act in low-
performing teams was smaller (M=2.86, SD=1.21) than that of medium-performing teams (M=3.62, SD=2.65),
which was smaller than that of high-performing teams (M=3.87, SD=0.91). Figure 2 below summarizes the
distribution of the SMS acts by performance categories.

                  Figure 2. SMS percentage means of the teams by performance categories.

CSCL 2019 Proceedings                                  325                                                        © ISLS
Discussion
In this paper we examined teams' sense-making patterns in the main content-based discussions and during after-
discussion reflections, where teams made sense of their collaborative process. We also examined the relationship
between  sense-making     patterns and collaborative   discourse  quality. Our   findings showed    that all the teams
engaged in collective socio-metacognitive sensemaking (SMS) talk during content-based discussions and after-
discussion reflections. Further supporting the design of the original intervention, which aimed at pushing students
to engage in SMS talk as part of reflections for the purpose of getting them to figure out regulate activity during
content-based talk and then actually regulate activity. We also found a strong positive correlation between the
frequency of socio-metacognitive sense-making talk and the quality of collaborative activity.
         These findings add to the growing body of research that indicate that supporting collective regulation of
group interactions may be the key to enhancing the quality of collaborative processes (Kwon, Liu, & Johnson,
2014; Rogat & Adams-Wiggins, 2015). Prior research has suggested that students do not have the ability to
monitor and regulate individual and collaborative activities (Borge & White, 2016; Kwon, Liu, & Johnson, 2014;
Gabelica et al., 2014; Winne & Nesbit, 2009), and if not provided with sufficient amount of guidance, they will
likely to develop dysfunctional collaborative habits (Borge et al., 2018; Kozlowski & Ilgen, 2006; Webb &
Palincsar, 1996). The current study suggests that guiding students to develop sense-making skills to monitor and
reflect their ongoing/past    collaborative processes  can  help  teams  to  developing   sophisticated  collaborative
competencies. What is more, this work suggests it is possible to enhance collaborative processes without over-
scripting collaboration as it occurs or creating inauthentic collaborative environments (Dillenbourg, 2002; Fischer
et al., 2013). As such, this paper also extends what is known about the impacts of socio-metacognition on teams'
ability to improve the quality of collaborative discussion processes.
         Our    findings  imply  that  scripting socio-metacognitive    sense-making    activities  before   and  after
collaborative activity can help teams to regulate that ongoing collaborative activity, thereby developing socio-
metacognitive competence and optimizing their ongoing collaborative activities. However, it also underlines the
need for a more blended approach to scripting that supports collaborative process both during and after they occur.
Nonetheless, it is crucial to note that further research is needed to support these findings and to explore how after-
discussion  reflection  impact  students' sense-making   of their collaborative  process,  their socio-metacognitive
development, and collaborative process improvement.
         One limitation of this study is the low number of the groups for each quality discourse category. Further
studies are needed to see if the patterns observed in the study remain once the number of observations increases.
Furthermore, the focus of this study did not fully address impacts of emotions on social interactions, which were
also prevalent forms of process-based talk. Building on our theoretical framework, collaboration emerges as a
product  of  interactions among    individuals (Stahl, 2006), which     entails the exchange   of   ideas, knowledge,
emotions, and feelings (Järvenoja & Järvelä, 2009). Thus, further research is needed to examine the dynamics
between socio-metacognitive expertise, socio-emotional interactions, and the quality of collaborative discourse.

References
Baker, M., Andriessen, J., Lund, K., van Amelsvoort, M., & Quignard, M. (2007). Rainbow: A framework for
         analyzing   computer-mediated      pedagogical  debates. International   Journal   of   Computer-Supported
         Collaborative Learning, 2(2), 315-357.
Barron, B. (2003). When smart groups fail. The Journal of the Learning Sciences, 12(3), 307-359.
Borge,  M.,   &  Carroll, J.  M. (2014,   November).   Verbal equity,   cognitive   specialization, and  performance.
         In Proceedings of the 18th International Conference on Supporting Group Work(pp. 215-225). ACM.
Borge,  M.,   &  White,  B. (2016). Toward   the development   of  socio-metacognitive    expertise: An    approach  to
         developing collaborative competence. Cognition and Instruction, 34(4), 323-360.
Borge,   M.,  Ong,  Y.    S., &  Rosé,  C.  P.   (2018). Learning  to   monitor   and  regulate   collective  thinking
         processes. International Journal of Computer-Supported Collaborative Learning, 13(1), 61-92.
Bruner, J. (1999). Postscript: Some reflections on education research. In E. C. Lagemann & L. S. Shulman (Eds.),
         Issues  in education   research: Problems   and  possibilities (pp. 399-409).    San Francisco:   Jossey-Bass
         Publishers.
Chi, M.  T.   H. (1997).  Quantifying  Qualitative Analyses   of  Verbal   Data: A  Practical Guide.  Journal    of the
         Learning Sciences, 6(3), 271-315.
Dillenbourg, P. (2002). Over-scripting CSCL: The risks of blending collaborative learning with instructional
         design. In P. A. Kirschner (Eds.), Three worlds of CSCL. Can we support CSCL (pp. 61-91). Heerlen:
         Open Universiteit Nederland.

CSCL 2019 Proceedings                                    326                                                     © ISLS
Fischer, F., Kollar, I., Stegmann, K., Wecker, C., Zottman, J., & Weinberger, A. (2013). Collaboration scripts in
       computer supported collaborative learning, In C. Hmelo-Silver, C. Chinn, C. Chan, & A. O'Donnell
       (Eds.), The International handbook of collaborative learning (pp. 403-419). New York, NY: Routledge.
Gabelica, C., van den Bossche, P., De Maeyer, S., Segers, M., & Gijselaers, W. (2014). The effect of team
       feedback and guided reflexivity on team performance change. Learning and Instruction, 34, 86-96.
Hogan, K. (1999). Thinking aloud together: A test of an intervention to foster students' collaborative scientific
       reasoning. Journal of Research in Science Teaching, 36(10), 1085-1109.
Järvelä, S., & Hadwin, A. F. (2013). New frontiers: Regulating learning in CSCL. Educational Psychologist,
       48(1), 25-39.
Järvenoja, H., & Järvelä, S. (2009). Emotion control in collaborative learning situations: Do students regulate
       emotions evoked by social challenges. British Journal of Educational Psychology, 79(3), 463-481.
Kerr, N., & Tindale, S. (2004). Group performance and decision making. Annual Review of Psychology, 55(1),
       623­655.
Kozlowski, S., & Ilgen, D. (2006). Enhancing the Effectiveness of Work Groups and Teams. Psychological
       Science in the Public Interest, 7(3), 77-124.
Kozlowski, S. W., Watola, D. J., Jensen, J. M., Kim, B. H., & Botero, I. C. (2009). Developing adaptive teams:
       A theory of dynamic team leadership. In S. Eduardo, G. F. Goodwin, & C. S. Burke (Eds.), Team
       effectiveness in complex organizations: Cross-disciplinary perspectives and approaches (pp.113-155).
       New York, NY: Routledge.
Kreijns, K., Kirschner, P. A., & Jochems, W. (2003). Identifying the pitfalls for social interaction in computer-
       supported  collaborative learning   environments:   A review of the research. Computers   in Human
       Behavior, 19(3), 335-353.
Creswell, J. (2015). Educational research: Planning, conducting, and evaluating quantitative and qualitative
       research. New York, NY: Pearson.
Kwon, K., Liu, Y. H., & Johnson, L. P. (2014). Group Regulation and Social-Emotional Interactions Observed in
       Computer   Supported  Collaborative   Learning:  Comparison  Between  Good    vs. Poor Collaborators.
       Computers & Education, 78, 185-200.
Nesbit, P. L. (2012). The Role of Self-Reflection, Emotional Management of Feedback, and Self-Regulation
       Processes in Self-Directed Leadership Development. Human Resource Development Review, 11(2), 203-
       226.
Rogat, T. K., & Adams-Wiggins, K. R. (2015). Interrelation between Regulatory and Socioemotional Processes
       within Collaborative Groups Characterized by Facilitative and Directive Other-Regulation. Computers
       in Human Behavior, 52, 589-600.
Salomon, G., & Globerson, T. (1989). When teams do not function the way they ought to. International Journal
       of Educational Research, 13, 89-99.
Stahl, G. (2006). Group cognition: Computer support for building collaborative knowledge. Cambridge, MA:
       MIT Press.
Webb, N. M., & Palincsar, A. S. (1996). Group processes in the classroom. In D. C. Berliner & R. C. Calfee
       (Eds.), Handbook of educational psychology (pp. 841-873). New York, NY: Macmillan.
Winne, P. H., & Nesbit, J. C. (2009). Supporting self-regulated learning with cognitive tools. In D. J. Hacker, J.
       Dunlosky, & A. C. Graesser (Eds.), Handbook of metacognition in education (pp. 259-277). New York,
       NY: Routledge.
Zimmerman, B. J., & Schunk, D. H. (2001). Self-regulated learning and academic achievement: Theoretical
       perspectives (2nd Eds.). Routledge.

CSCL 2019 Proceedings                                 327                                                 © ISLS
