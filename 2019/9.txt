    Analysis of Touchscreen Interactive Gestures During Embodied
Cognition in Collaborative Tabletop Science Learning Experiences
     Nikita Soni, Alice Darrow, Annie Luc, Schuyler Gleaves, Carrie Schuman, Hannah Neff, Peter Chang,
                       Brittani Kirkland, Amanda Morales, Kathryn A. Stofer, Lisa Anthony
               nsoni2@ufl.edu, aldarrow@ufl.edu, annieluc@ufl.edu, schuylergleaves@ufl.edu,
            carrie.schuman@ufl.edu, hannahneff@ufl.edu, pchang27@ufl.edu, bgkirkland@ufl.edu,
                            mondily@ufl.edu, stofer@ufl.edu, lanthony@cise.ufl.edu
                                                 University of Florida

                                          Jeremy Alexandre (9th author)
                                            jeremy.alexandr@gmail.com
                                  City University of New York--Brooklyn College

         Abstract: Previous work has used embodied cognition as a theoretical framework to inform
         the design of large touchscreen interfaces for learning. We seek to understand how specific
         gestural interactions may be tied to particular instances of learning supported by embodiment.
         To help us investigate this question, we built a tabletop prototype that facilitates collaborative
         science learning from data visualizations and used this prototype as a testbed in a laboratory
         study with 11 family groups. We present an analysis of the types of gestural interactions that
         accompanied embodied cognition (as revealed by users' language) while learners interacted
         with our prototype. Our preliminary findings indicate a positive role of cooperative (multi-
         user) gestures in supporting scientific discussion and collaborative meaning-making during
         embodied  cognition.     Our  next steps  are to   continue    our analysis  to identify additional
         touchscreen   interaction design   guidelines for  learning    technologies, so that  designers can
         capitalize on the affordances of embodied cognition in these contexts.

Introduction
Large touchscreen interfaces like multi-touch tabletops have become increasingly widespread, particularly in
informal learning environments such as science museums (Geller, 2006). For example, science educators and
researchers are using large touchscreen interfaces to support learning about complex global phenomena such as
Earth's ocean system (Cheek, 2010). Yet, many visitors have trouble understanding the information presented
on these interfaces, let alone interacting with the interfaces to develop a deeper understanding of the information
(Cheek, 2010). Previous interaction design research has used embodied cognition as an underlying theoretical
framework to inform the design of large touchscreen interfaces for learning (Lin et al., 2016; Piper et al., 2012).
Embodied cognition theory posits that cognition is not solely based in the mind, but also in the body, i.e., some
of our cognitive processes occur through "perceptually-guided motions" (Wilson & Golonka, 2013). Learning
around   a shared interactive display   like   a touchscreen     tabletop with direct-touch   gestures draws   on  the
affordances of embodiment. These interactive gestures play an important role in how learners explain abstract
ideas and   promote    scientific  discussion  (Piper  et   al., 2012).   Although    embodiment   theories   consider
sensorimotor activities like direct-touch gestures to be integral to learning, the community has noted that there is
still no "conceptually coherent and empirically validated design framework" to inform the design of embodied
learning   experiences (Abrahamson     et al., 2018,   p.1243).   Prior   research that  has explored  the  design of
touchscreen  interfaces for learning   from  the  viewpoint  of   embodied   cognition   has assumed   that embodied
cognition is driving learning during any motion or interaction, without considering how specific interactions
may influence particular instances of learning. If we could identify the types of touchscreen interactions being
used  when  people are  engaged    in embodied    cognition during    a learning   episode, we could  design  learning
experiences that explicitly support and encourage touchscreen interactions directly linked to learning.
         In this paper, we analyze touchscreen gestural interactions that accompany linguistic cues of embodied
cognition during collaborative learning (Kirschner et al., 2018), in the context of science learning about data
visualizations of Earth's global ocean system. We built a tabletop application prototype to support collaborative
learning and used this prototype as a testbed in a lab study with 11 family groups. Throughout this paper, we use
the term   "learning"  to reflect  meaning-making,     that is,   the process  of  integrating  new  knowledge    and
coordinating it with existing beliefs and knowledge (Vygotsky, 1978), rather than its traditional use to reflect
"knowledge   acquisition."  Based   on previous   research  that  has   shown  conceptual    metaphors are  a type of
embodiment language (Lakoff & Johnson, 2003), we relied on identifying the three main conceptual metaphors

CSCL 2019 Proceedings                                     9                                                     © ISLS
(metonymy, orientation, and ontology) in groups' utterances as cues to the occurrence of embodied cognition.
We then analyzed participants' gestures that co-occurred with these utterances and linked these gestures to how
people were making meaning at that moment. Specifically, we investigated the following research questions:
(RQ1) When groups' utterances reveal that embodied cognition is occurring, what gestures are they making,
during learning with an interactive tabletop? (RQ2) How can we support and encourage these types of gestural
interactions in the design of computer-supported collaborative learning experiences on an interactive tabletop, to
afford embodied   cognition   more  directly?  Our  preliminary  findings  indicate    a positive       role of  cooperative
(multi-user) touchscreen gestures in supporting collaborative learning on tabletops during embodied cognition.
This paper outlines two initial themes and touchscreen interaction design guidelines for affording embodied
cognition, to inform the design of future tabletop experiences for science learning.

Embodied cognition in designing tabletop applications for learning
Prior  work  investigating  collaborative  learning  around    multi-touch   tabletops   has       already   used embodied
cognition as a theoretical framework to design these learning experiences (Lin et al., 2016; Piper et al., 2012).
Embodied cognition provides an ideal framework for understanding interactions with multi-touch technology
because  it considers direct hands-on   interactions with the  digital  content  to be   integral       to cognition  (Kirsh,
2013). Lin et al. (2016) used embodied cognition as their theoretical basis to design a tabletop application to
promote  collaborative   thinking among   high school  students.   Their findings show        that  externalizing    learners'
thinking by  adding   scaffolding such  as sticky  notes made   it easier for  groups    to     share   and  reflect on  their
thoughts collaboratively. They note that being able to enlarge or zoom sticky notes facilitated joint reading and
group thinking, implying that touchscreen gestures and collaborative thinking are intertwined. Piper and Hollan
(2009)  compared    affordances  of tabletops and  pen-paper   material  for  supporting        scientific discussions   with
undergraduate students and noted that tabletops better supported understanding of abstract science concepts,
such as how a neuron fires. Learners used bimanual and collaborative whole-handed gestures over various parts
of the axon during scientific discourse. On the other side, prior work has also explored how to design direct-
touch interaction generally to support science learning activities around tabletops (Horn et al., 2009; Shaer et al.,
2011),  although  not explicitly  from the standpoint  of embodied     cognition. Horn        et al. (2009)   developed   an
information  visualization  tool  (Involv) for exploring  the   Encyclopedia    of  Life        to help    groups of   adults
effectively interact with large datasets on the tabletop. Shaer et al. (2011) explored tabletop interactions for
classroom science learning and found that multi-touch tabletops encourage reflection and foster collaboration.
However, these studies focus more broadly on the role of interaction in supporting science learning, rather than
looking at specific interactions that augment scientific discussion and collaborative learning. Our work builds
upon both of these lines of prior work. In addition to using embodied cognition as our conceptual framework,
we analyze the gestures that are co-occurring with instances of embodied cognition (as signaled by the users'
language) to understand the role of gestures in facilitating the collaborative science learning process.

Language as demonstration of embodied cognition
The theory of embodied cognition posits that learning occurs not only within the mind, but also in the body,
through the learner's movements and interactions with the environment (Wilson & Golonka, 2013). Lakoff and
Johnson (2003) discussed language as one signal of embodiment, specifically the use of "conceptual metaphors"
for space and place, outlining three types (Table 1). Orientational metaphors use spatial words for ideas that are
not inherently  spatial. Ontological  metaphors   allow  an abstract idea,   such   as an       experience,   to represent a
concrete substance like an object. A specific example is personification: giving objects human-like qualities so
we  can  relate experiences  through  human   characteristics. Metonymy    is  when  one        idea stands   in for another
similar idea, as when someone speaks about a location they are not physically in as "here". Wilson and Golonka
(2013)  have  critiqued  some embodied     cognition research,  however,   suggesting    that      linguistic expression   of
metaphors   found in  much   embodied   cognition  research (including    this paper)  is     at   best a  precursor  to true
embodied cognition, as language still occurs in the mind. The tasks investigated so far that Wilson and Golonka
Table 1: Examples of the three types of conceptual metaphors from Lakoff and Johnson (2003).

 Conceptual Metaphors                                              Examples
      Orientational        When we use "up" to describe feeling happy, e.g., "I'm feeling up." (p.16)
        Ontology           When a physical object is described as "talking" or "giving" as if it were a person, e.g., "His
                           religion tells him that he cannot drink fine French wines." (p.28)
       Metonymy            When we use the name of one location to describe our experience associated with the events at
                           that physical location, such as using the term "Grand Central Station" to mean "a crowded
                           place," e.g., "It's been Grand Central Station here all day." (p.32)

CSCL 2019 Proceedings                                    10                                                            © ISLS
suggest  represent   "true" embodied    cognition tasks,  such   as catching   a fly  baseball,  are   considerably   less
"representation hungry" than our more abstract task of learning from data visualizations. Our task deals with
thinking about data that is not concretely present in the learner's environment (Kiverstein & Rietveld, 2018).
For examine, one part of our task is understanding what the colors in the data visualizations depict, another part
is determining the pattern(s) of the colors, and a third part is ascribing meaning to the patterns. Decomposing
our meaning-making task into elements that engage embodied cognition as Wilson and Golonka do is a question
outside the scope of this paper. The specific conceptual metaphors learners use are likely to be influenced by the
specific science domain; however, our focus is not to categorize the types of conceptual metaphors used by
learners in this domain. Instead, we focus on linking the touchscreen gestures during embodied cognition to the
learning that is occurring. These insights will inform the design of future touch-interactive learning experiences
to more successfully capitalize on the affordances of embodied cognition in data visualization tasks.

Our prototype application for learning about Earth's ocean system
To study our research questions, our team, which consisted of both human-computer interaction experts and
learning scientists,  designed  a  touch-interactive tabletop prototype   for  science   data  visualizations to engage
family groups in collaborative meaning-making. The application ran on a Samsung SUR40 tabletop computer.
The resolution was 1920 x 1080 (55 DPI), and the display size was 40 inches, measured diagonally. We created
the interface with the Open Exhibits Software Development Kit (SDK) (1).
          To aid groups in the collaborative meaning-making process, we provided scaffolding (Stofer, 2016),
e.g., cognitive affordances that can help non-expert users understand the ocean temperature data visualizations
better, such as audience-appropriate color schemes and geographic labels. We used two map views with color
                                                                      scaffolding    designed    by    NASA     (2).  The
                                                                      visualizations   we    used  are  similar    to  the
                                                                      NASA       Earth   Observations    visualizations,
                                                                      which    have   been    deployed  in   nearly   100
                                                                      museums     worldwide       (3).  Our     prototype
                                                                      consisted   of   two    "maskviewer"      interface
                                                                      elements, each containing a set of ocean data
                                                                      visualizations from the year 2015 on top of a
                                                                      base Earth     map    with land   maps    and   blue
                                                                      oceans (4) (Figure 1). The first visualization
                                                                      used a pink-to-purple color scale that showed
Figure 1. Prototype design with the red/blue maskviewers ona base Earth map, along with a time slider (yellow bar) tochange months. Text labels have been enlarged for legibility.baseline ocean temperatures, and the secondvisualization used a red-to-blue color scale torepresenttheextremesofthetemperaturedifference(e.g.,anomaly)frombaseline
(Stofer, 2016). The   maskviewers     implemented    "layers" of  information  which     could be  zoomed,    rotated, or
dragged  to  allow   users  to flexibly control them  and  facilitate shared   communication.     To   help  participants
recognize the difference between the two maskviewers, we attached a temperature legend and title directly to
the maskviewers.     Each  visualization contained   six continent  hotspots   that, when    tapped,   would  pop-up   an
information box with content about El Niño, an ocean phenomenon affected by temperature, which could then
be zoomed, rotated, or dragged in relation to that continent. We chose the content for these information boxes
from  international   weather   and climate  organizations    (5) to  supplement     the content   shown    on  the   data
visualizations. Finally, to enable participants to explore how baseline temperatures or temperature anomalies
changed over a year, our prototype contained a "time slider" (yellow bar in Figure 1). When the user performed
a hold  (long tap) anywhere     on the  map, the time  slider would   pop  up,  and  the user  could   slide their finger
horizontally along it to change the months and observe the continuous temperature changes. The time slider had
tick marks for each month, labels at either end of the slider, and the current month displayed above the user's
finger. We used this prototype as a testbed to identify interactive touchscreen gestures that co-occurred with
instances of embodied cognition while groups engaged in a scientific discussion about ocean temperatures.

Participants and study design
A total of 30 participants in 11 family groups participated in our study (16 female). Each group consisted of at
least one child (ages 8 to 13, M: 10.07 yrs, SD: 1.49 yrs, 15 children total) and one parent or guardian (max
group size: four). We recruited participants via an email sent to a faculty list and flyers distributed at a local
science  museum.   Our  protocol   was   approved by  our  Institutional Review      Board.  After  obtaining   informed
consent  and  assent, we   instructed groups in  how  to  use the   think-aloud  process    (Greenberg  et al., 2011)  to

CSCL 2019 Proceedings                                     11                                                       © ISLS
expose what they were thinking while completing the learning activity. We also did a practice think-aloud with
them as a group to solve a two-column addition problem. During the study, we asked each group to perform
these four tasks in order, while thinking aloud: (1) Explore this interactive visualization as you would if you saw
it in a science museum. We are interested in seeing what people do. Tell us what you find out about the ocean.
(2) Open the information box for South America and tell us how the information there compares to the ocean
data displayed in general. (3) Find the Gulf of Mexico and tell us how ocean temperatures there change month
to month. (4) Find the Eastern Pacific Ocean basin and tell us how ocean temperatures in the basin compare to
long-term baseline ocean temperatures for that basin.
         The order of tasks increased in difficulty, with the aim that, as participants explored the prototype, they
would be    better  able  to answer    the  more    involved questions.   After  the  tasks, participants completed  a
demographics questionnaire. Most of our participants (over 85%) were frequent science museum visitors (e.g.,
three to twelve times per year). Each family group received a $30 grocery store gift card for participating.

Data collection and analysis
During the study, we video recorded participants using cameras placed at a side angle and directly overhead.
Session lengths (excluding break time between tasks) ranged from 14 minutes (min) 30 seconds (sec) to 27 min
45 sec (average duration: 21 min 43 sec, just under 6 min per task). We transcribed the groups' utterances from
the videos. One group spoke some Spanish during their interaction, and we asked a colleague to assist with
translation. To understand how specific interactions may be tied to particular instances of learning supported by
embodiment, we first identified the set of learning "episodes" in which groups' utterances signaled embodied
cognition (e.g., from the presence of conceptual metaphors). As mentioned, the three main types of conceptual
metaphors are metonymy, orientation, and ontology (Lakoff & Johnson, 2003). Three researchers reviewed the
transcripts to identify  utterances containing     these  conceptual metaphors.  We    then  analyzed the participants'
touchscreen gestures (e.g., tap, pinch to zoom, etc.) that co-occurred with these utterances during each learning
episode, to  help   us understand   the role    of  these gestures in  supporting collaborative  making-meaning,     as
facilitated by embodied cognition. Our team conducted a thematic analysis on these examples, in which we
discussed   themes  that emerged    in terms   of  which  types of gestures most  often   accompanied  the   conceptual
metaphors and seemed to be essential in affording embodied cognition during collaborative meaning-making.

Findings
We present two over-arching themes that illustrate the types of gestures groups used to facilitate collaborative
meaning-making when engaged in embodied cognition in our study.

Gestures for orienting the group
We observed that participants' interactions with the interface elements guided their thinking process and helped
them orient  themselves    and the  group    to the science  content   displayed on   the prototype.  We  saw  that the
physical affordances offered by the maskviewer element, such as being able to resize and move it around the
tabletop, allowed participants to focus the group's attention on how temperatures change at various geographic
locations.  Segment    1 (Figure 2) is  an  example   of  this  theme. We   identified it as an instance  of  embodied
cognition based on lines 4, 5, and 6. These utterances illustrate the use of metonymic conceptual metaphor, in
which P2 and P3 [Group597] are speaking as if placing themselves physically into the world represented by the
prototype: they are talking about the temperature change as if it was actually occurring, saying that "It's really
cold!" and using words "...out here in like the middle..." even though P2 is not in the middle of the ocean
himself. This segment shows how participants are dragging the maskviewer element to incrementally build their
understanding of the temperature visualization by promoting discussion about temperature variations at specific
geographic locations. In their gestures, P2 is dragging the baseline ocean temperature maskviewer element over
the Indian Ocean, as he says, "So check that out, that's very cold right there." P2 further drags the maskviewer
near the middle of the Pacific Ocean to compare the temperature patterns for the Indian and Pacific Oceans, as
he says, "So out here in like the middle of the Pacific Ocean, it looks like it's upwards of 88 degrees."
         We    also observed   that resizing    the maskviewer     helped focus  the  group's  attention  and facilitate
collaborative meaning-making. Segment 2 (Figure 2) is an example of this theme. We identified it as an instance
of embodied    cognition  based  on    line 2, which  exemplifies  a   common    type  of ontological metaphor  called
personification: P2 [Group912] treats the prototype as a "storyteller" by saying that the prototype is "telling"
them something. In their gestures, we see the group resizing the maskviewer to focus the group's attention and
facilitate meaning-making. Initially, P2 [Group 912] directs P1 to re-size the maskviewer in order to focus on a
specific geographic location, as he says: "Let me see Florida ...Ya, shrink it and put it like right over the north
east ... So, it tells what different temperatures are here or ...". At this moment, P1 is using the maskviewer

CSCL 2019 Proceedings                                       12                                                   © ISLS
element as a tool to help direct the group's attention onto Florida and analyze how the temperature changes near
it: "Pretty warm." The above examples show dragging and zooming gestures co-occurring with the utterances
signifying embodied cognition. Thus, we can infer that participants were using the maskviewer element as an
embodied lens for comparing temperature patterns through physical interactions with the prototype. Without the
maskviewer, the data visualization would encompass the entire interface, making it more difficult to direct the
group's attention and focus on subcomponents of the dataset before the group is ready to think at a higher level.
Dragging  and zooming   the maskviewer    and attending  to what  temperature       patterns they   reveal   for    different
geographical locations seemed to push participants to focus on a subset of the data at a time as their conceptual
understanding is constructed piece by piece. Therefore, future touchscreen interfaces for learning should support
interactions that bring key aspects of the science content gradually into focus in this way.

Cooperative gestures for facilitating collaborative meaning-making
Another theme  frequent in  our analysis  was participants  using cooperative       (multi-user)    gestures to     facilitate
collaborative meaning-making. Morris et al. (2006) defined cooperative gestures as interactions in which the
interface interprets the simultaneous gestures of more than one user as contributing to a single command. We
observed that the interaction constraints offered by our time slider, in that it required simultaneous gestures
(either using two hands or by two users at the same time), encouraged participants to actively participate in
collaborative meaning-making through cooperative gestures. Segment 3 (Figure 3) is an example of this theme.
We identified it as an instance of embodied    cognition   based on  lines      5 and  6, which  illustrates  the     use  of
metonymic   conceptual metaphor: saying   "...go to..." and "...go  ahead..."       to stand in for  manipulating      time
within the visualization, as one cannot physically move to the time under discussion. This segment presents an
example  of how  interface  elements that allow  users  to interact cooperatively       encourage   group    members       to
actively participate and contribute to the group's shared knowledge. In this example, initially, we see that P3 is
passively watching P1 and P2 [Group247] interact with the prototype and discuss their observations, instead of
actively participating and contributing to the group's understanding. We see later on (starting line 6) that since
P1 is utilizing both of his hands (right hand for holding the slider and left hand to point towards the temperature
pattern on the map), he looks at P3 to ask for help with changing months using the time slider by saying: "Go to
like, go to like May or June. That's April. Look how much warmer it's getting. That's June. Look how super
warm it is." Though P3 was just helping P1 change months, directly manipulating the prototype content helps
P3 to focus on the temperature trends P1 and P2 were discussing. While P3 changes the month on the slider, he

Segment 1
                                                            1. P1: Yeah, yeah let's do the Indian Ocean (dragging the
                                                              maskviewer to the Indian Ocean).
                                                            2. P2: So, check that out, that's very cold right there* (dragging
  P3                                                          the maskviewer towards Europe and the top of Africa)3. P1: Woooow!

          P2                 P1                P2

                                                            4. P2: Up in here it's pretty chilly too* (dragging the
                                                              maskviewer towards the Pacific Ocean).
P3                                                          5. P3: It's really cold6. P2: Find a really hot spot. So out here in like the middle of the
                                                              Pacific Ocean, it looks like it's upwards of 88 degrees 
                                                              [Group 597]
  P2                          P1       P2

Segment 2
                                                            1. P1: Let me see Florida (dragging maskview over Florida, then
                                                              enlarging maskview)
                                                               ... (other talk)
                                                            2. P2: Ya, shrink it (P1 shrinks maskviewer) and put it like right
                                                              over the north east* (P1 drags the maskviewer over Florida).
                                                              Here you go. So, it tells what different temperatures are here 
P2            P1                              P1              or ... (P1 zooms in map over Florida)3. P1: Pretty warm [Group 912]
  Figure 2. Example of a family group using the maskviewer to compare temperature patterns for geographic
  locations. The zoomed in view of the image on the right shows details of participants' interaction with the
 prototype. Participants' utterances signifying conceptual metaphors are noted: metonymic, personification.
          Utterances marked with an asterisk (*) co-occurred with the gestures shown in the images.

CSCL 2019 Proceedings                                   13                                                            © ISLS
starts contributing to the group's understanding by making inferences about how temperature patterns change at
different geographical locations, as he says: "But that's like in the middle. Here, let me show you." Gestures can
serve as a mechanism for cognitive offloading; that is, by taking up some of the cognitive efforts of attention
and focus, they allow a learner to focus cognitive resources on other aspects of a task such as drawing inferences
(Goldin-Meadow & Beilock, 2010). Furthermore, when working collaboratively on a complex task, reducing
individual cognitive   load and   encouraging    group  members     to exchange       knowledge      helps  in collaborative
knowledge-building (Kirschner et al., 2018). In their gestures, the participants work together in order to operate
the time slider. P2 holds his hand on the screen in order for the time slider to pop up, while P3 drags the slider to
different months. Thus, instead of having one participant focus their cognitive resources on operating the time
slider, the group collaboration reduces the amount of cognitive effort required by each participant. Then, the
participants can use more of their cognitive resources to engage in science discussion, seen in the utterance by
P3, "It's [temperature] near av- [average] that's where it was near average." The above example illustrates
that using cooperative  gestures  facilitates collaborative   meaning-making         by   encouraging    group  members       to
engage in a scientific discussion together in a more hands-on and active manner.
        We also observed that interacting collaboratively allows group members to build upon each other's
understanding of the temperature patterns. Segment 4 (Figure 3) is an example of this theme. We identified it as
an  instance of embodied    cognition based   on  lines 2, 4, 5, 6,    and 8,   which     illustrate the use   of      metonymic
conceptual   metaphor: participants talk as   if they are  actually experiencing         the time  and  temperature       change
portrayed  within the  prototype, by  saying, "it's almost   white  now".   In      this example,   we  see that       P1 and P2
[Group765] are collaboratively interacting with the time slider. As P2 changes months, she says, "It's definitely
warm. June, warmer." P1 further builds upon the group's understanding of temperature by saying, "Definitely
warm, it's getting closer to 95 [degrees]." The group continues to change months on the time slider and watch
the  temperature  change within   the  visualization.   Finally, P1    says,    "it's    almost   white  now."    Segment      3
demonstrates  that participants   are using   cooperative  gestures    to  discover       the   mapping  between        different
scientific variables of the visualizations (i.e., what temperature the colors depict and how temperature changes
with time). Both of these examples show cooperative gestures co-occurring with utterances signifying embodied

Segment 3
                                                          1. P2: No, that's a different. So wait no no no. So, oh yeah temperature
                                                            wouldn't, this isn't precipitation.
                                                          2. P1: Well yeah, but I mean they are definitely related*. The issue that I
                                                            see- (long tap gesture to activate time slider, then dragging time slider
                                                   P2       with middle finger)3. P2: So warm and humid? So, if we move this over here (dragging the
 P1              P3         P2        P1                    maskviewer).P34. P1: Yeah.
                                                          5. P2: And go to, it was May, June, and July right?  (deactivates time
                                                            slider by stopping long tap gesture) That it said El Nino was? 
                                                          6. P1: Was it? Okay. So, if we go till, yeah go ahead* (long tap gesture
                                       P1                   to activate time slider) and January the temperature isn't that much ofa difference. Go to like, go to like May or June (P3 dragging time
                                                            slider). That's April. Look how much warmer it's getting. That's June.
                                                            Look how super warm it is. 
                                                          7. P2: Yeah but you would think if it's warm up there (pointing gesture)
 P1                                                         then that would cause rain over here because of warm humid air. 
                P3          P2         P3           P2    8. P1: Depends on the wind pattern.9. P3: But that's like in the middle (pointing gesture). Here, let me show
                                                            you. (deactivates time slider by stopping long tap gesture) (other talk)
                                                          10. P1: July through October
                                                          11. P3: It's near av- that's where it was near average. [Group 247]
Segment 4
                                                          1. P2: Alright, what happens next? April.
                                                          2. P1: It's getting warmer*  (dragging the time slider) ...
                                                          3. P2: It's definitely warm. June, warmer...
                                                          4. P1: Definitely warm, it's getting closer to ninety-five. 
                                                          5. P2: Yeah that's pretty warm. 
                                                          6. P1: Oh! Dang, that's getting warmer. 
                                                          7. P2: July's pretty hot.
  P2                   P1                P2        P1     8. P1: It's almost white now  [Group765]
  Figure 3. Examples of family groups using cooperative gestures to construct collective working memory. The
      zoomed in view of the image on the right shows details of participants' interaction with the prototype.
   Participants' utterances signifying conceptual metaphors are noted: metonymic, personification. Utterances
                  marked with an asterisk (*) co-occurred with the gestures shown in the images.

CSCL 2019 Proceedings                                     14                                                               © ISLS
cognition. Thus, we can infer that participants are using these cooperative gestures as a mechanism for cognitive
offloading, and to focus their cognitive resources on meaning-making and drawing inferences related to how
both time and space affect ocean temperatures. Without the cooperative gesture required by the time slider,
participants would not need to coordinate to change the time period shown, making it more difficult to keep the
group's understanding aligned. Using cooperative gestures seemed to push participants to create a collective
working memory and encouraged group members to exchange knowledge. Therefore, future interactive learning
experiences should encourage interactions that afford group members to cooperatively manipulate the interface
elements like the time slider to facilitate embodied-cognition-supported learning.

Discussion, implications, and conclusion
Prior studies    offer  evidence   in favor    of using  touchscreen     interaction to  encourage     embodied-cognition-
supported learning activities around tabletops, e.g., (Lin et al., 2016). However, these studies did not analyze
what  specific   touchscreen     gestures people   make    when  engaged     in embodied    cognition,  and whether    these
gestures are tied to particular instances of learning. We seek to understand how to support and encourage the
types of gestures that accompany embodied cognition as revealed in learners' language. Based on Lakoff &
Johnson  (2003),    in  our study,  we    used linguistic  cues  from  groups'    utterances, in   the form of  conceptual
metaphors,    as signals    that embodied   cognition    was   occurring.    We  saw    instances  of  embodied   cognition
reflected in groups' language in the use of both metonymy and ontology (i.e., personification): by speaking as if
placing themselves physically into the world represented by the prototype and by speaking as if viewing the
prototype as a "storyteller". During these types of utterances, groups made a variety of touchscreen gestures,
including  attention-focusing    gestures  and    cooperative  gestures.  Groups   used  the  maskviewer    elements   as an
embodied   lens  to focus   the  group's  attention  on  local areas   of interest  and guide   their  thinking process,  by
moving   and    re-sizing   the  maskviewers      to reveal   the   underlying   temperature    patterns   across different
geographical locations. These interactions with the maskviewer pushed participants to focus on subsets of the
dataset at a time as their conceptual understanding is constructed piece by piece. In the future, designers of such
learning interfaces    could consider  ways    to  incorporate   similar  embodied   lenses  (not  limited to maskviewers
specifically) or  even    multiple  embodied    lenses  within   an application   that  bring key  aspects  of  the science
content  into focus.   These   lenses could    be manipulated    by users  to   explore and  compare   multiple  regions  of
multidimensional data visualizations through operations such as dragging and zooming (instead of just exposing
the entire dataset at once) to facilitate scientific discussion. Also, cooperative touchscreen gestures add value to
applications by increasing participation and enhancing the social aspects of an interactive experience (Morris et
al., 2006). Our findings indicate a positive role of such cooperative gestures in supporting scientific discussion
and collaborative meaning-making, from the viewpoint of embodied cognition. We saw that using cooperative
gestures supports groups in creating a collective working memory by directing the group's attention towards the
same  focus   point and   encouraging     group   members   to  contribute   to the  meaning-making     process.  Thus,   we
believe our findings add to the field's understanding of the role of cooperative gestures in terms of showing both
how   they are   involved   in embodied    cognition    and how   they   support  collaborative   meaning-making.     Future
tabletop learning environments could encourage similar interactions that afford group members to cooperatively
manipulate the interface elements to foster group learning, as in the case of multiple lenses above.
         The    instances   of   embodied   cognition   we   identified   in  participants'  language   suggest   that their
interactions  with  the   prototype   and  its data  were   immersive    and  sensory-based,   as  participants envisioned
themselves as if they were physically "in" our prototype. This finding is consistent with theories of embodiment
suggesting   that  learners'   thoughts   and   understandings    are  shaped    by  their   prior and   ongoing    physical
interactions  with  the   environment     (Wilson    &  Golonka,    2013). The    language   metaphors   and    gestures  we
identified support future work into examining how embodied cognition underlies the interpretation of scientific
content and processes in touchscreen-based learning environments. We disagree with Goldinger et al.'s (2016)
critiques of embodiment utility as a theory. Those authors do not discuss learning, and we believe the value of
being  able  to  design   affordances  to  support   learning  through   embodiment     will be strong.  Our    study  points
toward a need for a deeper investigation of designing with embodied cognition in mind, especially in the context
of science content and practices. Identifying how embodied cognition actually manifests in both language and
gestures in these types of experiences will allow us to design touchscreen interactions more targeted towards
fostering embodied-cognition-supported learning. In our future work, we will continue to analyze our dataset for
further themes, by considering other theories related to the role of both direct touch and in-air hand gestures in
facilitating collaborative meaning-making (Goodwin, 2007). While our study presented a new view of designing
tabletop interfaces    by identifying  the  nature   of gestures  that occur    during  embodied   cognition,   future work
should   evaluate   the effect   of encouraging      these  gestures  to  determine     how  well  they  support  learners'

CSCL 2019 Proceedings                                         15                                                      © ISLS
underlying  cognitive    processes     in a  real learning    environment   and  expand   this inquiry into other  learning
domains beyond data visualization and Earth's ocean temperature phenomena.

Endnotes
(1)  http://openexhibits.org/downloads/sdk
(2)  https://neo.sci.gsfc.nasa.gov/
(3)  https://sos.noaa.gov/What_is_SOS/sites.php
(4)  https://svs.gsfc.nasa.gov/2915
(5)  http://www.bom.gov.au/, http://www.grida.no/resources/7045

References
Abrahamson, D., Andrade, A., Bakker, A., Nathan, M. J., Walkington, C., Lindgren, R., ... Lindwall, O. (2018).
       Moving    Forward: In   Search     of Synergy     Across  Diverse   Views   on the  Role of Physical    Movement   in
       Design for STEM Education. In International Society of the Learning Sciences (pp. 1243­1250).
Cheek,  K.  A.   (2010).    Commentary:       A   Summary      and  Analysis    of Twenty-Seven     Years   of  Geoscience
       Conceptions Research. Journal of Geoscience Education, 58(3), 122­134.
Geller, T. (2006). Interactive Tabletop Exhibits in Museums & Galleries. IEEE COMPUT GRAPH, 26(5), 6­11.
Goldin-Meadow, S., & Beilock, S. L. (2010). Action's Influence on Thought: The Case of Gesture. PERSPECT
       PSYCHOL SCI, 5(6), 664­674.
Goldinger,  S.   D., Papesh,   M.   H.,   Barnhart,   A.  S., Hansen,  W.   A., &  Hout,   M.  C.  (2016). The    Poverty of
       Embodied Cognition. Psychonomic Bulletin and Review, 23(4), 959­978.
Goodwin, C. (2007). Environmentally Coupled Gestures. In Gesture and the Dynamic Dimensions of Language
       (pp. 195­212). John Benjamins Publishing.
Greenberg,  S.,  Carpendale,    S.,    Marquardt,     N., &    Buxton,   B.   (2011).  Sketching  User  Experiences:    The
       Workbook. Elsevier.
Horn, M., Tobiasz, M., & Shen, C. (2009). Visualizing Biodiversity with Voronoi Treemaps. In Proceedings of
       the International Symposium on Voronoi Diagrams (pp. 265­270).
Kirschner,  P.   A., Sweller,  J.,  Kirschner,    F.,  &  Zambrano    R.,   J. (2018). From    Cognitive   Load   Theory  to
       Collaborative Cognitive Load Theory. INT J COMP-SUPP COLL, 13(2), 213­233.
Kirsh, D. (2013). Embodied Cognition and the Magical Future of Interaction Design. ACM Transactions on
       Computer-Human Interaction (TOCHI'13), 20(1), Article No. 3.
Kiverstein, J.   D., &   Rietveld,     E. (2018).    Reconceiving   Representation-Hungry      Cognition:  An   Ecological-
       Enactive Proposal. Adaptive Behavior, 26(4), 147­163.
Lakoff, G., & Johnson, M. (2003). Metaphors We Live By (2nd ed.). Chicago: University of Chicago Press.
Lin, M., Preston, A., Kharrufa, A., & Kong, Z. (2016). Making L2 Learners' Reasoning Skills Visible: The
       Potential of  Computer   Supported       Collaborative    Learning   Environments.   THINK   SKILLS     CREAT,     22,
       303­322.
Morris, M. R., Huang, A., Paepcke, A., & Winograd, T. (2006). Cooperative Gestures: Multi-User Gestural
       Interactions for Co-located Groupware. In Proceedings of CHI'06 (pp. 1201­1210).
Piper, A.  M.,   Friedman,  W.,     &  Hollan,    J.  D. (2012).   Setting the  Stage  for Embodied    Activity:  Scientific
       Discussion around a MultiTouch Tabletop Display. IJLT, 7(1), 58­78.
Piper, A. M.,  &    Hollan, J. D.   (2009).   Tabletop    Displays  for Small   Group  Study:   Affordances    of Paper and
       Digital Materials. In Proceedings of CHI'09 (pp. 1227­1236).
Shaer, O.,  Strait,  M., Valdes,    C.,   Feng,   T., Lintz,  M.,  & Wang,     H.  (2011). Enhancing   Genomic     Learning
       Through Tabletop Interaction. In Proceedings of CHI'11 (pp. 2817­2826).
Stofer, K. A. (2016). When a Picture Isn't Worth 1000 Words: Learners Struggle to Find Meaning in Data
       Visualizations. Journal of Geoscience Education, 64(3), 231­241.
Vygotsky, L. S. (1978). Mind in Society: Development of Higher Psychological Processes. Harvard University.
Wilson,  A. D.,   &   Golonka,      S. (2013).    Embodied    Cognition    is  Not What    You  Think  It  Is. Frontiers  in
       Psychology, 4, Article No. 58.

Acknowledgments
This work   is partially  supported       by National    Science   Foundation  Grant   Awards   #DRL-1612485      and #IIS-
1552598. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the
authors and do not necessarily reflect these agencies' views. The authors also thank the Florida Museum of
Natural History for allowing us to recruit participants from their visitors. This work was conducted while author
Jeremy Alexandre was a summer intern at the University of Florida.

CSCL 2019 Proceedings                                           16                                                    © ISLS
