 Cognitive Strategies in Dynamic Modeling: Case Studies of Opportunities
                                             Taken and Missed

            Steven J. Stratford, PhD, Educational Studies, University ofMichigan (sstrat@umich.edu)

     Joseph S. Krajcik, Associate Professor, Educational Studies, University ofMichigan (krajcik@umich.edu)

         Elliot Soloway, Professor, Electrical Engineering and Computer Science, University ofMichigan
                                               (soloway@umich.edu)

INTRODUCTION
        New technologies offer new opportunities for engaging students in performances of higher-level thinking
[Perkins 1985; Salomon, Perkins & Globerson 1991]. The increasing presence of sophisticated computers in class
rooms, their expanding computational power, and advances in software interface design [Jackson, Stratford, Krajcik,
& Soloway in press] bring opportunities closer to every student, and make it possible for.them to engage in more
sophisticated and technology-intensive activities such as   dynamic  modeling.    Computer-based    dynamic   modeling
has long been recognized as a scientific activity that promotes deep system analysis, careful cause-and-effect reason
ing, and synthesis, along with  articulating explanations about   phenomena    and testing  and  debugging.   With the
development ofdynamic modeling environments for learners, we're investigating the extent to which students who
construct dynamic models take advantage ofthose opportunities for cognitive engagement. Technology can play a
supportive role in helping learners accomplish unfamiliar or cognitively difficult tasks [Guzdial 1995; Jackson et al.
in press]. In this paper we present two case studies ofpairs of students who, while creating models of their own
design, engaged in a range of cognitive activities. The cases we present represent students in the middle range of
abilities and performance: they did not struggle as they created their model, nor was what they produced exemplary.
Opportunities For Cognitive Performance Afforded By Dynamic Modeling
        A review ofliterature on   creating scientific models [Forrester 1968;  Miller  et  al. 1993; Stratford 1996]
suggests that there are many cognitive strategies in which a person may engage while creating a dynamic model of a
system:  analyzing  it   and identifying  main    components;    reasoning about   significant   causal relationships;
synthesizing components and relationships into a coherent model; testing and debugging the model; and verbalizing
explanations. We'll refer to these various strategies as Cognitive Strategies  for Modeling.    Additionally,  dynamic
models may be viewed as "artifacts" because they are "critiquable, sharable externalizations of knowledge" [Guzdial
et al. 1992]; they may demonstrate connections the modeler has made between concepts and may reveal what he or
she understands about the subject matter ofinterest [Wisnudel et al. in press; Lehrer 1993]. Analysis ofa model's
structure, content, and behavior may reveal evidence for such understandings.

Modeling Process: Analyzing, Relational Reasoning, Synthesizing, Testing and Debugging, and Explaining
        Analyzing is separating something whole into its constituent parts, and carefully examining those parts.
Analysis is important in dynamic modeling because component parts and meaningful relationships between those
parts are the building blocks from which a dynamic model is constructed. Analyzing a system may be carried out at
a shallow or a deep level and with varying degrees ofaccuracy, depending upon the goals, knowledge, and skills of
the modeler. As s/he creates a model, the modeler draws upon knowledge of and experience with other phenomena,
or upon relevant science content knowledge.
        Relational reasoning involves describing and defining relationships between parts of a system and drawing
inferences and conclusions about the causal nature ofthose relationships. In causal reasoning, the modeler looks for
a causal agent that causes changes in another part (or other parts) ofthe system, and then creates a mathematical rep
resentation in the model to mimic those changes. Causal reasoning is also applied when making predictions about
the effects ofchanges to a system or about how a model should behave when it is run. Reasoning may also be in
voked in judging or comparing the accuracy ofa model's output in relation to actual data or to the real world.
        Synthesizing   a model  refers to combining   parts together to  make   a  coherent whole.  After analyzing  a
system into parts and selecting appropriate relationships between parts, the modeler synthesizes them into a coherent
whole, literally "re-presenting" the phenomena inside the computer. The interrelationships between concepts, as
formulated by the   modeler  into  a model,  will likely reflect many aspects   of his or   her understanding   of the
phenomenon,   as  may  the behavior exhibited  by  the   model   when it   is run  (inasmuch    as such behavior   was
intentionally designed into the model).

 514
          Testing and debugging enables one to ascertain whether a model is working as expected. Testing may focus
· on a single relationship to examine its operation (or to gain a better understanding of how it works), or on running
an entire model   with    multiple    inputs and  outputs    in  order to observe    its overall   behavior    under   varying
circumstances. The modeler debugs the model when, as it is tested, it doesn't behave as planned or expected. S/he
may   recognize that  something   is  wrong,    articulate  the problem,  and suggest    and  try  some    possible  solutions.
Debugging may also lead to rethinking one's reasoning about how the model was constructed.
          Explanations  are  oral or  written verbalizations    of one's  understanding   of such   things    as the   system's
decomposition into parts, the mechanisms underlying a causal relation, or the reasoning for creating a model with
certain behavior. Explanations may be simple statements offact or definitions, reasons why something happens as it
does, or  explications  of how    some  causal  mechanism       works. They  may  be   incorporated   into  dynamic    models
through text fields,   or may  be     embedded   within     the choice of terminology     and   definition  of relationships.
Explanations may also be expressed orally within the context of creating a model.

Modeling Product: Structure, Content, and Behavior
         Dynamic models created with newer graphic-based modeling environments contain an observable configura
tion, or structure, of elements (such as stocks, flows or converters in the case of STELLA, or objects, factors, and
relationships in the case of Model-It). A model's structure may correspond with the modeler's understanding of the
phenomenon s/he is attempting to model. For example, if the modeler understands parts of a phenomenon to re
spond in a "chain reaction" fashion (that is, that an event can trigger a sequence of causal actions and reactions), then
the modeler is likely to create a model ofthat phenomenon with a structure that visually resembles a chain or a line.
Additionally, the model may or may not have a coherent structure in which all elements are unified into a model,
and that structure may indicate how well the person who created it understands the phenomenon.
         Because a model purports to represent some reality (some physical phenomenon, perhaps), it should con
tain representations of scientific content related to that phenomenon. If a scientist were to create such a model, s/he
might incorporate scientific content with a great deal of precision, completeness, and accuracy. On the other hand, if
a student created a model of the same phenomenon, s/he might include content with less precision, completeness,
and accuracy. The difference between the two is in how deeply and broadly each understands the phenomenon. Thus,
dynamic models may contain conceptual representations ofthe modeler's understanding ofa phenomenon.
         Dynamic models are designed to be run, that is, to take some values as input, to process, and to produce
output. The input of a dynamic model consists of some starting conditions (or, in the case of interactive modeling
software like Model-It, conditions that may be modified while the model runs). Processing is accomplished by a
"simulation engine" that operates on the inputs according to carefully defined algorithms. Output is displayed as
animations, graphs, or other visual displays. The output of a dynamic model, by its very nature, forms a pattern of
behavior that  can be  observed,  described,  and analyzed. The goal,     then, of creating   a dynamic    model    is that its
behavior mimic, in some carefully circumscribed way, the behavior ofthe phenomenon it is intended to represent. If
we assume that the behavior observed when a model is run is the behavior the modeler intended it to have, then it is
reasonable to infer that the model's behavior reflects the modeler's understanding ofthe phenomenon's behavior.
         It is evident  from the  above discussion     that opportunities for thinking   exist  in dynamic    modeling.   The
question, therefore, is not  if opportunities   exist, but  whether  students   take advantage     of those   opportunities to
demonstrate and develop understanding [Perkins 1985].

OUR STUDY
         For this paper, we draw upon data from a study of technology-enhanced project-based science in a ninth
grade science classroom located in a public, alternative school enrolling students with a range of racial, academic,
and socioeconomic characteristics (though the majority of students are white middle- to upper middle-class). The
science curriculum,   called Foundations     of Science    [Huebel-Drake, Finkel,    Stem &    Mouradian    1995]    combines
modem data collection and processing technologies (e.g., portable computers and visualization software) with long
term authentic inquiry projects (in this case, local stream water quality monitoring). One component of this study
has been to design, develop, and test dynamic modeling software, which has been dubbed "Model-It." Prior to their
experience with Model-It, these students were engaged in a multi-month ecological investigation of a local stream,
collecting, compiling, and reporting biological, physical, and chemical assessments. Their investigation involved
considerable computer use, using them to organize data and to write and present their reports. Therefore participants
entered this study with considerable prior knowledge of both stream ecosystems and computer technology.
         In order to help the reader better understand how dynamic models are created with Model-It, we present a
very brief description. To   create a model   of some   phenomenon     with Model-It,    the user  selects or creates  Objects
related to the phenomenon (for example, a golf course or an insect population), then defines quantifiable Factors of
those objects (for example, the measured level of dissolved oxygen in a stream or the size of a golf course). Related

                                                                                                                          515
factors are connected by Relationships, created using qualitative rather than formulaic representations. For example,
a direct causal relationship represented mathematically by     the  function y  =  x   may be expressed    qualitatively  in
Model-It with the phrase "as x increases, y increases by about the same." Verbal representations are visually linked
with graphical representations. The model as a whole may be viewed with an abstract representation (similar to a
concept map) called   a Factor Map.      Finally,  the  software provides graphs,   meters,  and  sliders  for interactively
Testing  the model.   Space  does not    permit  a complete   description  of the  program's  operation,   but   for a  more
complete description the reader may consult Jackson, Stratford, Krajcik and Soloway [in press].
         After spending.6 days learning how to create models with Model-It (assisted by a written guide), students
built stream ecosystem models based on several suggested ecological scenarios. They worked in pairs for 2 to 3 one
hour class periods, each pair sharing a computer and creating one model. Eight pairs (out of approximately 50) were
selected for a focus group. As the focus group students worked on their models, the computer screen output, along
with their conversations, was videotaped. The focus group students also completed a separate concept map on the
topic of "the water in  Traver Creek" prior   to  their modeling    sessions in order  to  provide an indication     of prior
conceptual knowledge of stream ecosystems.
         To analyze the videotaped data, we divided the modeling sessions into brief (two to fifteen minute) epi
sodes based upon    discrete activities (for instance,  students creating  a relationship  would   be one   episode,    while
testing to see if the relationship worked would be another). Then we described what happened in each episode,
categorizing   each episode  by event   and  task  (e.g., "creating relationships" or  "running   the model").    Then    we
analyzed each episode description for evidence that students were engaging in Cognitive Strategies for Modeling,
attempting to capture characteristics and quality oftheir engagement in the form of a high-level analytical narrative.
Finally, using the narratives of Cognitive Strategies for all episodes, we looked for patterns in students' modeling
process, illustrating those patterns by selecting examples from the descriptions, transcripts, and analytical narratives,
and articulating statements capturing the essence of the patterns we observed.      The goal of our final analysis write
up was   to compose  a case story about the strategies in   which each group engaged while        creating their model,   in
order to capture both the characteristics and qualities of students' modeling process.
         To analyze the model data, we evaluated each model's structure, content, and behavior. Structure was
evaluated   by a  model's form    (e.g., "star" or  "linear')  and   coherency   ("unified," "partially    fragmented,"   or
"fragmented"). Content was evaluated for factors, relationships, and written explanations in the model. Factors were
assessed according to the accuracy (according to generally accepted scientific knowledge) of their descriptions and
ranges, and according to breadth (how well the selected factors "covered" the scenario). Relationships were assessed
according to their accuracy of definition. Explanations were assessed as to their accuracy, depth (e.g., "descriptive,"
"correlational," or "causal"), and integration (inclusion   of relevant examples    or scientific content). Behavior     was
evaluated in two areas: behavior over time ("constant," "straightforward complex," or "sophisticated complex") and
fidelity (how closely the behavior ofthe model matched the expected behavior ofthe scenario or phenomenon).
         For this paper, we selected two out of the eight focus group        cases for discussion.  Both    cases represent
moderate levels of engagement in thinking strategies; that is, in both cases, students engaged in some but not all
Cognitive Strategies for Modeling, and they produced acceptable but not necessarily outstanding models. For each,
we provide   a brief profile of background    information,   such   as the topic  they chose  for  their   model,    a rough
assessment of prior knowledge, how       well they appeared to   work together  or relate to technology, and how        many
guide sections they completed. Then we discuss results related to their modeling process and product. Finally, for
each case, we present an interpretation in which highlights of their modeling process are related to their product.

CASE 1-CORY AND DAN
Profile
         Cory and Dan chose to create a model showing the impact, on a stream ecosystem, of urban runoff(such as
sewage overflow or urban street runoff) containing human and animal waste        [see Fig. l for a "Factor Map"      of their
model]. In their self-paced introductory work, they had completed all seven (through predator-prey models) before
working on their final model. They had comparable, moderate prior knowledge of stream ecosystems, as evidenced
in their fairly well-developed and interconnected concept maps. They seemed to work well together during their two
modeling sessions and seemed comfortable using the computer technology.
Process
         Cory  and Dan's modeling   process was     marked by    frequent  planning  and  thorough  analysis   of potential
factors and key relationships, but they seemed to engage in somewhat shallow reasoning, little causal explanation,
and minimal testing. They analyzed their model scenario in depth by generating many ideas for factors, but most of
their reasoning about how those factors were related was either shallow or nonexistent. When present, their reason
ing was more correlational than causal. However, they attempted to carefully reason out several key relationships in

516
their model, specifically, the relationships between "runoff," "animal feces," ''fecal coliform," and "water quality."
When they viewed the Factor Map, it seemed to catalyze their analysis and synthesis activities by helping them
think of additional           factors to  include and by     helping  them   focus  on  their  overall modeling    goal. Several  times
during their two modeling sessions they stopped to consider their course of action. On the other hand, they engaged
in very little verbal explaining during their conversations, included no written explanations in their model during
the two modeling sessions that were recorded. Their few explanations seemed to be based more upon correlational
reasoning, with only a few instances in which they discussed causality between factors. They tested their model a
few times, but engaged in very little conversation about what they were observing, neither about whether the model
was behaving as they expected nor whether the model was behaving realistically. They made no attempt to identify
any problems with their model or to "debug" it.

          animals

      animal  dropp1no;,s

 '---------------------=Ill                                                                                                        Ill
              Figures 1 & 2. Cory & Dan's model, left; Rachel and Sam's model, right (Factor Map representations).
Product
             Cory and Dan's model was coherent and generally accurate, but a few aspects ofits behavior were faulty. In
their model        [Figure     1 ], "animals,"    "heavy rainfall," and "rain"   affect "fecal coliform,"  "street runoff' and   "water
quality," which in turn affect the "mayfly" population. The qualitative relationships between those factors was as
expected according to the scenario they were attempting to model (e.g., higher levels of fecal coliform cause water
quality to decrease, etc.). Their model's structure was somewhat like a "funnel," with inputs from "animals" and
"rain" and "heavy rainfall" eventually affecting the single factor "fecal coliform," which in turn affected both "water
quality" and the macroinvertebrate population (the grouping of three factors to the right, "count," "rate of growth,"
and "rate ofdecay." There were no descriptions for factors; consequently, factor accuracy could be assessed only on
the names and ranges            of factors.     Some  ranges (e.g., for fecal  coliform)   were accurate   according to  water quality
definitions, and others (e.g. "animals") were defined with default ranges assigned by Model-It. Their relationships
were qualitatively accurate. It should be noted here that during their two class periods of modeling, they did not
type in any explanations into Model-It; however, their final model contained typed explanations, so they obviously
typed  those       explanations       in outside    of class. Most    of their written  explanations   were accurate but   a  few were
questionable or wrong; some explanations were deeper than others and contained expressions of causality.
             Their        model behaved      in a straightforward   complex    manner:  raising the  "animals"   factor  made  "animal
droppings" go up, which made "fecal coliform" rise, which made quality go down; more "rain" led to increased
''urban runoff," an increase in "fecal coliform," and a decrease in quality. The mayfly population, however, did not
appear to work properly-it always rose to its maximum and stayed there, regardless of the how the independent
variables     ("animals"        or   "rain') were   set. The  overall effect of  the model    (changes  in fecal coliform  leading     to
eventual effects upon the benthic macroinvertebrate population) is questionable, because the presence of more fecal
coliform      may          correlate with    fewer  benthics  depending  on    other    conditions, but  fecal coliform   bacteria     by
themselves don't necessarily cause an adverse effect upon benthics).
Interpretation
             Cory and Dan's frequent planning and thorough analysis helped them to create a model with fairly rich and
generally accurate content. Their focus on several key relationships helped them organize their model into a coherent
structure. However, their            overall  lack  of discussion   of causality in  their relationships,  perhaps  combined   with    a
faulty conception of how macroinvertebrates are affected by fecal coliform, may have led to the flaw in the overall
synthesis         of their   model.   Also,     the few  times they   tested their model   were  probably   insufficient  for them     to

                                                                                                                                   517
understand how their model behaved, which may account for the questionable mayfly population behavior.

CASE 2-RACHEL AND SAM
Profile
          Rachel and Sam chose to create a model showing how benthic macroinvertebrate organisms in a stream can
be indicators of water quality [see Figure 2]. They completed all seven sections of the guide. They were pretty well
matched in terms of prior knowledge; both of their concept maps were accurate, each containing seventeen concepts
and a moderate level of connectivity. They seemed to work together well, and generally attended to the modeling
task. There were  a few occasions, however,  in which    they had difficulty with    the operation of  the computer      or
Model-It and became frustrated when the computer didn't do what they thought it should.
Process
          Rachel and Sam's   modeling process was characterized by   analyzing with   the  help  of  a reference  source,
both causal and non-causal reasoning and explaining, using the Factor Map as an aid to synthesizing their model,
encountering procedural problems they couldn't solve, and engaging in testing with which they were not satisfied.
As they analyzed their scenario, they frequently referred to their water quality text. They used the text to look for
potential factors and to refresh their memory about    how    potential factors were  related to   other   factors. Their
modeling  sessions were marked by   both causal and non-causal reasoning and       explanations. That  is, some     of the
relationships they created were causal, and they discussed them appropriately, but other relationships they discussed
were not causal, but they talked about them as if they were causal. For example, they discussed the relationship
between   "temperature" and macroinvertebrate populations  by  saying that  temperature   affected "water  quality"    and
quality  then affected macroinvertebrates. Although there may     be a causal relationship   between   temperature     and
macroinvertebrates, the one they were discussing was correlational at best. They used the Factor Map as an aid to
synthesizing their model: it helped them see  what they   had  so far and   helped them   find unfinished portions.    On
several occasions in their modeling sessions, they encountered procedural problems that they were unable to solve.
For example, on several occasions they attempted to create relationships to factors of a population, but in each case
experienced some difficulty figuring out how to do it. Finally, they seemed to be unsatisfied with attempts to test
their model. Each time they tried to test it, the populations whose behavior they were trying to test did not seem to
behave in a realistic way as they expected, and they never did identify the bugs they thought were in their model.
Product
          The model   they creat to   show   how water quality factors affected rnacroinvertebrates [Figure 2]         was
structured so that "total solids," "pH," and "fecal coliform" affected two benthic populations, Taxon l and Taxon 3
(the third benthic population, Taxon 2, also acted as an independent variable,  directly affecting the  Taxon   1   rate of
growth). Factors were accurately described and factor ranges were either accurate according to water quality defini
tions or  else were defined using Model-It's default. Most    relationships and their   explanations were  accurate,     al
though several were questionable, such as the connection between total solids and temperature (they explained that
more total solids would increase heat absorption, thus increasing the temperature of the water) or the connection
between Taxon 2 and Taxon 1 mentioned above. Some of their explanations expressed causality but others did not.
Their model exhibited straightforward complex behavior with     moderate    fidelity to  reality. The Taxon   1   benthic
population responded as expected to changes in total solids, pH, and fecal coliform, but the Taxon 3 population did
not respond to any changes in the independent variables. Although Taxon 3 is the most pollution tolerant of all
three taxa, at high levels of pollution they should be adversely affected; their model doesn't reflect that behavior.
Interpretation
          Rachel and Sam's process of searching an information source during their analysis helped them create a
model with mostly accurate factors and relationships. Their reference to the Factor Map helped them synthesize a
unified model. However, their somewhat mixed causal and correlational reasoning may have led to the creation of
several questionably causal relationships. The difficulties they encountered with testing and with connecting factors
with populations may have resulted in a model with behavior that was only partially realistic.

DISCUSSION
         The two cases presented here illustrate how, when using Model-It to create dynamic models, students take
advantage of some opportunities for thinking but don't take advantage of others as much as we'd like them to. Both

518
cases   show  them    talcing   opportunities  for   analyzing an ecosystem scenario by         thinking of relevant    factors, using
both prior knowledge and          classroom     reference   sources   to  generate   ideas.  They   also   took  the  opportunity   to
synthesize a model, as evidenced by the relatively coherent models they created, and as seen in the ways they made
use ofthe visual representation oftheir model in the Factor Map. Opportunities for reasoning and explanation were
inconsistently talcen: in one case, the students expended effort reasoning about a few key relationships, but did not
explain the causal mechanisms upon which many oftheir relationships were based; in the other case, the students
mixed causal and correlational reasoning, consequently producing a model with several questionable relationships.
Finally, neither group ofstudents effectively tested their model, with one group engaging in superficial testing, and
the other encountering difficulties they didn't understand and apparently giving up.
         The results should not be surprising. We should not expect students to talce part in the full range of Cogni
tive Strategies for Modeling in their first attempts to create models ofa complex system.                      Based on the evidence
presented, students took part in some ofthe Cognitive Strategies for Modeling that we would hope they could per
form; moreover, they created reasonable models.             However, to promote students' participation in the full range of
Cognitive Strategies for Modeling, enabling them to create models with coherent structure, rich content, and realis
tic behavior,    we still need   to think    of more   ways   to encourage and support their        efforts. Several possibilities are
suggested here, in two areas: software support and instructional support. First, in order to support articulating causal
explanations in the software, we suggest that the relationship-construction tools ofModel-It should not be blank,
but should include text fields "seeded" with a causal phrase, for example, "Golfcourse size affects stream fertilizer
runoff because _." This would focus students' attempts to type explanations upon the causal mechanism. Second,
in order to support testing and debugging in the software, we suggest that the software monitor students' actions
with the software and when some number ofrelationships have been created, to prompt the user to test the model.
In addition, we also suggest that students be prompted to reflect upon the results oftheir testing and to record pre
dictions, observations, and conclusions. Third, to support causal reasoning, teachers should discuss the differences
between causes and correlations and provide opportunities for students to identify and articulate statements of both
kinds. Finally, to support more accurate and coherent models with higher fidelity behavior, we suggest that models
should be critiqued and revised by other student groups, by teachers, or by experts in the field. This will allow ad
ditional opportunities to engage in a cycle of development and revision ofnot only the model itself, but of stu
dents' understanding.

REFERENCES
Forrester, J. W. (1968). Principles of systems. Cambridge, MA: Wright-Allen Press.
Guzdial, M., Soloway, E., Blumenfeld,        P., Hohmann,   L., Ewing, K, Tabak,     I., Brade, K. (1992).   The future   of computer
    assisted   design: technological     support  for  kids building   artifacts. In  D. Balestri, S. Ehrmann,  & D. Ferguson    (Eds.),
    Learning to Design, Designing to Learn. Washington: Taylor & Francis.
Guzdial, M. (1995).    Software-realized     scaffolding  to  facilitate programming     for  science learning.  Interactive  Learning
     Environments, 4 (!), 1-44.
Huebel-Drake, M., Finkel, E., Stem, E., & Mouradian, M. (1995). Planning a course for success. The Science Teacher, 62 (7),
     I 8-2 I.
Jackson, S., Stratford, S. J., Krajcik, J. & Soloway, E. (in press). Making dynamic modeling accessible to pre-college science
    students. Interactive Learning Environments.
Lehrer, R. (1993). Authors of knowledge: patterns of hypermedia design. In Computers as Cognitive Tools. In S. D. Lajoie,
    S. (Eds.) Hillsdale, NJ: Lawrence Erlbaum Associates, 197-228.
Miller, R,    Ogborn,  J., Briggs,  J., Brough,  D., Bliss,   J., Boohan,   R, Brosnan,   T., Mellar,  H.,   & Sakonidis,    B. (1993).
     Educational tools for computational modelling. Computers in Education, 21 (3), 205-261.
Mitchell, M. K. & Stapp, W. B. (1994). Field manual for water quality monitoring: an environmental education program for
    schools. (8th ed.). Dexter, MI: Thomson-Shore Printers.
Perkins, D.   N. (1985).  The    fingertip   effect:  How   information-processing      technology    shapes   thinking.   Educational
    Researcher, August/September, 11-I 7.
Salomon, G., Perkins, D., & Globerson, T. (1991). Partners in cognition:             Extending  human   intelligence  with   intelligent
    technologies. Educational Researcher, 20 (3), 2-9.
Stratford, S. J. (1996).  Opportunities    for thinking: constructing    models   in precollege science  classrooms.  Paper  presented
    at the Annual Meeting of the American Educational Research Association. New York, New York, April, 1996.
Wisnudel,  M.,   Stratford, S., Krajcik, J., Soloway,  E. (in press). Using technology to support    students'   artifact construction
    in science. To appear in Linn, M.C. (Ed.) Int'l Handbook of Science Education. Netherlands: Kluwer.
Acknowledgments
         We would like to extend our appreciation to members ofthe Highly Interactive Computing (HI-C) research
group and to teachers and students at Community High School in Ann Arbor, for their participation and support.
The National Science Foundation (RED 9353481) and the University ofMichigan supported this research.

                                                                                                                                  519
