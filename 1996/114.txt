            The      Role       of    Student       Tasks       in   Accessing          Cognitive            Media      Types

                           Michael Byrnet, Mark Guzdial,* Preetha Ram**, Richard Catramhonet,
                             Ashwin Ram*, John Stasko*, Gordon Shippey*, Florian Albrecht*
                                                    *College of Computing
                                                     tSchool of Psychology
                                                Georgia Institute of Technology
                                                     Atlanta, Georgia 30332
                                                        **Emory University
                                                    Department of Chemistry
                                                         Atlanta, Georgia
    byme@cc.gatech.edu, guzdial@cc.gatech.edu, chempr@emory.edu, rc7@prism.gatech.edu, ashwin@cc.gatech.edu,
                                stasko@cc.gatech.edu, shippey@cc.gatech., florian@cc.gatech.edu

            Abstract:      We believe  that identifying media by  their cognitive roles   (e.g., definition,  explanation,
           pseudo-code,    visualization) can improve   comprehension  and usability   in hypermedia    systems  designed
           for   learning. We refer to  media links organized   around their cognitive  role as cognitive      media types
            [Recker, Ram, Shikano, Li, & Stasko, 1995].     Our hypothesis is that the goals that stu_dents bring to the
           learning  task  will affect how they  will use the hypermedia support   system    [Ram & Leake,      1995]. We
            explored student use of a  hypermedia   system based  on cognitive  media  types     where students performed
           different orienting tasks: undirected, browsing in order to answer specific questions, problem-solving,
           and problem-solving with prompted      self-explanations. We   found significant differences  in   use behavior
           between problem-solving and browsing students, though no learning differences.

Introduction
        Hypermedia is typically oriented around physical media types, for instance, text, video, graphics, and audio. The
World Wide Web's use of Mime types, which are based on physical characteristics, is a good example [Berners-Lee, Cailliau,
Luotonen, Nielsen, & Secret, 1994]. We believe that identifying media by their cognitive roles (e.g., definition, explanation,
pseudo-code, visualization) can improve comprehension and usability in hypermedia systems designed for learning. We refer
to media links organized around their.cognitive role as cognitive media types. In a recent study, students using a hypermedia
system organized around cognitive media types performed better on a post-test on the content in the system than students
using a hypermedia system organized around physical media types [Recker, et al., 1995].
        However, we also believe that the goals that students bring to the learning task will affect how the students will use
the hypermedia support system [Ram & Leake, 1995]. Certainly, the system can be tuned to different tasks. Researchers
working on the Superbook hypermedia system          found  that different interfaces better  supported    either browsing   activity or
searching for a particular kind of information  [Egan, Remde, Gomez,         Landauer, Eberhardt,   et   al.,  1990].  Our question was
whether setting different tasks for students (and thus, different goals) would affect (a) how students utilized cognitive media
types and (b) student learning.
        In particular, we explored two kinds of task differences:
                 Problem-solving    vs. browsing:   We hypothesized that students who        had  a specific   problem  to solve would
                 access the hypermedia organized around cognitive media types differently and would learn more than students
                 who were more or less simply browsing for interesting content.
                 Self-explanations: The work    of Chi    and others   [Chi,  1992]  [Chi,   Bassok,     Lewis,  Reimman,   &  Glaser,
                 1990] suggests that students who self-explain the content and their actions learn more effectively than those
                 who do not self-explain. We hypothesized that we might be able to prompt students to self-explain based on
                 their activity in the hypermedia system to achieve gains in learning. Our efforts here are similar to those of
                 [Bielaczyc, Pirolli, & Brown, 1991] [Bielaczyc & Recker, 1991] who also engineered prompts to encourage
                 and teach  self-explanation   behavior.   Our   additional  question   was      whether students  prompted   for   self
                 explanations would access the cognitive media types differently than students without such prompts.
        Self-explanation prompts are less interesting in a browsing condition than in a problem-solving condition because
there is less student activity to· explain.  Therefore,  we only   used   self-explanation   prompts     as  a second  problem-solving
condition. We note, however, that the prompts in a self-explanation condition can also serve to direct student exploration.

    114
To explore that role, we had two browsing conditions - one without direction. and one directed to answer specific questions.

Methods
        In an   attempt to   test some   of  our ideas about   cognitive media    types, students' tasks, and self-explanation,    we
designed instructional software for teaching students how to solve problems involving the determination of molecular shape.
This subject is   particularly difficult for introductory   Chemistry    students so the   potential benefits   in  this domain    are
considerable.
        The software, which we called ChemLab, is based on an outline of the problem solution procedure developed by our
domain expert (P. Ram).      The steps in the procedure are laid out in a map that students can follow to arrive at a solution.
There are 22 steps in the procedure that the students could examine. Each step contained up to four cognitive media types:
        1) Definitions.   Key   concepts relevant to   this step and the operations required  at   this step of the procedur were
presented here.
        2) Examples.    Concrete examples of this step in the procedure or the key concept in the step were presented here.
        3) Worked problems.       This was generally a "before and after" presentation in which the students saw a partial
solution before the step was performed and then after it was performed. Explanations for the operations were also present.
        4) Problem sets.     This was similar to a worked problem but provides an opportunity to test one's knowledge.           The
screen presents a "before the step" situation and asked the learner to execute the step in their head or on scratch paper.       The
learner could request that the solution then be shown to verify their solution.
        Because this particular domain is very visual, ChemLab makes extensive use of figures along with the text [see Fig.
1]. However, use of other physical media types were limited. The semi-public computer cluster environment constrained our
ability to make use of sound and time constraints made construction of animations impossible.        Future  versions of  ChemLab
will make more extensive use      of animation   to illustrate the more  dynamic   content, though   the  use of   sound  is still an
unresolved issue.
Subjects

        The   subjects in the ChemLab     experiment were approximately 80 undergraduates who were           taking an   introductory
Chemistry course at Emory University. They participated in the experiment for extra-credit. Subjects had attended lectures
(scattered over a two week period) in their course which addressed the material covered in ChemLab.
        The participant sample was a strong group of students with a self-reported       grade-point average of 3.4  out  of 4.0   and
mean SAT scores of 571 on the Verbal scale and 663 on the Quantitative scale. Thus, it was a very capable group that used
the ChemLab software. Random assignment procedures were effective in that there were no mean group differences on any of
these participant variables.
Apparatus/Materials

        Subjects were given paper guides to help them navigate the ChemLab software.               The software itself was run on
Macintosh Centris personal computers and developed in Apple's HyperCard (version 2.3).

                                                                                                                          115
               Full Map                                        When the central atom has four electron pairs around it,
                                                              its geometry is tetrahedral and the angle between the
                               Count                           bonds (or between any bond and any electron cloud of
                              electronpairs                    the lone pair) is 109.5°. A tetrahedron is  a3-dimensional pyramid shape with four sides.
                           -                                   If one of the electron groups around the central atom isa lone pair, the total geometry of the molecule is stilltetrahedral but the shape of the molecule is trigonalpyramidal.
                                                               If two of the electron groups are lone pairs (and not
                                                               bonds), then the geometry of the molecule is bent.

                        Show me a/an...
            ® Definition:            Tetrahedral

            Q EHample:               NH:s
           0 Worked Problem:         CH4
           0 Problem Set:            H20                           Tetrahedral
           - Go:-;:==::.-=.-,.-_:-_-_-. ,.
            [Back) [ NeHt ) ( Work )          Ask:  ( Whif]

                                                     Figure 1: ChemLab screen shot
Design

       All subjects had access to the same instructional material in the software and were given the same post-test after
using the software.   The  post-test · was   a short quiz designed by   the regular Chemistry instructors for the  course. The
ChemLab software also kept a log ofmouse clicks so it was possible to analyze browsing patterns for the subjects.
       The presentation of the ChemLab software was in slightly different contexts for different subjects.    There were four
presentation groups:
       1) Passive watching. In this condition, subjects were given access to the ChemLab instructional material and little
guidance.  They were instructed to "work through the materials until you are pretty sure you     are prepared to   solve   some
problems." This is, in essence, the control condition.
       2) Directed watching.  In this condition, we wanted to give the subjects some learning goals  while browsing    through
ChemLab.    Thus, subjects were given a series of questions to answer to help guide their browsing.           These questions
concerned different aspects: some were oriented towards the procedure (e.g. "how do you do X?" or "what do you do after step
Y?"), some were oriented toward specific content (e.g. "what's the chemical formula for Hydrazine?"), some were definitions,
and many were purposely obscure things that subjects would have to find as distractors.
       3) Problem solving.    Subjects      in this condition could not only browse  ChemLab,   but were   asked  to solve two
specific molecular shape problems. As they were doing so, they were allowed to use ChemLab as a resource to help them
solve the problems.  This condition was included to encourage subjects to spontaneously generate learning goals directly
relevant to solving real problems.
       4) Problem solving with prompting.           This condition was identical to condition 3 with one addition:   at various
points while subjects solved the problems, they were prompted to explain what they were doing and why.        It was hoped that
this manipulation would increase reflection and self-explanation.
       Subjects in the two problem-solving conditions were asked to use special software called the Molecule Construction
Kit to solve the problems [see Fig. 2]. This made the whole environment more self-contained and gave us the opportunity to
examine partial solutions.

    116
                Molecule Construction Kit                                                                       Periodic Table

                                              H                          ··      0  ··
                               H        -     IC               - C       -       IIC-      ··N     -    H
                                                                  I                         I
                                                                 H                         H

                Complete the Lewis Structure for this molecule

                                               Figure 2: Molecule Construction Kit
Procedures

        Participants first filled out the consent fonn. After this, they were allowed to spend as much time as they wanted
with the ChemLab software. The software also prompted them to report their grade-point average and SAT scores. Once they
were satisfied with the time  they had spent using ChemLab, they received a         brief questionnaire asking them to rate     their
confidence in their ability to solve molecular shape problems. Once they had completed the questionnaire, they were given the
post-test on paper. They were proctored during the post-test and given unlimited time to solve the problems on the quiz.
        An example problem on the post-test was: Arrange the following species in order of decreasing F--A--F bond angles,
where A is the central  atom: BF3, BeF2, CF4.  The correct solution was BeF2, BF3, CF4.
Results

        Browsing patterns were clearly affected by the condition to which subjects were assigned. Subjects in the problem
solving conditions visited an average of only 5.4 of the 22 steps, while subjects in the browsing conditions visited an average
of 17.6 of the 22 steps, and this difference is reliable (F(l,74) = 82.70,p < 0.001). This is an extremely large difference; on
average, subjects in the problem-solving conditions saw less than one-fourth of the steps in the procedure while browsing
subjects saw 80% of the steps. This difference carried into the number of visits to the different media types; for all four media
types, subjects in the browsing conditions had more visits than subjects in the problem-solving conditions.
        This may have been compensated for by the amount of time subjects spent looking at the screens when they were
presented. An average visit to a screen for the subjects in the browsing conditions lasted 36 seconds, while for the problem
solving subjects the average visit lasted 86 seconds. Again, this difference is reliable (F(l,43) = 12.10, p = 0.001) and is quite
large; subjects in  the problem-solving conditions spent          almost two and a  half  times as long  looking at a  screen     of
infonnation than subjects in the browsing conditions. The average total time (not including reading the initial instructions)
for the experiment was 17.85 minutes. This was weakly related to post-test score (r(77) = 0.62, p = 0.058).

                                                                                                                        117
        While there were clear effects on      browsing   patterns for problem-solving    vs.   browsing,   there were   no effects
associated with prompting except the total amount of time spent using ChemLab. The source of this difference is probably
the extra typing subjects in the prompted condition did in answering the prompts. In any case, the difference here, though
reliable, was not large.
        Unfortunately, there were no reliable differences in post-test scores between the groups. Table 1 summarizes the four
groups' post-test scores. Overall, subjects    did reasonably well on  the  post-test, averaging 8.15    points of a possible  10.
Interestingly, self-reported confidence (the rating taken right before the post-test) was only weakly correlated with the actual
quiz scores, r(80) = 0.20,p = 0.07; self-reported confidence was more highly correlated with SAT Quantitative scores r(69) =
0.28,p = 0.02 even though SAT scores were not correlated with quiz performance.

                                   Grouo        A vera2e    Score      Standard     Deviation
                                     1                 8.34                     1.91
                                     2                 8.34                     1.26
                                     3                 8.00                     1.52
                                      4               7.96                      1.59
                                           Table 1: Summarizing Four Groups' Post-Test Scores
Discussion

        The lack of differences between groups' post-test scores are surprising - our hypotheses predicted differences both in
browsing behavior (which was observed) and in learning performance (which was not). However, this lack of difference might
be attributed to the complexity of the task (or rather, lack thereof) and student ability. The students were very capable, and
even those who received relatively little support from the hypermedia system (i.e., the problem-solving group students who
saw less than 25% of the steps in the procedures) performed very well on the post-test (8.15 out of 10 points, with a standard
deviation of less than 2.0 - almost nobody got less than half the quiz right), which may also suggest a ceiling effect The
students were doing so well that there was little opportunity to discover group differences.
        More interesting is the dramatic differences between use patterns among the groups. Students who were trying to
solve a problem visited fewer cognitive media types but spent more time studying the media that they did visit. It may be that
if the task were more complex and the students needed more of the information in the hypermedia system to solve the task, as
opposed to having  seen much   of   the information  in class already, the differences in use  behaviors  might   result in greater
differences in performance on the learning task and on the post-test.
        The observation that   problem-solving     students spent more time   studying  the   screens is in marked   contrast with
prevailing wisdom in hypermedia design. Generally, hypermedia designers reduce the amount of text in their systems in favor
of more "glitzy" media, such as graphics, sound, and video, arguing that users are not willing to spend the time reading text.
In fact, some researchers have even claimed that reading will become an obsolete skill as multimedia computers become more
common [Papert, 1993). While the case and value of "glitz" is real and important, our data suggest that if users are actively
trying to problem-solve and use the content in a hypermedia system, they will invest the time on the content, perhaps even
reading the text
        Our results offer no insights on the question of self-explanation prompts. We have no evidence of differences in
learning performance nor   of differences   in use  behavior  due to self-explanation  prompts,  other   than the  time  differences
required to respond to the prompts. It may be that if we were to repeat the experiment with a more complex task and with
more necessary hypermedia content, then students might be more motivated to respond to the prompts more thoughtfully,
resulting in greater learning differences.
Conclusions

        While we have not been able to support our hypotheses about self-explanation and about learning effects, our results
do support the hypothesis that a difference in user task will cause a difference in browsing behavior.    Thus, there are several
paths of interest for future work:
        ·         Certainly, repeating the experiment with a more complex task where hypermedia support is more critical
                  might provide useful insights on the learning question.      Since the problem-solving students spend more
                  time   studying content but  view  less of the  overall system,  good navigation    mechanisms   that  lead these
                  students to the necessary content   will   be a critical feature of  future systems  [Shippey,  Ram,   Albrecht,
                  Roberts, Guzdial, et al., 1996].
                  Time spent on the content suggests an interesting avenue for an exploration where the relative amounts of

   118
                 different cognitive media types are varied to investigate further the relationship between media types, task,
                 and learning.
                 Varying the kinds of physical media corresponding to cognitive media and varying the kinds of interaction
                 with the system offers another interesting interface component whose use may vary with task.
References

         [Bemers-Lee, Cailliau, Luotonen, Nielsen, & Secret, 1994] Bemers-Lee, T., Cailliau, R., Luotonen, A., Nielsen, H.
F., & Secret, A. (1994). The world-wide web. Communications of the ACM, 37(8), 76-82.
         [Bielaczyc, Pirolli, & Brown, 1991]  Bielaczyc, K., Pirolli, P., & Brown, A.  L. (1991). The effects of training in
explanation strategies on the acquisition of programming skills Technical Report. University ofCalifornia at Berkeley.
         [Bielaczyc & Recker, 1991] Bielaczyc, K., & Recker, M. M. (1991). Leaming to learn: The implications of strategy
instruction in computer programming. In Proceedings of the Conference on Learning Sciences
         [Chi, 1992] Chi, M.    T. H. (1992). Conceptual change within    and across ontological categories: Examples   from
learning and discovery in science. In R. Giere (Eds.), Cognitive Models of Science: Minnesota Studies in the Philosophy of
Science (pp. 129-160). Minneapolis, MN: University of Minnesota Press.
         [Chi, Bassok, Lewis, Reimman, & Glaser, 1990] Chi, M. T. H., Bassok, M., Lewis, M., Reimman, P., & Glaser,
R. (1990). Self-explanations: How students study and use examples in learning to solve problems. Cognitive Science, 13,
145-182.
         [Egan, Remde, Gomez, Landauer, Eberhardt, et  al.,   1990]    Egan, D., Remde, J. R.,   Gomez, L.,   Landauer,  T.,
Eberhardt, J., & Lochbaum, C. (1990). Formative design-evaluation of SuperBook. ACM Transactions on Office Information
Systems, 7, 30-57.
         [Papert, 1993] Papert, S. (1993). The Children's Machine: Rethinking School in the Age of the Computer. New
York: Basic Books.
         [Ram & Leake, 1995] Ram, A., & Leake, D. B. (Ed.). (1995).         Goal-Driven learning. Cambridge, MA:        MIT
Press.
         [Recker, Ram, Shikano, Li, & Stasko, 1995] Recker, M. M., Ram, A., Shikano, T., Li, G., & Stasko, J. (1995).
Cognitive media types for multimedia information access.Journal of Educational Multimedia and Hypermedia, 4(2/3), 185.
        [Shippey, Ram, Albrecht, Roberts, Guzdial, et al., 1996] Shippey, G., Ram, A., Albrecht, F., Roberts, J., Guzdial,
M., Catrambone, R., Byrne, M., & Stasko, J. (1996). Exploring interface options in multimedia educational environments.
In D. C. Edelson & E. Domeshek (Ed.), International Conference of the Learning Science. Evanston, IL.

                                                                                                                  119
