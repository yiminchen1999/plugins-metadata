                   Short-Term versus Long-Term Effects of Cognitive and
                              Metacognitive Prompts in Writing-to-Learn
                Matthias Nückles, University of Göttingen, Göttingen, matthias.nueckles@psych.uni-goettingen.de
                                Sandra   Hübner, Alexander Renkl, University of Freiburg, Freiburg,
                                              Email: renkl@psychologie.uni-freiburg.de

                 Abstract: Journal writing is a promising follow-up to course-work. A learning journal is a
                 written explication of one's own learning processes and outcomes after a lecture or seminar
                 session.  To   fully exploit the  potential   of journal   writing, instructional   support  is required.
                 Experimental    studies  showed   that  prompts     are effective  in  optimizing   journal  writing.   To
                 investigate the long-term effects of prompts, we conducted a longitudinal study. Students (N =
                 50) wrote journal entries about weekly seminar sessions over a whole term. The experimental
                 group    received cognitive  and   metacognitive    prompts    for  their writing.   The   control group
                 received a non-specific instruction without prompts. The prompts proved to be effective in the
                 short term. However, in the long term, they had negative effects on (1) learning strategies
                 elicited in the journals, (2) learning success and (3) students' writing motivation. In order to
                 avoid such pitfalls of over-prompting, a gradual fading of the prompts might offer a solution.

       Introduction
                 Typically,  lesson   and lecture   content   "evaporates"    rather quickly.  After   the  students  have   left the
         classroom   or lecture auditorium,   only  a  few    continue   to reflect  on   learning contents   they  have  just  been
         confronted with. The students rarely organize learning contents in a meaningful and coherent fashion. Seldom
         do they come    up  with  examples   to   illustrate abstract concepts.    Also,  few  students   routinely  monitor   their
         understanding for knowledge gaps and employ remedial strategies that could help close the gaps. Students'
         failure to apply such beneficial cognitive and metacognitive learning strategies typically results in a lack of
         understanding and, thereby, also leads to poor long-term retention. The writing of learning journals (Berthold,
         Nückles, & Renkl, 2007; McCrindle & Christensen, 1995) is a learning method that may help to overcome these
         shortcomings. A learning journal typically represents a written explication of one's own learning processes and
         outcomes  over   an extended   period of   time (e.g.    over a  whole  term   or school    year). Learning   journals   are
         especially appropriate for follow-up course work. They help students apply the above-mentioned cognitive and
         metacognitive strategies.
                 Journal   writing  has  been  shown     to   be  effective  in  improving    students'  learning   across   various
         educational settings and subjects (e.g., Cantrell, Fusaro, & Dougherty, 2000; Connor-Greene, 2000; McCrindle
         & Christensen, 1995). However, there is also evidence that without appropriate instructional support, students
         do not apply the learning journal method in an optimal way (Nückles, Schwonke, Berthold, & Renkl, 2004).
         Therefore, we have developed specific prompts that support the writing of effective learning journals. In this
         contribution, we present an experimental longitudinal study that examined the long-term effects that prompts
         had on the application of cognitive and metacognitive strategies, as well as on students' learning success and
         motivation for journal writing.

        Cognitive and Metacognitive Strategies in Writing Learning Journals
                 Typically,  a  learning  journal  is  not    merely a   summary    of  a writer's   learning outcomes    gained   in
         preceding learning episodes (e.g., a lecture or seminar session). Rather, it is an opportunity to apply beneficial
         cognitive and metacognitive learning strategies in order to deepen and expand the newly acquired knowledge.
         Cognitive learning strategies include organization and elaboration strategies. Organizational strategies refer to
         the identification of main ideas and their interrelations, the highlighting of central concepts, and the structuring
         of contents (Weinstein    &  Mayer,  1986).   That    way,  "internal"  links  that  relate relevant  aspects   of the new
         material to each other are constructed (Mayer, 1984). In other words, the learning contents are organized in a
         meaningful way. Elaboration strategies help to construct so-called "external" links that relate the new material
         to the learner's prior knowledge     (Mayer,  1984).    The   generation   of examples,   the use  of   analogies, and   the
         critical discussion of issues are commonly regarded as prototypical elaboration strategies (Weinstein & Mayer,
         1986). Such strategies assist the learner in going beyond the given knowledge by creating links between her or
         his prior knowledge and the new information (Mayer, 1984), thus enabling deeper understanding and retention
         (Barnett, DiVesta, & Rogozinski, 1981; McCrindle & Christensen, 1995).
                 Besides   cognitive   strategies, the writing    of learning   journals   is further  supposed    to stimulate   the
         application of metacognitive   strategies. Metacognition      refers to the   knowledge     and awareness    of one's  own
         cognitive processes and the ability to actively control and manage those processes (Efklides & Vauras, 1999;

2-2
Flavell,   1976).   Learners    may  use journal    writing to consciously   acknowledge      which   aspects   of the learning
material   they    have  already     understood  well   (positive   monitoring),    or  they  may     identify  comprehension
difficulties  (negative     monitoring;   see  Chi,  Bassok,   Lewis,   Reimann,      &  Glaser,  1989).     The elicitation   of
metacognitive      strategies    during  the   production   of  a learning      journal can   help   to  prevent   illusions   of
understanding     (Chi   et al., 1989;  Renkl,  1999)   and  trigger   remedial  cognitive    strategies  in order to  clarify a
previously identified comprehension problem.
           To  stimulate    the  use  of  cognitive   and   metacognitive    strategies  in   writing   learning   journals, we
developed different sets of prompts. Prompts are questions or hints that induce productive learning processes in
order   to overcome     superficial   processing    (King,  1992; Pressley   et  al., 1992).  They    can  be  conceived  of   as
strategy activators (Reigeluth & Stein, 1983) because they induce learning strategies that the learners are, in
principle, capable of, but do not spontaneously demonstrate, or demonstrate to an unsatisfactory degree. Several
experimental studies investigated the effects of prompts on strategy use and learning outcomes. In the study by
Berthold   et  al.  (2007),  students   received    one of  four  instructions  for   writing a  journal   entry ­  a  so-called
"learning protocol" ­ about a videotaped lecture they had previously viewed. The instruction either included six
cognitive (i.e., organizational and elaborative) prompts (e.g., "How can you best structure the learning contents
in a meaningful way?"), six metacognitive prompts (e.g., "Which main points haven't I understood yet?"), a
mixture of three cognitive and three metacognitive prompts, or no prompts at all (control condition). Results
showed     that   learners   who     received   cognitive,   or   cognitive     and   metacognitive     prompts    significantly
outperformed the control group with regard to (a) the amount of cognitive and metacognitive strategies in the
learning protocols, and (b) the learning outcomes on both an immediate comprehension test and a seven-days
delayed retention test. Hübner, Nückles, and Renkl (2006) successfully replicated the results of Berthold et al.
using an improved and expanded experimental design. They further showed that prompting the application of
regulation   strategies,    that is,  "remedial"    cognitive  strategies    in order   to solve   comprehension       problems
previously    identified    by  self-monitoring,    had a   surplus value    regarding   learning  success.    Accordingly,    in
Hübner's    et  al. study,  the   highest   learning  success  resulted   in that   experimental   condition   where   students
received   prompts   for    (1)  the organization   and  elaboration   of learning    contents,  (2)  the  monitoring  of their
understanding, and (3) the application of regulation strategies in case of perceived comprehension problems.
           Together, these studies suggest that prompts can be a very effective means to support the application of
beneficial cognitive and metacognitive learning strategies in writing a learning journal. Nevertheless, in both
experimental studies, students were required to produce only one single journal entry (i.e., a learning protocol).
In real world instructional settings, such as, university seminars or lectures, students typically do not produce
just one   single   learning   protocol  or journal  entry  like  they did   in the  experimental  studies.    Rather, they  are
required to write journal entries regularly over a longer period of time, for example as follow-up course work
over  a  whole    term.  Thus,   the  question  arises,  whether    prompts   will  be  effective  in stimulating   productive
learning strategies not only for a short time but also in the long term.

Research Questions
           To investigate the long-term effects that cognitive and metacognitive prompts had on strategy use, and
learning success, we conducted an experimental longitudinal study. In this study, we were further interested in
the  students'  motivation      for writing  a learning  journal. Inasmuch      as prompts    as "strategy   activators" are  an
external requirement to apply cognitive and metacognitive strategies, their effects on intrinsic motivation for
writing learning journals were of particular interest. In our study, undergraduate students of psychology kept a
learning journal as follow-up course work for an introductory course in developmental psychology. They wrote
a journal entry about each weekly seminar session over the whole term. The experimental group received a
combination of cognitive and metacognitive prompts that had proved to be most effective in previous research
(cf. Berthold   et  al., 2007;   Hübner     et al., 2006).  These   students    were  compared    with   a control  group    who
received   a  rather  vague     and  non-specific    instruction  that contained    no  prompts    at all.   We  addressed   the
following research questions:
     1.    Will cognitive      and   metacognitive   prompts    foster the   application   of cognitive    and   metacognitive
           strategies in the learning journals not only in the short term but also in the long term?
     2.    Will cognitive and metacognitive prompts foster learning outcomes not only in the short term but also
           in the long term?
     3.    How will the prompts affect the students' motivation for journal writing in the long-term?

Method
Sample and Design
           Fifty first semester students of Psychology (34 females, 16 males, mean age: 21.74 years) participated
in the study. They were randomly assigned to two parallel introductory courses in Developmental Psychology.
The courses lasted four months. Within this time, 14 seminar sessions were held. Except the first and the last
session, the students were required to write a learning journal entry after each session as follow-up course work.

                                                                                                                                     2-2
         Hence, the students had to write twelve journal entries in total. In both courses the same contents were taught by
         the same   lecturer   (i.e.,  the first author).   The  students    completed     these  courses    as  regular  part  of their
         undergraduate studies in Psychology at the University of Freiburg. They received 20 Euro for their participation
         in the testing sessions. To investigate the long-term effects of prompts, we used a control-group design: The
         participants of   one   course (i.e., the  experimental    group, n   =  25)  received   prompts    for writing  their   journal
         entries, whereas     the participants   of the other course    (i.e., the control    group, n  =  25)   received no   prompts.
         Dependent    variables   encompassed     measures    of the   learning   strategies   elicited in the  learning  journals,   the
         students' knowledge acquisition as well as measures of the students' motivation for writing the learning journal.

        Materials, Codings and Instruments
        Instructions for Writing the Learning Journal (Experimental Variation).
                   In both conditions, a brief general instruction on writing a learning journal was given. The participants
         in the experimental condition additionally received six prompts, that is, three cognitive and three metacognitive
         prompts.  The   cognitive    prompts    were  intended  to  stimulate    organization    strategies ("Which    were   the main
         points of today's seminar session in your opinion?") and elaboration strategies ("What examples can you think
         of  that illustrate, confirm,  or   conflict  with  the learning  contents?"     ­   "Which    cross-links can  you   construct
         between today's seminar session and the previous sessions?"). We further applied two types of metacognitive
         prompts:   Monitoring      prompts   were   meant   to  elicit monitoring     strategies   ("Which   main   points  haven't   I
         understood yet?" ­ "Which main points have I already understood well?"). A planning-of-regulation prompt
         encouraged the students to consider ways of regulating their learning process ("What possibilities do I now have
         to overcome my comprehension problem?").

        Analysis of the Learning Journals
                   Two independent raters, who were blind to the experimental conditions, scored the amount of cognitive
         and metacognitive learning strategies in the journal entries by using 6-point rating scales ranging from 1 (=
         dimension not present) to 6 (= dimension clearly present). A score of 6 on the rating scale of cognitive strategies
         could be achieved if the journal entries were highly organized (e.g., by identifying main points and arranging
         them in an ordered sequence, such as "first ..., second ..., third ...") and highly elaborated (e.g., by providing
         own examples to illustrate abstracts concepts: "A good example of Piaget's notion of infantile egocentrism is
         when my little son shows me something in his picture book and disregards that I cannot see what he sees from
         my perspective."). A score of 6 on the rating scale of metacognitive strategies could be achieved if the journal
         entries included a high amount of monitoring (e.g., by specifying which contents were not yet understood: "I
         have not yet understood the exact difference between Piaget and Carey."), and planning of self-regulation (e.g.,
         attempts  to solve   the   perceived  gap  in  one's knowledge:       "I will try  to call to  mind  the   presentation  where
         different approaches to conceptual change were explained..."). Inter-rater reliability as determined by the intra-
         class coefficient was very good (ICC = .81 for the rating of cognitive strategies, ICC = .84 for the rating of
         metacognitive strategies).

        Learning Success
                   Learning success was assessed by two comprehension tests, one of which was administered after the
         first half of the term and the other at the end of the term. Each test consisted of six open-end questions regarding
         the topics that had so far been discussed in the preceding seminar sessions. In order to answer these questions,
         the learners had to apply their knowledge, for example, by using theoretical concepts to explain self-generated
         examples   (e.g., "Please    provide  a moral  justification   of a   solution   for the `Heinz   dilemma'   which    is on  the
         conventional level according to Kohlberg's theory of moral development!"), or by dealing with the material in a
         critical manner     (e.g.,  "Please  discuss   the challenges   and    rewards    of  Piaget's  theory   of cognitive     stages
         critically!"). To score the level of comprehension in the answers we used the SOLO-Taxonomy ("Structure of
         Observed Learning Outcome") by Biggs and Collis (1982). Following the SOLO-Taxonomy, each answer was
         differentiated into six levels of knowledge ranging from 1 (= no central points, low level of understanding,
         incoherent)  to   6  (=  all central  points,  high level  of  understanding,     very   coherent). Inter-rater  reliability as
         determined by the intra-class coefficient was very high (ICC = .96).

        Motivation for Writing Learning Journals
                   The students' motivation for writing learning journals was assessed after the first half of the term and
         again at the end of the term. We were interested to what extent the students enjoyed writing a learning journal,
         evaluated  this   type   of  follow-up  coursework      as valuable   and   useful,   and  how    competent  they   perceived
         themselves to be in doing this. To measure these motivational factors, the students received subscales of the
         intrinsic motivation inventory which we adapted to the domain of journal writing (IMI; cf. Deci, Eghari, Patrick,
         &   Leone, 1994).    In  detail,  we  administered   modified     versions    of the  subscale  interest/enjoyment    (e.g., "I
         enjoyed doing this activity very much"), of the subscale effort/importance (e.g., "I put a lot of effort into this"),

2-2
and of the subscale  perceived   competence  (e.g., "I think  I   am  pretty  good   at this activity"). The  students
responded to these items on a 7-point rating scale ranging from 1 (= not at all true) to 7 (= very true). The
reliability of the scales was good (Cronbach's   = .74 ­ 86).

Procedure
         The students   were asked  to write a journal entry  after   each    weekly seminar   session.  The required
minimum text length was one page. For writing their weekly journal entry, the students logged on a web server.
They downloaded a prepared file in Rich Text Format which included the instructions for writing the journal
entry. Thus, it was guaranteed that the students had the instructions available while writing. After completing
the journal entry, the students uploaded it on the web server. Students who failed to upload their journal entry in
time got a friendly reminder via email by the experimenter. That way it was ensured that the journals entries
were written regularly and the number of missing protocols was kept low. The students' learning success and
their motivation for journal writing were assessed twice: Once after the first half of the term and once again at
the end of the term. These assessments took place in extra sessions at the Institute of Psychology. As part of
these sessions, the students completed  a comprehension      test and the  motivation    questionnaire.  Each  session
lasted about 1.5 hours.

Results
         Table 1 shows the mean scores and standard deviations for both experimental groups on the learning
strategy measures, the comprehension tests, and the motivation scales. The mean scores are plotted separately
for the two measurement times, that is, after the first half of the term and at the end of the term. In case of the
learning strategy ratings, mean scores were obtained by averaging the ratings for the six journal entries which a
student had produced in the first half of the term, and, similarly, by averaging the following six entries the
student had written until the end of the term. An alpha level of .05 was used for all statistical tests. As an effect
size measure, we used   2 ­ qualifying values < .06 as a weak effect, values in the range between .06 and .13 as a
medium effect, and values > .13 as a large effect (see Cohen, 1988).

Table1: Means and Standard Deviations (in Parentheses) of the Analysis of the Learning Journals, Learning
Success and Motivation for Journal Writing in the Experimental Groups.

                          CognitiveStrategiesaMeta-cogitiveStrategiesaLearningSuccessbInterest/Enjoy-mentcEffort/Impor-tancecPerceivedCompe-tencec

1st Halfof TermPrompts    4.00(0.76)      2.68(1.18)      3.95             3.96           4.43            3.84(0.97)(0.89)(1.20)(0.72)
             No           3.70            2.19            3.55             4.01           4.44            3.75
             Prompts      (0.74)          (0.91)          (0.88)           (1.15)         (1.06)          (1.17)
2nd Half/    Prompts      3.78(0.63)      1.63(0.71)      3.51             3.10           3.38            3.41(0.79)(1.00)(1.09)(0.89)
End ofTerm   No           3.90Prompts(0.82)1.68(0.84)     3.56             3.89           4.01            3.90(0.85)(1.31)(1.17)(1.42)
a 6-point rating scale ranging from 1 (= dimension not present) to 6 (= dimension clearly present).
b 6-point rating scale ranging from 1 (= no central points, low level of understanding, incoherent) to 6 (= all
central points, high level of understanding, very coherent).
c 7-point rating scale ranging from 1 (= not at all true) to 7 (= very true).

Analysis of the Learning Journals
         We analyzed the extent to which cognitive and metacognitive strategies were present in the learning
journals. For this purpose, we compared the mean ratings of the six journal entries written in the first half of the
term with the mean ratings of the six journals entries written in the second half of the term. Averaging across
single journal entries had two major advantages: 1) The single journal entries produced by a student may vary
considerably. Thus, using average scores provided us with a more reliable measure of the learning strategies
which  a student elicited in her or his learning journal. 2)  We    avoided    an unnecessary   loss of  data. As  we
analyzed the learning strategy measures with repeated measures analyses of variance (our sample size was too
small for HLM), students who had one or more missing journal entries would have been completely excluded as
cases from the data analysis. Accordingly, the mean ratings for cognitive and metacognitive strategies were
subjected to separate mixed repeated measures analyses of variance with measurement time (first half of the

                                                                                                                         2-2
         term vs. end of the term) as a within-subjects factor and experimental condition (prompts vs. no prompts) as a
         between-subjects   factor.  In   the first MANOVA,      the  mean    ratings of cognitive  strategies were  treated as the
         dependent variable. The MANOVA revealed a significant interaction effect between experimental condition and
         measurement time, F(1, 48) = 9.68, p < .01,          2 = .17 (large effect). Neither the main effect of experimental
         condition, F(1, 48) = 0.20, ns, nor the main effect of measurement time, F(1, 48) = 0.34, ns, were significant.
         Figure 1 (left graph) shows the interaction between experimental condition and measurement time with regard to
         the presence of cognitive learning strategies in the students' learning journals.

                                  Cognitive Strategies                               Metacognitive Strategies
                  4,5                                                     3
                                                                                Prompts
                          Prompts
                    4                                                  2,5
                            No                                                   No
                  3,5    Prompts                                          2    Prompts

                    3                                                  1,5
                         1st Half of the        2nd Half of the                 1st Half of the     2nd Half of the
                              Term                    Term                          Term                 Term

         Figure 1. Interaction effects between experimental condition and measurement time with regard to the presence
            of cognitive strategies (left diagram) and metacognitive strategies (right diagram) in the learning journals.

                   As  the  left diagram   in  Figure   1 shows, students    who  regularly  received   prompts  for   writing their
         learning  journal, elicited  more    cognitive   strategies  in their learning  journal in the  first half of the term   as
         compared     with the   journals entries   produced in  the  second   half.  In contrast, students who     received only a
         general instruction that contained no specific prompts applied less cognitive strategies in the first half of the
         term than in their journal entries written in the second half of the term.
                   Similar results were obtained with regard to the presence of metacognitive strategies in the learning
         journals. Again,   there  was  a  significant  interaction   effect between   experimental  condition   and measurement
         time, F(1, 48) = 5.78, p < .05,      2 = .11 (medium effect). The main effect of the experimental condition was not
         significant, F(1, 48) = 0.88, ns. However, a significant main effect of measurement time was found, F(1, 48) =
         48.07, p < .001,   2 = .50 (large effect). As Figure 1 (right diagram) shows, the extent to which the students in
         both conditions elicited metacognitive strategies in their learning journals clearly decreased in the second half of
         the term. However, the significant interaction effect indicates that the students in the prompting condition were
         more strongly affected by this decrease because they started on a higher level and applied more metacognitive
         strategies in their journals entries written in the first half of the term than the students in the control condition.

                                    Learning Success                                     Interest/Enjoyment
                   4,5                                                   4,5

                                                                                                           No
                     4     Prompts                                        4                              Prompts

                   3,5       No                                          3,5
                           Prompts
                                                                                                 Prompts
                     3                                                    3
                         Half of the Term       End of the Term                Half of the Term     End of the Term

            Figure 2. Interaction effects between experimental condition and measurement time with regard to learning
              success (left diagram) and the students' interest/enjoyment for writing learning journals (left diagram)

        Learning Success
                   The analysis of the two comprehension tests, administered in additional sessions, one taking place after
         the first half of the term and the other at the end of the term, adds to the overall picture provided by the analysis
         of the learning    journals.  As     before, a   significant interaction effect  between    experimental   condition   and
         measurement time occurred, F(1, 48) = 4.29, p < .05,          2 = .08 (medium effect). The main effect of experimental

2-2
condition was not significant, F(1, 48) = 0.59, ns. However, the main effect of measurement time approached
statistical significance, F(1, 48) = 4.10, p < .05,    2 = .08 (medium effect). As Figure 2 (left diagram) shows, this
main effect of measurement time cannot be interpreted independently of the significant interaction effect: In the
first comprehension test after half of the term, the students in the prompting condition achieved clearly higher
test scores than the students in the control condition. However, when learning success was measured again at the
end of the term, the experimental group did not perform any better than the control group without prompts.

Motivation in Writing Learning Journals
          To investigate the effects of prompts on the students' motivation for writing a learning journal, we
analyzed the students' mean ratings for interest/enjoyment, effort/importance and perceived competence. The
results fit in the overall pattern reported so far. For interest/enjoyment, a significant main effect of measurement
time resulted, F(1, 48) = 14.47, p < .001,       2 = .24 (large effect). However, this main effect should be qualified
by the significant interaction between experimental condition and measurement time, F(1, 48) = 8.36, p < .01,                             2
= .15 (large effect). As Figure 2 (right diagram) shows, the students' enjoyment of writing a learning journal
decreased   over  the   course of  the  term.  However,    this decrease  was   evidently             much  more    marked  for   the
prompting condition as compared with the control condition. The main effect of experimental condition was not
significant, F(1, 48) = 2.13, ns.
          Similar results   occurred  for  the students'  invested effort and   their perceived            competence    in journal
writing.  When    invested  effort  was    treated as  the dependent    variable, again              a significant  main   effect  of
measurement time, F(1, 48) = 30.61, p < .001,           2 = .39 (large effect), and a significant interaction between
experimental   condition   and  measurement      time,  F(1, 48) =  5.37, p <   .05,               2 = .11 (medium    effect), were
obtained. Thus, the students generally put less effort in writing their learning journal towards the end of the
term. However, this decrease was substantially stronger for the students in the prompting condition than for the
students in the control condition. The main effect of experimental condition was not significant, F(1, 48) = 1.21,
ns.
          In the final analysis, we tested whether the students' perceptions regarding their competence in writing
a learning journal changed over the course of the term. In this MANOVA, the main effects of measurement time
and experimental condition were not significant, F(1, 48) = 1.28 and F < 1, respectively. However, consistent
with the previous analyses, the interaction between experimental condition and measurement time again reached
statistical significance, F(1,  48)  =  5.62,  p < .05,   2 = .11 (medium effect). Surprisingly, the students in the
experimental condition felt more competent in journal writing in the beginning of the term than in the end of the
term. In contrast, the perceived competence of the students in the control condition increased towards the end of
the term.

Discussion
          The  aim   of the present  longitudinal   study  was  to investigate  the long-term              effects of prompts   as a
method   to  support  the writing  of  productive   learning journals.  Previous  experimental              research  suggests  that
prompts   may  be    very effective  in stimulating    cognitive and  metacognitive     strategies          in  writing a  learning
protocol or journal entry (cf. Berthold et al., 2007; Hübner et al., 2006). However, usually students are not
required to merely produce one single journal entry. In school and academic educational settings, journal writing
is typically introduced as a regular follow-up course work activity, for example, writing a journal entry after
each weekly seminar session (cf. McCrindle & Christensen, 1995; Nückles et al., 2004). Thus, the question
arises how   the provision   of prompts    as  strategy activators (cf. Reigeluth   &   Stein,           1982)   will influence   the
application of beneficial learning strategies, the students' learning success and also their motivation and interest
in keeping a learning journal.
          The results of the present study confirm the results of the previous experimental studies inasmuch as
short-term   effects are  concerned.    In the   short term, the  prompts  effectively               stimulated beneficial learning
strategies in the students' journal entries: Students who received cognitive and metacognitive prompts elicited a
higher degree of cognitive and metacognitive strategies in their first six journal entries in the beginning of the
term than the students in the control condition. In the long term, however, this picture changed. Students who
received    prompts  applied   significantly  fewer  strategies  than in  their initial journal            entries. Their  learning
success, their invested effort and interest in journal writing clearly decreased. The control students' writing, in
contrast, developed more positively over the course of the term: They elicited more cognitive strategies in their
journal entries written in the second half of the term than in their entries produced in the first half. Also, their
writing motivation evidently suffered less than the motivation of the students in the experimental group.
          How can these results be explained? One can say that the results of the present study impressively
demonstrate the pitfalls of prompting procedures in writing-to-learn. In the beginning of the term, the prompts
successfully activated strategies which the students were, in principle, capable of, but which they would have
spontaneously demonstrated to a rather unsatisfactory degree. However, the more the students became familiar
with the learning journal method and "internalized" the tendency to elicit the desired strategies spontaneously by

                                                                                                                                              2-2
         themselves, the more the external guidance by prompts became dispensable and might have interfered with the
         students' internal tendency to apply the strategies by themselves. Thus, at some point in the term, the prompts
         probably did not function any longer as strategy activators ­ in the sense of Reigeluth and Stein (1982) ­, but
         they rather functioned as strategy "inhibitors". As a consequence, the students in the experimental condition felt
         more  and    more  restricted  and  controlled   by  the  prompting    instruction. Consequently,   their  effort to elicit
         cognitive and metacognitive strategies decreased resulting in a substantially lower learning success.
                   Negative side-effects of instructional support methods, such as this "over-prompting-effect" in journal
         writing, have also been reported in other domains and learning settings. Kalyuga, Ayres, and Chandler (2003)
         reported  experimental     evidence   for a  so-called   expertise-reversal  effect. That is, instructional   aids   which
         effectively facilitate learning for beginners and off-load working-memory may produce reverse effects when
         offered to advanced learners with a higher level of prior knowledge and/or skills. Following Kalyuga et al., for
         advanced     learners, the instructional  aids turn  into  "redundant"    information which   is difficult to  ignore and
         therefore produces additional extraneous cognitive load (see also Kalyuga, 2007). Accordingly, it is possible
         that the more the students in our study became skilled in applying the desired strategies, the more the external
         guidance by prompts turned into a redundant stimulus and increased the amount of extraneous cognitive load.
                   Similarly, in the domain of computer-supported collaborative learning, several authors have recently
         discussed the problem of "over-scripting" in relation to computer-supported cooperation scripts (cf. Dillenbourg
         & Jermann, 2007; Weinberger, Reiserer, Ertl, Fischer, & Mandl, 2005). In the case of cooperation-scripts, the
         danger of over-scripting is particularly likely if the script makes very concrete and detailed prescriptions of how
         to behave. These prescriptions may limit the learners' autonomy and latitude too much. As a result, the learner's
         motivation   to enact   the activities prescribed    by the  script may   be corrupted.  Similar   detrimental effects of
         external regulation on intrinsic motivation have previously been discussed in relation to Deci and Ryan's self-
         determination theory (cf. Deci, Koestner, & Ryan, 1999).
                   Regardless of whether the over-prompting effect found in the present longitudinal study was mainly
         cognitive, or rather motivational, or both, a gradual fading of the prompts might offer a possible solution to such
         problems.    Fading    of  instructional  support has    originally  been  proposed   within  the   theory  of  cognitive
         apprenticeship (Collins, Brown & Newman, 1989). Since then, this principle has been successfully applied in
         different settings (McNeill, Lizotte, Krajcik, & Marx, 2006; Puntambekar & Hübscher, 2005). Hence, future
         research should explore how the negative side-effects of over-prompting in writing learning journals can be
         successfully mitigated by a gradual fading of the prompts.

       References
         Barnett, J. E., DiVesta, F. J., & Rogozinski, J. T. (1981). What is learned in note taking? Journal of Educational
                   Psychology, 73, 181-192.
         Berthold, K., Nückles, M., & Renkl, A. (2007). Do learning protocols support learning strategies and outcomes?
                   The role of cognitive and metacognitive prompts. Learning and Instruction, 17, 564-577.
         Biggs, J. B.,   &  Collis,  K.  F. (1982).   Evaluating  the quality   of learning:  The SOLO    taxonomy.    New    York.
                   Academic Press.
         Cantrell, R.  J., Fusaro,   J. A., &  Dougherty,  E.    A. (2000).  Exploring  the   effectiveness of journal  writing on
                   learning social studies: A comparative study. Reading Psychology, 21, 1-11.
         Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Self-explanations: How students
                   study and use examples in learning to solve problems. Cognitive Science, 13, 145-182.
         Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale, NJ: Erlbaum.
         Collins, A., Brown, J. S. & Newman, S. E. (1989): Cognitive apprenticeship: Teaching the crafts of reading
                   writing, and mathematics. In: L. B. Resnick (Eds.): Knowing, learning, and instruction. Hillsdale, NJ:
                   Erlbaum, 453-494.
         Connor-Greene, P. A. (2000). Making connections: Evaluating the effectiveness of journal writing in enhancing
                   student learning. Teaching of Psychology, 27, 44-46.
         Deci,  E. L.,   Eghari,    H., Patrick,   B. C.,  &  Leone,   D.    R. (1994).  Facilitating  internalization: The    self-
                   determination theory perspective. Journal of Personality, 62, 119-142.
         Deci, E. L., Koestner, R., & Ryan, R. M. (1999). A meta-analytic review of experiments examining the effects
                   of extrinsic rewards on intrinsic motivation. Psychological Bulletin, 125, 627-668.
         Dillenbourg, P., & Jerman, P. (2007). Designing integrative scripts. In F. Fischer, I. Kollar, H. Mandl & J. M.
                   Haake   (Eds.).   Scripting  computer-supported    collaborative   learning.   Cognitive,   computational   and
                   educational perspectives (pp. 259-289). Berlin: Springer.
         Efklides, A., & Vauras, M. (Eds.). (1999). Metacognitive experiences in their role in cognition (Special Issue).
                   European Journal of Psychology of Education, 14, 455-584.
         Flavell,  J. H.   (1976).  Metacognitive     aspects of  problem    solving. In L.   B.  Resnick   (Ed.),  The nature  of
                   intelligence (pp. 231-235). Hillsdale, NJ: Lawrence Erlbaum.

2-0
Hübner, S., Nückles, M., & Renkl, A. (2006). Prompting cognitive and metacognitive processing in writing-to-
        learn enhances learning outcomes. In R. Sun, N. Miyake & C. Schunn (Eds.), Proceedings of the 28th
        Annual Conference of the Cognitive Science Society (pp. 357-362). Mahwah: Erlbaum.
Kalyuga, S. (2007). Expertise reversal effect and its implications for learner-tailored instruction. Educational
        Psychology Review, 19, 509-539.
Kalyuga,   S., Ayres,  P., Chandler,  P. &   Sweller, J.  (2003). The    expertise reversal  effect. Educational
        Psychologist, 38, 23-31.
King, A.   (1992).  Comparison   of self-questioning, summarizing,    and  note-taking-review  as    strategies for
        learning from lectures. American Educational Research Journal, 29, 303-323.
Mayer, R. E. (1984). Aids to text comprehension. Educational Psychologist, 19, 30-42.
McCrindle,  A.,  &  Christensen, C.  (1995). The  impact  of learning journals on  metacognitive  and   cognitive
        processes and learning performances. Learning and Instruction, 5, 167-185.
McNeill, K. L., Lizotte, D. J., Krajcik, J., & Marx, R. W. (2006). Supporting students' construction of scientific
        explanations by fading scaffolds in instructional materials. Journal of the Learning Sciences, 15, 153-
        191.
Nückles, M., Schwonke, R., Berthold, K., & Renkl, A. (2004). The use of public learning diaries in blended
        learning. Journal of Educational Media, 29, 49-66.
Pressley, M., Wood, E., Woloshyn, V. E., Martin, V., King, A., & Menke, D. (1992). Encouraging mindful use
        of  prior  knowledge: Attempting    to construct explanatory  answers  facilitates learning. Educational
        Psychologist, 27, 91-109.
Puntambekar, S., & Hübscher, R. (2005). Tools for scaffolding students in a complex learning environment:
        What have we gained and what have we missed? Educational Psychologist, 40, 1-12.
Reigeluth, C.  M.,  & Stein, F.  S. (1983).  The elaboration  theory  of instruction. In C. M.  Reigeluth (Ed.),
        Instructional-Design theories and models: An overview of their current status (pp. 335-382). Hillsdale,
        NJ: Lawrence Erlbaum Associates.
Renkl, A. (1999). Learning mathematics from worked-out examples: Analyzing and fostering self-explanations.
        European Journal of Psychology of Education, 14, 477-488.
Weinberger, A., Reiserer, M., Ertl. B., Fischer, F., & Mandl, H. (2005). Facilitating collaborative knowledge
        construction in computer-mediated learning environments with cooperation scripts. In R. Bromme, F.
        W. Hesse, & H. Spada (Eds.). Barriers, biases and opportunities of communication and cooperation
        with computers (pp. 15-37). New York: Springer.
Weinstein, C.   E., &  Mayer,  R. E.  (1986).  The teaching  of learning  strategies. In C.  M. Wittrock  (Ed.),
        Handbook of research in teaching (pp. 315-327). New York: Macmillan Publishing Company.

                                                                                                                       2-
