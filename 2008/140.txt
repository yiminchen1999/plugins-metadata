         What is learned from computer modeling? Modeling modeling
                                   knowledge in an experimental study
            Sylvia P. van Borkulo, Wouter R. van Joolingen, University of Twente, Faculty of Behavioral Sciences,
                   Department of Instructional Technology, P.O. Box 217, 7500 AE Enschede, The Netherlands
                                Email: s.p.vanborkulo@utwente.nl, w.r.vanjoolingen@utwente.nl
                 Elwin R. Savelsbergh, Utrecht University, Utrecht, The Netherlands, e.r.savelsbergh@phys.uu.nl

                  Abstract: In this poster, we present an experiment in computer modeling that compares two
                  rather contrasting conditions: traditional teaching and inquiry modeling. The modeling pre-
                  and  posttest operationalized   the  segments   of  a  modeling  knowledge     framework.   The
                  experimental group of modelers performed significantly better than the traditional group on
                  the complex items. Surprisingly, no differences were found for items about the creation of
                  models. The implications for future research are discussed.

       Introduction
                  Computer modeling is an emerging topic in secondary science education. The topic of modeling is
         relatively new in secondary education and not much is known about the specific learning outcomes and how the
         outcomes can be measured (Löhner, 2005; Spector, Christensen, Sioutine, & McCormack, 2001).
                  The essence of modeling is creating a simplified representation of reality to understand a situation.
         With the model one can do predictions and give explanations of phenomena. Computer modeling provides the
         possibility to run the model, perform experiments and evaluate the outcomes. Computer modeling tools such as
         STELLA (Steed, 1992), Model-It (Jackson, Stratford, Krajcik, & Soloway, 1996) or the modeling tool in Co-
         Lab (van Joolingen, de Jong, Lazonder, Savelsbergh, & Manlove, 2005) enable students to create models to
         represent their ideas about a domain, in terms of entities, variables and relations. When executing the model, the
         tool computes the values of the variables as they develop over time. This allows the learner to see the course of
         events predicted by the model.
                  A   number of studies have  investigated  the  processes of modeling  as a  learning activity  (Hogan &
         Thomas,   2001; Löhner,   van Joolingen, Savelsbergh,   &   van Hout-Wolters, 2005;   Sins, Savelsbergh,   &  Van
         Joolingen, 2005), but less focus has been directed to the specific learning outcomes of modeling as a learning
         activity (Löhner, 2005). To be able to measure the specific types of knowledge that are built by learners as a
         result of  a  modeling activity, we  developed   a   framework  that gives  a precise   operationalization of the
         knowledge and skills involved (van Borkulo, van Joolingen, Savelsbergh, & de Jong, in press). In the following
         section, we will describe this framework. An experimental study is described and implications are discussed.

       Modeling knowledge framework
                  The framework, in more detail described in van Borkulo, van Joolingen, Savelsbergh, & de Jong (in
         press), distinguishes three dimensions: 1) reasoning, 2) complexity, and 3) domain-specificity.
                  The reasoning dimension describes the specific reasoning processes that are involved in the process of
         learning activities in modeling. These reasoning processes can be derived from the process of scientific inquiry
         (de Jong, 2006; van Joolingen & de Jong, 1997; Klahr & Dunbar, 1988), where a mapping can be made from the
         processes  of inquiry  to reasoning processes  involved  with   modeling. We  distilled three types  of reasoning
         related to experimenting with models. First, predicting and explaining using a model, which we call applying the
         rules of a model. Second, evaluating a model or data computed by simulating a model represents a search in
         experiment   space and the   mapping of  its results to the predictions made  by  the model.  Third, and   finally,
         creating a model is building a new model or expanding an existing model, by adding or modifying variables and
         relations in the model. Creation processes represent a move in the hypothesis space.
                  The second dimension concerns the complexity of the model that is subject of the reasoning processes
         in the first dimension. A model has complexity due to its behavior, for example the occurrence of equilibrium,
         and due to its structure, for example the number of variables and relations, and the number of steps in a step by
         step reasoning. To cope with these aspects of complexity, we define simple as the smallest meaningful unit of a
         model, with one dependent variable and direct relations to that variable only, and complex as a larger chunk that
         contains  indirect relations and  maybe  (multiple)    loops and  complex   behavior,   requiring more   complex
         reasoning.
                  The third dimension concerns the influence of the domain in which the modeling takes place. Working
         with a model will be a mixture of general reasoning skills and stored domain-specific knowledge. Prior domain
         knowledge influences the reasoning processes, leads to reproducible knowledge, and may alter the complexity
         under the influence of preconceptions.

3-0
Research questions
          In this paper, we will explore the differences in performance on a modeling test based on the modeling
knowledge   framework   of   two   conditions:   traditional teaching and  inquiry  modeling.   The main    difference
between   the two  groups  is the  presence   of a   modeling  tool and a  simulation which  will  affect the  type  of
reasoning. The traditional group is given the same textual information as the modeling group and mainly the
same assignments, however they had to solve these assignments without simulation or modeling tool.

Method
          Subjects were eleventh grade students from two schools. In total 74 subjects completed the experiment.
The  subjects in  the modeling     condition could   use a   simulation of the  basic energy  model    of the earth, a
modeling editor to create and run models, and graphs and tables to evaluate their data.
          The experiment consisted of two sessions of each 200 minutes. Both the experimental and the control
condition  attended a  first session   in which    modeling  was introduced.  After 150   minutes  of  instruction, all
subjects completed a paper-and-pencil test in a fantasy domain about the harmony of the spheres (50 minutes).
For the second session, the students were separated in two groups. The students in the experimental condition
were given an elaborated modeling task about global warming for 150 minutes. The students in the control
condition were given an assignment to write a small report on the causes of global warming for 150 minutes.
After that, all subjects completed the black sphere test about global warming (50 minutes).
          To gain insight in the differences in performance of the two conditions, we analyzed the variance of the
mean scores on the black sphere test of subjects in the different conditions in the different segments of the
framework with the fantasy test score as covariate.

Results
          A significant difference was found in favor of the experimental condition on the complex item scores
(F(1, 72) = 8.780, p = .004). More specifically, the experimental condition performed significantly better on
both  the complex   items  on reproducing    domain    knowledge    (F(1, 72) = 6.097,  p =  .016)  and   the complex
evaluate items (F(1, 72) = 3.966, p = .050). For the other segments in the framework no significant differences
were found. No significant differences were found on the overall total score, though the experimental condition
performed  slightly better   (F(1, 72) =  2.972,   p = .089). No  significant differences were   found  on  the create
segment.

Discussion
          The aim of this study was to explore the differences in performance on a test that was based on a
modeling   knowledge   framework       (van Borkulo,   van   Joolingen, Savelsbergh,  &   de Jong,  in  press). In  an
experimental study, we compared two conditions: a group who followed a traditional approach in learning about
global warming, and a group who followed an inquiry modeling approach.
          Differences between      the two   conditions  were   found   with  respect  to the   complex   items.  The
experimental condition performed significantly better on the overall complex items, on the complex evaluate
items and  on  the  complex   reproduce     items. The control  condition  performed   slightly better on  the  simple
reproduce items. On the remaining simple items they performed equally well.
          We found no differences related to the creation of models. An explanation may be that the create items
were too difficult for both groups. The modeling condition experienced creating models during the experiment,
but each created model explicitly referred to a previous model. In the create items in the test, a model had to be
created from a given context without any reference to previous experience.
          In conclusion, the modeling test appears to be able to detect differences between a modeling group and
a traditional group concerning complex items. It is worthwhile to further investigate the operationalization of the
create items and the development of create skills in the instruction. Future research will need to focus on a
connection between the two.

References
van Borkulo, S. P., van Joolingen, W. R., Savelsbergh, E. R., & de Jong, T. (in press). A framework for the
          assessment of learning by modeling. In P. Blumschein, J. Stroebel, W. Hung & D. Jonassen (Eds.),
          Model-Based Approaches to Learning. Rotterdam, Netherlands: Sense Publishers.
Hogan, K., & Thomas, D. (2001). Cognitive comparisons of students' systems modeling in ecology. Journal of
          Science Education and Technology, 10(4), 319-345.
Jackson, S. L., Stratford, S. J., Krajcik, J. S., & Soloway, E. (1996). Making dynamic modeling accessible to
          pre-college science students. Interactive Learning Environments, 4, 233-257.

                                                                                                                          3-
        de Jong, T.  (2006).  COMPUTER       SIMULATIONS:     Technological   Advances  in Inquiry  Learning.  Science,
                 312(5773), 532-533.
        van Joolingen,   W.  R., & de Jong,  T. (1997). An extended   dual search space model of   scientific discovery
                 learning. Instructional Science, 25(5), 307-346.
        Klahr, D., & Dunbar, K. (1988). Dual-Space Search during Scientific Reasoning. Cognitive Science, 12(1), 1-
                 48.
        Löhner,  S.  (2005). Computer   Based   Modeling  Tasks:  the Role of External  Representation. University  of
                 Amsterdam, Amsterdam.
        Löhner,  S., van  Joolingen, W.  R., Savelsbergh, E.  R., & van Hout-Wolters,  B.  (2005). Students'  reasoning
                 during modeling in an inquiry learning environment. Computers in Human Behavior, 21(3), 441-461.
        Sins, P. H. M., Savelsbergh, E. R., & Van Joolingen, W. R. (2005). The difficult process of scientific modelling:
                 An analysis of novices' reasoning during computer-based modelling. International Journal of Science
                 Education, 27(14), 1695-1721.
        Spector, J.  M., Christensen, D. L., Sioutine, A. V., &   McCormack,  D.  (2001). Models   and simulations for
                 learning in complex domains: using causal loop diagrams for assessment and evaluation. Computers in
                 Human Behavior, 17(5-6), 517-545.

3-
