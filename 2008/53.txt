            A Needs Analysis for Instructional Support in LegSim
    Mahesh Joshi, Carnegie Mellon University, 5000 Forbes Ave. Pittsburgh, PA 15213, maheshj@cmu.edu
 Yi-Chia Wang, Carnegie Mellon University, 5000 Forbes Ave. Pittsburgh, PA 15213, yichiaw@cs.cmu.edu
         John Wilkerson, University of Washington, Seattle WA 98195, jwilker@u.washington.edu
    Carolyn Rosé, Carnegie Mellon University, 5000 Forbes Ave. Pittsburgh, PA 15213, cprose@cs.cmu.edu

         Abstract:  The  primary    research question we    address   in  this paper   is the   extent to which
         participation within a well-established game-based learning environment for Civics instruction
         called LegSim (Legislative Simulation) supports the needed development of argumentation skills.
         Our analysis suggests that although environments like LegSim offer great potential for providing
         students with valuable opportunities to develop important skills, active support during participation
         is needed to ensure that students take these opportunities when they are presented (1).

Introduction
         Recent developments in research point to computer-based simulations and games as valuable tools for
promoting learning (Alessi, 2000; Dorner, 1996). One focus of our research has been to develop methodologies
for advancing  critical thinking  and   argumentation    skills   in  a political  science  simulation    called   LegSim
(Wilkerson  &  Fruland,  2006;   http://www.legsim.org/).     LegSim      is primarily    designed  to  teach    legislative
procedures and strategies to the students of a college-level political science course. Within LegSim, students
assume  representational  roles and    advance  legislative   proposals   that   reflect  their own    political priorities.
Undoubtedly, it is successful in terms of providing students the opportunity to practice the procedures they are
learning in the didactic portion of their course with respect to the functioning of a legislature. The work reported
here, however, is not aimed at a formal assessment of the extent to which students learn legislative procedures.
The  question  we address in this   paper is the extent  to   which   the environment     supports  the   development   of
argumentation skills, and what needs exist in environments like LegSim for support either by instructors or by
intelligent agents that are able to participate in the conversations, as in some recent work in the area of dynamic
support for collaborative learning (e.g., Kumar et al., 2007).    To that end, we present an analysis of a semester's
worth of discussion  data  within   the LegSim   environment       in order    to shed   light  on the  opportunities  for
argumentation and critical thinking skills development and how students may or may not have taken advantage
of them. For our analysis we chose to explore the data from one college level class over a whole semester. 93
students participated in the course, with LegSim as a central part of the course.

Longitudinal Data Analysis
         Among other things, LegSim includes features that allow students to participate in "floor debates" in
the form of  online discussions, where    they are  able to   post comments       and opinions   regarding the   proposed
legislation (a.k.a. a bill). As is true in the real Congress, most of the bills that are introduced will never be
brought up for a vote, and as a result, many will never be debated. If there is a vote, a voting position can be
'yes', 'no' or 'present', the last category being equivalent to a neutral opinion on the bill. Over the course of the
semester, 304 bills were proposed by students.      Of these, 99 were promoted to the floor by their respective
committees.  And  of those, only    48 were  discussed   in   the floor  debate   forum.  Our   analysis  focuses   on the
comments posted for those bills that were debated, including 479 comments posted by 71 students.
         Our analysis was guided by two focal questions: (1) Where do we find opportunities for students to
experience successes and failures in their argumentation? (2) To what extent do we find evidence that students
gained competence in their argumentation ability over the course of the semester? We present this analysis as a
needs assessment, which leads to specific recommendations for support within environments like this.                In our
analysis, we were interested both in how effective student arguments were in swaying each other's opinions as
well as in the structural characteristics of the arguments, i.e., how sophisticated they were from a structural
standpoint. To that end, we have coded the comments posted by the students along two dimensions. The first
dimension is the sentiment or orientation of a comment towards the proposed bill (positive / negative / neutral).
With each comment coded this way, we can detect shifts in student positions on a bill over the course of a
conversation or before they vote, as well as success of argumentation in terms of swaying the overall vote. The
second dimension is the quality of argumentation found within each comment (high quality argumentation /
medium quality argumentation / no argumentation). The argumentation quality dimension in particular may be
interesting from the perspective of a course instructor who uses the LegSim system in his/her course, if one of
the goals of its use is helping students to learn better argumentation skills in the political science domain.
         In our data, only about 40% of bills came up for a vote before the last 10 days of the semester, and only
about 15%   of them  came  up   for a  vote  before the  last 20  days   of  the  semester.    Thus, while the strongest

                                                                                                                               3-3
       experience of success or failure of argumentation would come from having argued for or against a bill, and then
       seeing the vote come out either consistent with or inconsistent with that argumentation, few opportunities of that
       nature were forthcoming for students until relatively late just because of how the timing played out.
                A more limited sense of success or failure could come from students making an argument and then
       seeing other participants in the conversation swayed to change their position.      However, there were only four
       cases where students changed their position from the position expressed in their post(s) to the position expressed
       by their vote, with only one comment possibly reflecting a change of opinion due to arguments from others.
       When comparing the positions expressed in the on-line debate with the final vote cast by the students, we found
       only eight comments     corresponding  to  five bills where   the vote cast was  exactly opposite   to the  sentiment
       expressed. However, all except one of these five bills were scheduled for vote in the final week of the semester.
       Since students did not have the opportunity to observe both successes and failures on this level, and since failure
       to convince others was the norm regardless of the quality of argumentation, it seems doubtful that the failures to
       sway partners would be felt as failures, or if they would be perceived as related to the quality of argumentation.
                One reason why we may have observed so few changes in position could be because it was relatively
       rare for the same student to post more than one substantive comment on the same bill.        We find evidence that
       students did not always thoroughly express their positions. At the extreme end, 22 out of the 93 students in the
       course never participated in any of the floor debates.    There is evidence that the students who did post were the
       ones who felt most strongly about the bills.   In particular, using a binary logistic regression with student id, bill
       id, and vote status as factors, with a binary indicator of whether at least one comment was expressed by the
       student for that bill as the dependent variable, we find that students who voted either positively or negatively on
       a bill were significantly more likely to post at least one comment on the associated bill than students who were
       either absent  or abstained   from voting  (p   < .05).   Thus, while  there is evidence    that overall students are
       enthusiastic about their participation in the LegSim environment, there is evidence that valuable opportunities
       for participation are missed in cases where students do not feel strongly about a bill.
                Just as we have trouble finding evidence of opportunities for developing strong epistemological beliefs
       about argumentation based on participation, we also do not find evidence of an increase in sophistication in
       argumentation   as  the semester   progressed.    For   this analysis, we   observed the proportion    of  comments
       exhibiting high quality argumentation between the beginning third, middle third, and final third of the semester.
       Much to our surprise, the trend was for a higher proportion of comments submitted early in the semester (42%,
       with only 8 different students contributing at least one comment) to be high quality than in the middle (18%
       from 47 students) or at the end (24% from 62 students).

      Discussion and Current Directions
                The analysis of LegSim data casts doubt that providing a game environment alone, although it may be
       highly engaging for students, is enough to stimulate the development we would like to see in students.         Thus,
       our conclusion is that instructional support is needed to actively steer students towards opportunities to learn,
       and  thus research  is  needed  to further develop    technologies  that would  make  support    within  multi-player
       environments like this feasible, such as technology for automatic collaborative learning process analysis (Rosé
       et al., in press), and interactive collaborative learning support (Kumar et al., 2007).  Such technology could be
       used to prompt students to voice conflicts that have not yet been articulated or to encourage students to elaborate
       bald claims with warrants, data, and qualifiers. A further application of automatic analysis could be to give
       instructors insight into the dynamics of the conversations and how student argumentation is developing.

      Endnotes
        (1) This work was funded through ONR Cognitive and Neural Sciences Division grant N000140510043.

      References
       Alessi, S. (2000). Building versus using simulations. In J. M. Spector & T. M. Anderson. (Eds) Integrated &
                holistic  perspectives  on   learning,   instruction  and  technology:  Understanding      complexity.   The
                Netherlands, Kluwer: 175-196.
       Dörner, D. (1996). The logic of failure: strategic thinking for complex situations. New York, Henry Holt and
                Company.
       Kumar,    R., Rosé,  C.   P., Wang,   Y.  C., Joshi,  M.,    Robinson,  A.  (2007). Tutorial  dialogue   as adaptive
                collaborative learning support. In Proceedings of AIED 2007, 383-390.
       Rosé,  C. P.,  Wang,   Y.  C.,  Cui, Y., Arguello,    J., Stegmann, K.,  Weinberger,    A., Fischer, F.,  (In Press).
                Analyzing Collaborative Learning Processes Automatically: Exploiting the Advances of Computational
                Linguistics   in Computer-Supported      Collaborative   Learning.  International   Journal   of  Computer
                Supported Collaborative Learning.
       Wilkerson, J. & Fruland, R. (2006). Simulating a federal legislature. Academic Exchange: Teaching Political
                Science. 10(4).

3-
