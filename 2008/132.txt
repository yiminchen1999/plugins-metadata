                       Matching Model Representations to Task Demands
                            B. Slof, G. Erkens, P. A. Kirschner, Utrecht University, PO Box 80.140,
                                                3508 TC Utrecht, The Netherlands
                                 Email: B.Slof@uu.nl, G.Erkens@uu.nl, P.A.Kirschner@uu.nl

                 Abstract: In CSCL environments model representations can stimulate learners to explicate
                 their reasoning  and   elicit and   support    knowledge  construction.    But literature  on their
                 effectiveness is not clear with data indicating that representations not adapted to task demands
                 have  counterproductive    effects. By    matching  the  representational   guidance  of  a   model
                 representation to the demands of the task, learners can be supported during their collaborative
                 inquiry process because they receive the information they need when they need it.

                 Generally  speaking,   the goal  of  collaborative  inquiry  learning   is to  have learners  create  a  well
        developed  conceptual understanding    of a  subject such   that they are able   to solve problems    concerning  that
        subject (Jonassen,  2003).  This  conceptual   understanding   is considered   to   be well  developed   when  it has
        achieved  an integration of  both qualitative  and  quantitative  knowledge   representations  of  the subject matter
        (White, & Frederiksen, 1990). Such an understanding enables learners: to understand the core concepts and
        principles of the subject matter and the interrelationships between them (qualitative problem representation), to
        make   calculations according   to  these    principles and  to   understand  the   outcome    of  these  calculations
        (quantitative problem representation). However, research shows that learners encounter at least two difficulties
        when working with multiple representations, namely, (1) problems translating from and coordinating between
        different kinds of representations (Ainsworth, 1999), and (2) incongruence between a representation and the task
        demands of a specific inquiry phase (Van Bruggen, Boshuizen, & Kirschner, 2003). In the orientation phase of
        problem solution, the learning task is less structured and aimed at constructing a global problem representation.
        In order to find a proper overview of the problem, qualitative knowledge is more appropriate for constructing a
        cognitive bridge  between   the learners'  initial mental  model   and the mental      model that  has to be   created
        (Jackson, Stratford, Krajcik, & Soloway, 1996). In the solution phase, the task is more structured in the sense
        that learners must produce concrete solutions by making the relationship between the problem and the proposed
        solutions explicit by identifying causal relations. The knowledge remains qualitative, but it contains - along with
        the central concepts of the problem - causal information that supports learners in finding multiple solutions to
        the problem. During the evaluation phase, learners are occupied with simulating their proposed solutions and
        gaining insight into their effects. Learners must compare their proposed solutions with each other, "forcing"
        them to look at alternative possibilities. This task has a quantitative character and can only be understood if the
        learners have a well developed qualitative conceptual understanding of the subject matter.
                 A mismatch between representation and task demands is detrimental for learning and must be avoided.
        Therefore it is important to determine what model representation is congruent with the task demand of a specific
        inquiry phase. This can be achieved by determining the representational guidance that a model has (Suthers,
        1998).

       Representational Guidance of Model Representations
                 The representational guidance of a model is determined by its constraints and its salience (see Table 1).
        Constraints refers to what is expressed in the model: the concepts and their interrelationships (i.e., specificity),
        and how   accurate  they are represented  (i.e., precision). Salience  refers to the   differences in expressiveness,
        caused by the different constraints, and which leads to the determination of the number and types of inferences
        that can be made. Less specific and less precise models have the advantage of having a high processability
        (Larkin, & Simon, 1987); it is rather easy to make many inferences from such models (i.e., elaboration). Those
        models guide learners in elaborating about the concepts of the subject matter and in relating them to the problem
        (Jonassen, 2003). However, such simple models do not have much expressive power (Cox, 1999); the inferences
        cannot be very detailed. The order of a model determines in what way learners can reason about the subject
        matter (White, & Frederiksen, 1990), determining the quality of the inferences. A zero order model supports
        reasoning about the concepts and in relating them to the problem in qualitative way. A first order model is more
        expressive and  supports  reasoning  about   causal relationships and  guides  discussion   of possible  solutions. A
        second order model is the most expressive and guides learners in making quantitative inferences which should
        enable them to come to a final solution.
                 Each inquiry phase matches a specific model because the representational guidance of the model is
        congruent with the task demands of that phase. A mismatch means that the model is incongruent with the task
        demands. Reasons for this could be that the model is too simplistic because it contains no relevant information,

3-3
or it is too complex because learners do not have enough prior knowledge of the subject matter to grasp the
complexity of the model.

Table 1: Specification of the model representations

  Inquiry         Model                      Representational guidance
  Phase           representation
                                             Constraints                                 Salience

  Orientation     Conceptual model           Low             Undirected relations        Zero      Unstructured
  Solution        Prediction model           Middle          Causal directed relations   First     Quasi-structured
  Evaluation      Simulation model           High            Model directed relations    Second    Structured

Research Goal
         The focus of the study is on determining whether a model representation is suited for coping with the
task demands of a specific inquiry phase. If learners receive the information they need when they need it, their
inquiry process will lead to a better developed conceptual understanding, making it easier to solve current and
future problems concerning the subject matter.

Design and Expectations
         Learning triads  have   to solve a  business economics  problem   in a  computer  supported collaborative
inquiry-based  learning  environment.    We  have   chosen for collaborative  learning  because  learner discussion
provides insight into the   inquiry  process itself and the  conceptual  understanding  of the  subject  matter. All
experimental groups must solve the problem by working collaboratively through three different inquiry phases
with different task   demands.    In  the three  mismatch    conditions, the  groups    receive a different model
representation (i.e., conceptual-, prediction- or simulation model) which matches only with the task demands of
one of the three inquiry phases (i.e., orientation-, solution- or evaluation phase). In the fourth condition, the
groups receive all three models in a phased order. That is: they receive the most suited model for each inquiry
phase (i.e., there is a match between task demands and models for all inquiry phases).
         By analyzing the dialogue protocols and the quality of the answers of each inquiry phase, the effects
and  congruence  expectations    of  the  different model  representations will   be  determined. We    expect   that
congruent models   lead   to better  task performance    for a specific  inquiry  phase than   incongruent  models.
Furthermore, we expect that the groups from the complete match condition will arrive at a better solution to the
problem and create a better developed conceptual understanding because their knowledge has evolved in a more
progressive way.

References
Ainsworth, S. (1999). A functional taxonomy of multiple representations. Computers and Education, 33, 131­
         152.
Cox, R. (1999). Representation construction, externalised cognition and individual differences. Learning and
         Instruction, 9, 343­363.
Jackson, S.,  Stratford, S.  J., Krajcik, J. S., &    Soloway, E. (1996).  Making      systems  dynamics  modeling
         accessible to pre-college science learners. Interactive Learning Environments, 4, 233­257.
Jonassen, D. H. (2003). Using cognitive tools to represent problems. Journal of Research on Technology in
         Education, 35, 362­381.
Larkin, J. H., & Simon, H. A. (1987). Why a diagram is (sometimes) worth ten thousand words. Cognitive
         Science, 11, 65­100.
Suthers, D. D. (1998). Representations for scaffolding collaborative inquiry on ill-structured problems. Paper
         presented at the 1998 AERA Annual Meeting, San Diego, California.
Van Bruggen, J. M., Boshuizen, H. P. A., & Kirschner, P. A. (2003). A cognitive framework for cooperative
         problem solving with argument visualization. In P. A. Kirschner, S. J. Buckingham-Shum, & C. S. Carr
         (Eds.), Visualizing Argumentation: Software tools for collaborative and educational sense-making. (pp.
         25­47). London: Springer.
White, B. Y., & Frederiksen, J. R. (1990). Causal model for progressions as a foundation for intelligent learning
         environments. Artificial Intelligence, 42, 99­157.

                                                                                                                        3-33
