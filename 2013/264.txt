CSCL 2013 Proceedings                                                           Volume 1: Full Papers & Symposia

 Interface Tangibility and Gesture in Mediating Individual Agency
        Within Group Spatial Problem Solving With an Ecosystem
                                                   Simulation

          Helen Kwah, New York University, 82 Washington Square East, 6th Floor, New York, NY
                                                        10003
          Leilah Lyons, University of Illinois at Chicago, 851 S. Morgan (M/C 152), Chicago, IL 60607
          Dixie Ching, New York University, 82 Washington Square East, 6th Floor, New York, NY
                                                        10003
                             Email: helen.k@nyu.edu, llyons@uic.edu, dixie@nyu.edu

         Abstract: This paper examines how a tangible interface facilitates gesture-mediated spatial
         reasoning  during   collaborative   problem   solving, as  evidenced   by sensitivity   to  emergent
         spatial patterns within a complex system simulation of a watershed. We tested the interface
         against two non-tangible input comparison conditions (single- and multi-mouse) to control for
         access differences. To determine if groups' solutions displayed sensitivity to emergent spatial
         patterns,  we  constructed    a quantitative  "dynamism"     measure,   and    found  that  solutions
         produced   in the   tangible  condition were   significantly  more   spatially sensitive.  To  better
         understand why, we conducted a case study by selecting two representative participant groups
         and performing      qualitative multimodal    analyses of  participant  speech    and   gesture.   Our
         findings   indicate  that the    tangible interface's  greater  affordance     of  gesture    allowed
         participants  to express  concepts   containing  both  spatial and   temporal  properties,  with   the
         added benefit    of increasing   the agency   of less-verbally  participatory   group   members     to
         explore and contribute their ideas more equitably.

Introduction
When a complex system simulation is spatial, meaning that the location of simulation elements has an impact on
the emergent outcomes of the simulation, special supports may be needed to assist learners as they come to
reason spatially about the represented domain. By "reasoning spatially," we do not refer to classic visuospatial
cognition literature (e.g., wayfinding or mental rotation tasks), but rather to the class of problems identified by
the NRC Committee on Geography's (2006) "Learning to Think Spatially" report, wherein reasoning spatially
entails the ability to "perceive,   remember,    and   analyze  the static and,  via  transformations,      the dynamic
properties of objects and the relationships between objects." In our problem space (the integration of green
infrastructure into urban landscapes) spatiality is highly important: a given green infrastructure element may
have a large impact or none at all depending on its placement. Because traditional desktop simulations ask the
user to transmute spatial manipulations through (single-user) input devices like mice, we designed a tangible
simulation interface to support more direct spatial manipulations by multiple users and conjectured that the
tangibility would improve users' abilities to construct solutions sensitive to the simulation's emergent spatial
phenomena. We tested the interface against two non-tangible mouse input conditions, and gauged sensitivity of
solutions with a    "dynamism    measure"-    finding  that solutions   produced   in   the tangible   condition   were
significantly more spatially sensitive. A case study was then conducted to examine why, and our findings, which
are presented in this paper, indicated that the tangible interface afforded gesture-mediated spatial reasoning,
which enabled less verbally participatory group members to better consider spatial and temporal information
and more equitably express their reasoning, resulting in improved group problem solving outcomes.

Prior Work
In human-computer interaction research, tangible user interfaces (TUIs) in the form of multi-touch tabletop
displays have been shown to result in more equitable participation by group members working on tasks with
spatial or other physical constraints, like arranging office seating (Marshall et al., 2008) or planning itineraries
(Rogers & Lindley, 2004). In a study that compared a multi-touch tabletop to input using TUIs, the use of
tangibles encouraged greater participation from people who normally found it difficult to contribute verbally in
group settings (Rogers et al., 2009). Other research has found that the use of TUIs facilitated individual spatial
problem   solving  as users  leveraged   the  physical affordances  of  tangible objects   (Antle   et al., 2009). This
research adds to this prior literature by examining the affordance of TUIs for gesture-mediated spatial reasoning
in the context of group spatial problem solving with a dynamic, computer-based simulation.
         There is a growing body of research on the role gesture plays in facilitating spatial cognition (Alibali,
2005;   Goldin-Meadow,    2003).   There  are various  dimensions   or  types of gesture    that have  been  identified,
including   deictic   (pointing)   and   iconic  gestures   (McNeill,   1992).   Iconic     gestures   are   considered
`representational' as they present kinesthetic images of objects or actions; for example, waving a cupped hand

© ISLS                                                                                                              264
CSCL 2013 Proceedings                                                              Volume 1: Full Papers & Symposia

up and down while talking about playing basketball. Iconic gestures can thus facilitate spatial cognition by
helping the gesturer to focus upon and represent dynamic, temporal and spatial information--information that
would be difficult to convey in speech but that is necessary for understanding a topic (Pozzer-Ardenghi & Roth,
2007). In fact, gesture appears to provide a visuospatial modality for learners to explore ideas that cannot yet be
articulated in speech because the domain is new or the conceptual vocabulary has not been acquired (Alibali &
Goldin-Meadow,      1993;  Roth  &   Welzel,  2001).    In  research on    computer-supported      collaborative learning,
gestures  have been  acknowledged      as an  important    resource  for knowledge     building  although  the   focus has
primarily been on deictic gestures for establishing joint attention and shared reference (Stahl, 2003).        Studies of
small-group science inquiry have examined both deictic and iconic gestures, and found that iconic gestures in
particular can provide support for spatial reasoning and representation of information that could be shared and
negotiated by the group (e.g., Radinsky et al., 2008; Roth & Lawless, 2002). The studies by Radinsky, Goldman,
and Singer (2008; Singer et al., 2008) also examined tracing gestures, although traces were grouped with deictic
gestures, thus highlighting their indexical function. Tracing gestures can have both deictic and iconic elements
(Goodwin,    2003), and  in this study    we use  the term    "iconic-tracing"  to highlight  the  dual function  of such
gestures  to  publically index  an  object   visualized  in the environment,     and   to represent  the  visuospatial    or
dynamic characteristics of the object.

Study Design and Methods
29 triads (mostly   undergrads   and   high  school   students) were    given  the task   of  creating optimal   rainwater
infiltration solutions by placing gardens, called swales, on an urban map. Groups were given twelve minutes to
create multiple swale configurations, which they tested by viewing a simulation based upon their placements.
The groups had to balance ground rainwater infiltration against swale cost. To achieve a high combined score,
the participants had to become more effective at placing swales in locations that would more efficiently capture
more of the rainfall. The rain fell evenly across the map in the simulation, but the combination of ground
elevation and man-made features (like paved roads and water-diverting sewers) produced emergent flows and
pools  of surface   water.  Participants  could   see   these flow   paths  and   pools   by  watching  the simulation's
visualization as it ran. A simulation run terminated automatically when there was no longer any surface water
(having either drained into sewers, infiltrated into the groundwater supply via swales, or flowed off of the map
through a "sink" - a low-elevation point at the edge of the simulated map).
          Groups   performed   this task  in three input   conditions   (tangible  interface,  single-mouse,   and multi-
mouse). A repeated-measures-with-rotation design was used to alternate the order of input conditions as well as
to alternate the specific urban maps. Participation was incentivized by cash paid to each member based on the
distance of the group's best combined infiltration and swale cost scores from the ideal solution. In the tangible
interface condition participants were given physical tokens representing swales that they could place on a paper
map   representing  the  landscape.    An overhead    camera  recorded     the configurations   and  sent the  input   to a
computer which ran the simulation for the participants to view on a display placed behind the map. In the mouse
input conditions, participants used one or three mouse controllers to place swales on the landscape represented
on the screen of a shared laptop. The results of all configurations were exported at the click of a button to the
simulation computer, so in all conditions the simulation run was witnessed at the same viewing angles and
screen size. The three maps given to participants were determined to be equally difficult but different in surface
details to mitigate a practice effect.
          We wanted to know whether or not the participants became any better at placing the swales in response
to the observable emergent patterns in surface water flow, so we constructed a "dynamism" measure for each
grid square on each map. We defined "dynamism" to be the amount of water inflow each grid square received
from its neighbors over the length of the simulation run, in gallons, which we then normalized by dividing by
the highest inflow value obtainable on that particular map (these maximum values were roughly equivalent
across the   three maps).   To  assess  participants'   placements   in  a  particular trial, we   computed   an  average
normalized dynamism value, summing the normalized dynamism values of all of their chosen swale locations
and dividing by the number of swales placed. A higher average normalized dynamism value indicated that
participants were placing swales in locations that were more effective at trapping water for infiltration, whereas
lower dynamism values indicated that participants were less successful in placing their swales in locations that
would  intercept   surface  water.  We    noticed  that  participants   produced   solutions   that had   higher  average
normalized dynamism in the tangible condition than the other conditions (see results section) which prompted us
to select two representative cases to examine more deeply.
          All sessions   were  videotaped    in  order  to  quantify    the number     of different  configurations    and
qualitatively analyze the conversations and actions. For the case study (Yin, 2003), we selected two groups that
were representative of the higher average normalized dynamism observed in the tangible condition, but which
were different in all other ways. They were of different age groups (undergraduate vs. high school) and genders
(males vs. females), and experienced the tangible condition in opposite order (tangible, single mouse, multi-
mouse  vs.   single-mouse,  multi-mouse,     tangible). They   also  showed    different  relative dynamism    values  for

© ISLS                                                                                                                 265
CSCL 2013 Proceedings                                                          Volume 1: Full Papers & Symposia

single-mouse and multi-mouse (higher for single-mouse in Group 1, higher for multi-mouse in Group 2) despite
their mouse conditions being ordered identically (single-mouse followed by multi-mouse). We transcribed and
coded the videotaped sessions for the two groups using a multimodal format adapted from Goodwin (2003) in
order to examine both speech and gesture. We also segmented participants' speech into utterances following a
procedure similar to Kintsch (1998) where each utterance is defined as a meaningful unit that expresses a
proposition or sentiment. Therefore, even when a participant expressed agreement through a single word (e.g.,
"okay") in  a  single conversational  turn, this  was   counted as  an utterance.  Utterances were then   coded  for
evidence of contributions to collaborative problem solving for the following categories: 1) asserting an idea, 2)
expressing agreement, and 3) expressing disagreement.
        Gestures were annotated following a procedure modified from McNeill (1992) for their timing with
utterances, type, and description. Gesture types included deictic, metaphoric, iconic, and iconic-tracing gestures.
As explained earlier, we added an "iconic-tracing" category to emphasize both the emergent deictic aspect for
actively pointing out references, and the iconic aspect to describe the visuospatial or dynamic characteristics of
the references. Iconic gestures were distinguished by their holistic representation of an object, whereas iconic-
tracing gestures were distinguished by their schematic use to trace out the shape or flow of an object. Purely
iconic and metaphoric gestures did not appear significantly in the data and are not included in the analysis.

Findings
In this section, we first present the overall quantitative dynamism results, followed by the qualitative findings
for Group 1 and Group 2. These cases are presented separately with an overview of the three sessions' group
dynamics and gesture use, followed by a more detailed presentation for the tangible interface session. In the
final section, Groups 1 and 2 are discussed together. Note that group members are referred to by the Red, Green,
or Yellow color wrist bands worn during the sessions (e.g., Group 1 members are Yellow1, Green1, and Red1).

Quantitative Dynamism Results
The average normalized inflow values seen across all 29 groups of three participants was 0.074 (SD = 0.087) for
paper,  0.065  (SD  =  0.088) for  multi-mouse,    and  0.046 (SD   =  0.045)  for single-mouse,  which   showed  a
significant effect of interface style  on   the ability to target  high-dynamism    locations for swale   placement
[F(2,581) = 7.15, p < 0.001]. Post hoc comparisons using the Tukey HSD test indicated that the paper condition
differed significantly from the single mouse condition (p < 0.01), although there were no significant differences
between the paper and multimouse or between the multimouse and single mouse conditions.

Group 1 Overview of Three Sessions
Group 1 consisted of three male college students. The order of conditions that Group 1 received was: 1) tangible
interface,  2) single mouse,    and  3)  multi-mouse.      Table  1 below   presents  numeric    summaries   of  the
communicative output across the three conditions. From this data, it is evident that Yellow1 dominated for
utterances in every session, and for gestures in the second and third sessions--although mostly with deictic
gestures. Red1 produced the most gestures in the first session with many iconic-tracing gestures, but both his
utterances and gestures dropped precipitously over the following two conditions. Green1 consistently produced
a moderate number of utterances and smaller number of mostly deictic gestures, and appeared to grow more
assertive over the three sessions.

Table 1. Group 1: Communicative output per condition

                       Tangible Interface               Single Mouse                     Multi-Mouse
Data                   Yellow1      Green1      Red1    Yellow1     Green1    Red1       Yellow1   Green1     Red1
Total # of             74           47          55      54          32        23         73        65         56
Utterances
Asserting Idea         10           8           5       11          9         3          9         8          4
Express Agreement      7            4           16      8           5         10         7         5          15
Express                4            4           3       4           4         1          3         4          3
Disagreement
Total # of Gestures    13           7           16      11          7         3          7         2          2
Deictic                10           6           9       8           7         3          7         2          1
Iconic-tracing         3            1           7       3                                                     1

Fortunately the video data provides rich detail about how Red1's iconic-tracing gestures helped him to consider
dynamic spatial information, and assert his ideas verbally to the group. As will be described further below,
Red1's  assertion  of his main  idea  in the tangible   interface session also  effected an  interesting moment  of
convergence and agreement in the group.

© ISLS                                                                                                          266
CSCL 2013 Proceedings                                                          Volume 1: Full Papers & Symposia

Group 1, Session 1: Tangible Interface
In Group1's  first trial, Yellow1    led by stating his   understanding of the pattern of rainwater absorption  he
observed in the first run of the simulation, and by correcting Green1's orientation, who then realized he had
incorrectly transposed the landscape visualized on the simulation screen to the landscape represented on the
paper map. Yellow1 also initiated placing the first swales and suggesting that they pursue a strategy of either
lining a street with swales      or alternating swales in  a  checkerboard pattern (see Table  2, line 1). Green1
immediately joined Yellow1 in placing swales, but Red1 hesitated and made his first assertion (line 2) which
was not picked up by the others to spread out the swales more. This early group interaction (Table 2) shows
Yellow1 and Green1's agency in action and Red1's attempt to make an assertion which was ignored. Also, Red1
provides an early verbal expression of an idea about `spreading' that he only reasserts successfully later after
first inscribing it in gesture.

Table 2. Group1: Articulating first ideas and actions.

1   [02:18.20] Yellow1: (Yellow1 and Green1 are placing swales on map as Yellow1 is talking) So we should
    create this and see if we just need lines? Or do we want to alternate? [02:23.04]
2   [02:23.05] Red1: Or do you wanna spread em out maybe?... [02:25.00]
3   [02:25.13] Green1: Actually [02:26.03]
4   [02:26.10] Yellow1: Or to go, go opposite?[ 02:26.19]
5   [02:33.00] Green1: We also got quite a bit over here [02:33.08]
6   [02:36.04] Red1: (Still has not made any placement yet) Yeah (pause) should we...[ 02:40.23]

         Over the next trial, Red1 produced gestures as if considering the flow of water over the landscape but
accompanied by minimal or no speech. Such gestures indicate that they were made for thinking (McNeill, 2005)
rather than for speaking. In Table 3, frame 3a (below), Yellow1 had just finished pointing out how the water
ends up in the lower left quadrant of the map (not pictured) when Red1 moved his left hand in a wave-like
motion to start an iconic-tracing gesture at [06:41.01] before he actually spoke at [06:42.20] saying only, "and
um..." Through this gesture, Red1 appeared to be considering the emergent flow of water as he held his left
hand palm down and fingers splayed out, moving over the line of swale placements down towards the left
quadrant Yellow1 had just pointed out.

Table 3. Red1 Gestures for thinking examples.

                          R

          G                            Y

       3a.                                                       3b.
[06:41.01] Red1: {... and um...} (Red1 iconic-            [08:48.20] Red1: {um I don't feel like...} (Red1
tracing gesture, starts speaking at [06:42.20])           iconic-tracing gesture)

         Two minutes later in Table 3, frame 3b (see above), Red1 moved his left hand out along the left line of
swale placements again and said, "um I don't feel like..." With this iconic-tracing gesture, Red1's palm is
vertical to the map and loosely cupped, as if both considering the impact of the water flow down the street and
how much could be contained (cupped image) in this area. Table 4 below shows how Green1 then asserted a
disagreement with the current configuration (line 1), which gave Red1 an entry for asserting his idea leading in
with a pointing gesture (line 2) before speaking (line 4). As soon as Red1 pointed out the area of concern (lines
4, 5), there was an immediate convergence of overlapping speech, agreement, and pointing gestures by all three
group members. Red1then more fully articulated his idea for spreading out the swales (line 9), and making it "a
gradual change" (lines 15, 16), which echoed his initial assertion that was ignored at [02:23.05] (Table 2) to
"spread them out." However, Red1's idea and reasoning is more clear here, and presented an effective spatial
strategy for reducing     the   emergent water   flow  to the zone that  Yellow1   and  Green1 had  lined  with a
checkerboard of swales.

Table 4. Red1 articulates idea of "spreading" and "gradual change."

© ISLS                                                                                                        267
CSCL 2013 Proceedings                                                         Volume 1: Full Papers & Symposia

1. [08:48.24] Green1: There's a little bit too much.
2. [08:49.20] Red1: {Yeah} (moving right hand in, pointing gesture);
3. [08:50.21] Yellow1: Okay.
4. [08:54.04] Red1: ...{So let's, um, right here like, right here} (long pause)

                R
    G                              Y

5. [08:56.04] Red1: [{seems like        9. [08:58.10] Red1: that we could      12. [09:04.14] Yellow1: Let's
right here}].                           easily just take it away (pause)       spread this out maybe, we can get
6. [08:56.04] Yellow1:[{These two       10. [09:00.17] Red1: [like that].      rid of this one?
are neighboring}].                      11. [09:00.17] Green1: [also, it       13. [09:07.16] Green1: Yeah seems
7. [08:56.10] Green1:[{Yeah}].          didn't seem] to have much of an        like that would be better (Red1 and
8. [08:56.20] Red1:[{They're so         effect in that area.                   Green1 look up and check the
close] and that's neighboring}                                                 simulation).

14. [09:10.22] Red1: Yeah               15. [09:12.00] Red1: at least make     16. [09:13.26] Yellow1: Mm-hm
{because its concentration is           {it like a gradual (pause) change}.    (agrees).
centered here}

Group 2 Overview
Group 2 consisted of three female college students. The order of conditions that Group 2 received was: 1) single
mouse, 2) multi-mouse and 3) tangible interface. Table 5 presents a summary of the communicative output for
all three  conditions. From   this data it appears that Green2    dominated  in   every session for utterances, but
gestured  only in  the  first two  mouse   input sessions    with mostly  deictic pointing gestures.   Yellow2  was
consistent over the sessions but produced more utterances and gestures in the mouse conditions than Red2, who
was inconsistent by producing many utterances in the first session (although few gestures because she was in
control of the single mouse), but then dropping in utterances and gestures in the second session. In the third
session, Red2 dramatically increased her number of utterances and produced a large number of iconic-tracing
gestures, unlike her partners.
          From the video data for Group 2, it is evident that Red2 began to use iconic-tracing gestures in the third
(tangible) session to better articulate her thinking about the emergent spatial flows of rainwater. Over the three
sessions, Red2 appeared to have difficulty in verbally articulating her spatial reasoning, but by the tangible
interface session, Red2 became more insistent and better able present her ideas using iconic-tracing gestures as a
support.

Table 5. Group 2: Communicative output per condition.

                               Single Mouse                     Multi-Mouse                 Tangible Interface
Data                    Yellow2      Green2   Red2      Yellow2     Green2   Red2       Yellow2     Green2   Red2
Total # of              60           103      89        69          126      38         40          93       78
Utterances
Asserting Idea          11           18       6         5           16       5          6           18       13
Express Agreement       9            5        8         8           5        5          9           5        8
Express                 1            6        2         0           7        2          1           14       8
Disagreement
Total # of Gestures     23           42       4         10          14       5          5           13       26
Deictic                 21           35       2         9           12       3          5           13       12
Iconic-tracing          2            7        2         1           2        2                               14

© ISLS                                                                                                          268
CSCL 2013 Proceedings                                                     Volume 1: Full Papers & Symposia

Group 2, Session 3: Tangible Interface
In this session, Green2 made the first move by asserting her idea of placing swales on the "dark spots" (i.e., low
elevation locations) which led to a strategy pursued by all three group members to fill up an entire quadrant on
the map with swales, creating a catchment effect (where water would pool over a cluster of swales). However,
after viewing the simulation results from the initial swale configuration, Red2 referred to the edge of the map
closest to her and tried to assert a different idea. At [07:12.03] she said, "Let's just try..." but because Yellow2
and Green2 were busy talking, she went ahead and started placing swales near her edge. Red2 was more specific
at [08:29.26] with, "I don't know... I really want it along the edge though right here (pause) really, really, really
want that!" However, when Red2 added iconic-tracing gestures, she started to elucidate her reasoning.
          Table 6 below provides examples of these gestures and the added spatial information that supported her
reasoning. At [08:48.03] (Table 6, frame 6a), Red2 first argued, "Because I saw the water coming down like
this," while moving her right hand in a waving motion over the middle section of the paper map towards her
edge. About 20 seconds later (not pictured in Table 6), Red2 then repeated this gesture for herself without
speaking. By [12:13.21] (Table 6, frame 6b), Red2 had picked up more emergent information about the water
flows and used two hands to trace the flows down farther along the right side of the map, and then with two
cupped hands to move along her edge. Red2 was more sensitive to the location of the flows now and why there
was so much pooling towards her edge. While Green2 had been consistently arguing against Red2's concern for
the edge, by [14:03.04] Green2 gave in and agreed, although out of friendly exasperation, "Alright! Just try
things!" In Group2, although there wasn't a clear moment of group convergence, Red2's use of iconic-tracing
gesture enabled her to sensitize to emergent spatial flows in the system and better articulate her spatial reasoning,
which perhaps gave her the confidence to keep asserting her ideas until they became incorporated by the group.

Table 6. Red2 adds spatial information to her gestures.

                     R
   G                         Y

6a.                                   6b.                                    6c.
1. [08:48.03] Red2:  Because I saw    1. [12:13.21] Red2: The water at       2.[12:14.11] Red2: ...and onto the
the water coming down like this.      the end was towards the edges...       streets.
                                                                             3.[00:12:19.05] Red2: I don't think
                                                                             it was up there though (to Yellow2).

Discussion
As mentioned earlier, we chose Groups 1 and 2 to compare on the basis of having similar dynamic outcomes
despite different factors like age or gender or receiving the tangible interface condition in opposite order, which
potentially rules out the effect that familiarity with the task would have on participant performance in a given
input condition. Nevertheless, we found similar interaction patterns in the tangible interface condition for the
two groups in that the least dominant group member of each group, Red 1 and Red2, experienced failure at first
in articulating and asserting their ideas verbally, but then succeeded after using iconic-tracing gestures to pick
up and describe emergent spatial information. What were the particular representational affordances of the
tangible interface that enabled this iconic-tracing gesture based spatial sensitivity and reasoning? In both cases,
it appeared that Red1 and Red2 visualized patterns of water flows over the paper map, indicating that the paper
map provided a material support for their visualization. From a distributed cognition framework, Hutchins (2005)
describes how material forms serve as material anchors for stabilizing conceptions, and the paper map appeared
to serve this function. Similarly, the physical swale tokens also provided material anchors for visualizing water
infiltration strategies. The physicality of the tokens may even have provided a representation of depth to the
landscape. In fact at one point, Red2 began to stack the physical swale tokens on top of each other because she
wanted to address the greater need an area where water was pooling. But even given the ways that the tangible
interface functioned as a material anchor, it was nevertheless the use of iconic-tracing gestures that coordinated
between what would be a mental visualization and the material anchor for the visualization. This coordinating
function of gesture in bridging between the imagination and the material world has been noted in cognitive
linguistics studies (e.g., Williams, 2008), and deserves   further attention in  educational technology design
research.
          Another similarity between Red1 and Red2 was their focus upon the pattern of emergent water flows
over the landscape, although Red1 identified a different spatial strategy than Red2. Red1 expressed a "spread
out" idea that reflected a sponge-like solution, anticipating the flow of rainwater and absorbing it as early as

© ISLS                                                                                                    269
CSCL 2013 Proceedings                                                         Volume 1: Full Papers & Symposia

possible through a gradual distribution of swales. Red2 expressed a barrier-like solution of adding a cluster of
swales to a point right before her catchment area to absorb and redirect the influx of rainwater. In fact, there was
actually no single best solution for the rainwater infiltration problems presented to the groups because solutions
depend upon both fixed landscape features like the gradients, streets, and sewer locations as well as the dynamic
swale placements (e.g., the swale "barrier" Red 2 created in Table 6, frame 6b served to both absorb and divert
the surface   flow  into her catchment   area). Therefore  the better  solutions  could  only come  about  through
sensitivity to emergent patterns of water flow observed in the simulation, which entails a sensitivity to how
patterns unfold over time.
         The positive impact of both Red1 and Red2's contributions to group spatial problem solving in the
tangible interface condition was also evident in the quantitative dynamism measures (Table 7). The dynamism
values are highest for both groups in the tangible interface condition, mirroring what was seen across the 584
trials generated by the other 81 participants in the study. For Group 1, the effect of the interface is significant
according to a one-way ANOVA [F(2,14) = 138.76, p < 0.0001], with a post hoc Tukey HSD test confirming
significance for Tangible vs. Single-mouse (p < 0.01), Tangible vs. Multi-mouse (p < 0.01), and Single-mouse
vs. Multi-mouse (p < 0.01). For Group 2, although the interface condition did have a significant effect [F(2,23)
= 11.26, p < 0.001], this held only for the Tangible vs. Single-mouse (p < 0.01) and Multi-mouse vs. Single-
mouse (p < 0.01).

Table 7. Dynamism measures: Groups 1 and 2.

                         Group 1                                                    Group 2
Session 1:          Session 2:          Session 3:         Session1:           Session 2:          Session 3:
Tangible            Single mouse        Multi-mouse        Single mouse        Multi-mouse         Tangible
0.0464              0.0246              0.0181             0.0077              0.0270              0.1108

Implications and Conclusion
This study's findings indicate that a tangible interface provides a beneficial affordance for gesture-mediated
spatial reasoning, especially for less verbally participatory or assertive group members in the context of group
solving  of a complex    spatial problem  modeled    on a computer  based  simulation.   As a  modality for  spatial
reasoning and communication, gesture (and iconic-tracing gesture in particular) increased the communicative
agency of less dominant group members and resulted in equalizing group participation. In addition, the greater
sensitivity to emergent spatial information that these less dominant group members articulated through gesture
first and then speech had the effect of improving group spatial problem solving performance. While the role of
gesture  in spatial reasoning    and group  communication     has been studied   before, we   have focused  on  the
particular benefit of iconic-tracing gestures for working with emergent patterns, and how a tangible interface
supported such gesture use.
         We speculate that the materiality of the elements of the tangible interface, including the paper map and
physical tokens, provided a material anchor (Hutchins, 2005) for stabilizing what would otherwise be only
mental visualizations of emergent water flows and varying spatial features of urban landscapes, and that iconic-
tracing gestures provided the bridge to link participants' visualizations to these material anchors. Understanding
tangible interfaces in terms of material anchors and the use of gesture brings useful elements from distributed
cognition and cognitive linguistics frameworks into the design of interfaces for computer based group learning.
       Recently, several studies have demonstrated that students who were required to imitate the gestures that a
teacher performed for solving algebraic math problems did significantly better on subsequent tests than controls,
and researchers speculated that the visuospatial reasoning strategies inscribed in gesture provided the benefit
(Goldin-Meadow et al., 2009). To extend our current findings, we propose to borrow this strategy of intentional
gesture by testing the current simulation in science classrooms with teachers intentionally modeling specific
iconic-tracing gestures when introducing the simulation to students. In addition, teachers could simultaneously
add conceptual vocabulary to the gestural communication (e.g., words such as `surface flow' and `catchments'),
which would ground the terminology in relevant multimodal imagery (Alibali & Nathan, 2011).               Similarly,
another possibility for future research is to design a tangible interface that necessitates the performance of
iconic-tracing type  gestures  for manipulating  the  spatial parameters   of the ecosystem   simulation.  The  one
implication that is important to communicate from this study is that both individual group members and rest of
the group can be helped by more opportunities to draw upon the dual function benefits of iconic-tracing gesture
for both  actively  reasoning    about emergent  spatial  phenomena    and communicating      and  coordinating the
reasoning with others. The emergent properties of complex systems have proven to be challenging for many
learners (Sweeney   &    Sterman,  2007) and if  gesture  can  provide an  integral modality  for  reasoning  about
emergent    phenomena,    then   the design  of  educational   interventions--whether    in the   form  of  teacher
communication strategies or tangible interfaces for computer based simulations--should consider how iconic-
tracing and other types of gestures can play a role.

© ISLS                                                                                                          270
CSCL 2013 Proceedings                                                          Volume 1: Full Papers & Symposia

References
Alibali, M., & Goldin-Meadow, S. (1993). Gesture-speech mismatch and mechanisms of learning: What the
         hands reveal about a child's state of mind. Cognitive Psychology, 25(4), 468­523.
Alibali, M.  (2005).  Gesture   in spatial cognition:  Expressing,   communicating,    and  thinking about spatial
         information. Spatial Cognition & Computation, 5(4), 307­331.
Alibali, M., & Nathan, M. J. (2011). Embodiment in mathematics teaching and learning: Evidence from learners'
         and teachers' gestures. Journal of the Learning Sciences, 1­40.
Antle, A.  N.,  Droumeva,   M.,  &  Ha,  D.   (2009). Hands on   what?:  comparing   children's mouse-based   and
         tangible-based interaction. In Proc. IDC  '09, ACM,   80­88.
Goldin-Meadow, S. (2003). Hearing gesture: how our hands help us think. Cambridge, MA: Harvard Univ.
         Press.
Goldin-Meadow, S., & Beilock, S. L. (2010). Action's Influence on Thought: The Case of Gesture. Perspectives
         on Psychological Science, 5(6), 664­674.
Goodwin, C. (2003). Pointing as situated practice. In S. Kita (Ed.), Pointing: Where language, culture, and
         cognition meet (pp. 217­242). Mahwah, NJ: Lawrence Erlbaum Associates.
Hutchins, E. (2005). Material anchors for conceptual blends. Journal of Pragmatics, 37(10), 1555­1577.
Kintsch, W. (1998). Comprehension: A paradigm for cognition. NY: Cambridge Univ. Press.
Maher, M., & Kim, M. (2005). Do Tangible User Interfaces Impact Spatial Cognition in Collaborative Design?
         In Y.   Luo  (Ed.), Cooperative    Design,   Visualization, and Engineering,    (3675)  30­41.   Springer
         Berlin/Heidelberg.
Marshall, P., Hornecker, E., Morris, R., Sheep Dalton, N., & Rogers, Y. (2008). When the fingers do the talking:
         A study of group participation with varying constraints to a tabletop interface. In Proc. of Tabletop 08,
         37-44.
McNeill, D. (1992). Hand and mind: what gestures reveal about thought. Chicago: University of Chicago Press.
McNeill, D. (2005). Gesture and thought. Chicago: University of Chicago Press.
National  Research   Council  (2006).   Learning  to  Think Spatially:  GIS    as a Support   System  in the K-12
         Curriculum. Washington, D.C.: National Academies Press.
Pozzer-Ardenghi, L., & Roth, W. (2007). Performing concepts during science lectures. Science Ed., 91(1), 96­
         114.
Radinsky,   J., Goldman, S., &   Singer,   M. (2008).  Students' sense-making     with visual data in small-group
         argumentation. In Proc. ICLS'08, 237­245.
Rogers, Y., Lim, Y., Hazlewood, W., & Marshall, P. (2009). Equal Opportunities: Do Shareable Interfaces
         Promote More Group Participation Than Single User Displays? Human-Computer Interaction, 24(1),
         79­116.
Rogers, Y., & Lindley, S. (2004). Collaborating around vertical and horizontal large interactive displays: which
         way is best? Interacting with Computers, 16(6), 1133­1152.
Roth,  W.-M.,   &  Lawless,  D. (2002). Scientific investigations, metaphorical   gestures, and the  emergence of
         abstract scientific concepts. Learning and Instruction, 12, 285­304.
Roth, W.-M., & Welzel, M. (2001). From activity to gestures and scientific language. Journal of Research in
         Science Teaching, 38(1), 103­136.
Singer, M., Radinsky, J., & Goldman, S. R. (2008). The Role of Gesture in Meaning Construction. Discourse
         Processes, 45(4-5), 365­386.
Stahl, G. (2002). Contributions to a theoretical framework for CSCL. In Proc. CSCL'02, 62­71.
Sweeney, L. B., & Sterman, J. D. (2007). Thinking about systems: student and teacher conceptions of natural
         and social systems. System Dynamics Review, 23(2-3), 285­311.
Williams, R. F. (2008). Guided conceptualization: Mental spaces in instructional discourse. In T. Oakley & A.
         Hougaard   (Eds.),  Mental  Spaces   in Discourse  and    Interaction (pp.  209­234).  Amsterdam:   John
         Benjamins.
Yin, R. (2003). Applications of case study research (2nd ed.). Thousand Oaks: Sage Publications.

Acknowledgements
This work is funded by National Science Foundation REESE grant 1020065.
Design and implementation of simulation and experiment by Tia Shelley, Chandan Dasgupta, and Brian Slattery.

© ISLS                                                                                                        271
