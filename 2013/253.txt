CSCL 2013 Proceedings                     Volume 2: Short Papers, Panels, Posters, Demos, & Community Events

          Supporting Feedback Uptake in Online Peer Assessment
                              Alexandra. L. Funk, Astrid Wichmann, Nikol Rummel,
Institute of Educational Research, Ruhr University Bochum, Universit‰tsstraﬂe 150, 44780 Bochum, Germany,
              Email: Alexandra.Funk@rub.de, Astrid.Wichmann@rub.de, Nikol.Rummel@rub.de.

          Abstract:
          Starting point for the present study were students' problems in academic writing. The goal of
          the study is to improve students' writing in a peer assessment setting. Sixty-seven students
          participated in an online writing task. We investigate whether providing sense-making support
          during feedback reception leads to increased feedback uptake, better revisions of the original
          text, and improved writing skills, as compared to a condition without sense-making support.

Introduction
Writing   and revising   scientific texts is     a  challenging task   for  university students. The main   challenges  for
inexperienced writers are to detect errors in the text and to find appropriate ways to revise the text (Hayes,
2004). Receiving feedback early on in the writing process is important, but often students fail to capitalize on
the feedback they receive as they do not take it up (Van der Pol, Van den Berg, Admiraal, & Simons, 2008).
Problems with understanding the feedback and lack of reflecting on the feedback can be cited as reasons for the
lack of feedback uptake. Support is needed to help students in making sense of feedback with the goal to
prevent rejection    of the feedback,   to increase     elaboration     and understanding  of  the feedback,   and thus  to
improve feedback uptake.
          Peer assessment scenarios can help to meet the challenges of academic writing, because peer feedback
can be  provided     more   timely  and   more      frequently  than   feedback of the   course  instructor  (Falchikov  &
Goldfinch, 2000). Typically, peer assessment starts with a task asking the assessee to create a product (i.e. an
academic text). Secondly, the product is reviewed by one or several assessors resulting in feedback provision.
Thirdly, assessees receive feedback. And lastly, assessees revise their own product based on the feedback of the
assessors. Unfortunately, peer assessment activities do not necessarily lead to learning (Kollar & Fischer, 2010).
Major problems during peer assessment are related to the assessee's failure of feedback uptake (Van der Pol et
al., 2008). Feedback uptake refers to changes made to the assessee's product during revision that are clearly
based on and related to received feedback. Feedback in academic writing typically includes comments about
problems in the text and suggestions for revision. But even if students receive comments that point toward
errors in the  text, they   have problems        to capitalize on  the  feedback. Writers  often   reject feedback upfront
without engaging in sense-making processes or have problems with managing the feedback (Boero & Novarese,
2012).  Sense-making     processes   are  crucial,    however,    because   understanding  the   problems   is decisive for
improving performance in peer assessment (Schunn & Nelson, 2009).
          We expect that providing assessees with sense-making support during feedback reception will improve
feedback uptake. The hypothesis with regard to academic writing is that better feedback uptake would lead to
better revisions of the original text and yield improved writing skills. This hypothesis is investigated in the
present study.

Method
Altogether, 67 students (13 male, 54 female) served as participants. The students were recruited from three
courses of the bachelor program in educational sciences at a major German university. Students participated in
the study as part of their regular course activities. The study followed an experimental design with sense-making
support as independent variable. Students were randomly assigned to one of two conditions: Sense-Making
Support condition and No-Sense-Making Support condition. The sense-making support aimed at encouraging
the student to reflect on the feedback. Students were asked to rank and to judge received feedback and to plan
their corresponding revisions (Figure 1).

                I understand   I agree with the     I am going to use    I am going to improve my essay by doing the following:
  Comment      the comment.       comment.           the comment to
                                                      revise my text.

                Yes      No      Yes      No          Yes       No
Comment 1
Comment 2
General Questions
In my opinion the main points of critique are...
                                           Figure 1. Sense-Making Support

© ISLS                                                                                                                  253
CSCL 2013 Proceedings                  Volume 2: Short Papers, Panels, Posters, Demos, & Community Events

The writing task was embedded in an online peer assessment activity that was conducted over a period of 10
days. The activity consisted of three phases. During phase (1) Essay Writing, participants created an essay of
600 words, in MS Word. The content of the essay was related to identity formation, a well-known concept in
psychology. During phase (2) Feedback Reception, each student received comments on their essay from an
assessor (Figure 2). The feedback included 12 comments on five writing criteria for mistakes that frequently
occur in  students' written  products:  Sequence/Logic   of  Argument,    Transition Words, Nested   Sentences,
Direct/Clear Reference and Filler Words. Students were told that the feedback was given to them by a peer,
however, in reality the feedback was given by the first author of this paper and by trained tutors in order to
control for the amount and kind of feedback. Note that all participants only took the role of the assessee. The
experimental variation (Sense-Making Support vs. No Sense-Making Support) was implemented during the
Feedback Reception phase. During phase (3) Revision, students revised their essays based on the comments
they had received. We used Moodle (Moodle, 2013) to distribute instructions and questionnaires. Also, students
submitted their essays and revised essays to this platform. Feedback uptake, improvement of text quality and
writing skills (Pre-Post) were assessed as dependent variables.

                  Figure 2. Sample comment given to the students as peer feedback

Analysis & Results
The data are currently being analyzed. Feedback uptake is assessed by analyzing quantity of revisions, quality
of revisions, and self-perception of feedback use. Improvement of text quality is assessed by comparing the
quality of the original essays to the quality of the revised essays. Writing skills are assessed using parallel pre-
and posttest versions. The pre- and posttests assessed two distinct skills related to academic writing: problem
detection and  problem correction.   The test consisted  of a   text with 10 erroneous  passages. For assessing
problem detection skills, students had to highlight and label the problems in the text. For assessing problem
correction skills, participants were asked to correct erroneous text passages. Errors were again related to the 5
writing criteria representing common mistakes. By analyzing the relationship of feedback uptake, on the one
hand and improvement of text quality and writing skills on the other hand, we will be able to evaluate the
benefits of feedback uptake with regard to improving student procedural and declarative writing skills. Results
will be presented at the conference.

Significance and Contribution of Research
The present study will contribute to theory building in the area of online peer assessment and to research on the
acquisition of writing skills in university settings. On the practical side, our findings will provide university
teachers with guidelines for supporting students in the assessee role in online peer assessment scenarios.

Acknowledgments
This research is supported by the German-Israeli Foundation for Scientific Research and Development (1090-
25.4/2010). Special thanks to our project partners Miky Ronen, Moshe Leiba, Dan Kohen-Vacs and Ronen
Hammer from Holon Institute of Technology, Israel.

References
Boero, R., & Novarese, M. (2012). Feedback and Learning Encyclopedia of the Sciences of Learning (pp. 1282-
  1285).
Falchikov, N., & Goldfinch, J. (2000). Student peer assessment in higher education: A meta-analysis comparing
  peer and teacher marks. Review of Educational Research, 70(3), 287-322.
Hayes, J.R. (2004). What triggers revision? In L. Allal, L. Chanquoy & P. Largy (Eds.), Revision: Cognitive and
  instructional processes. (pp. 9≠20). Dordrecht, The Netherlands: Kluwer.
Kollar, I. & Fischer, F. (2010). Peer assessment as collaborative learning: A cognitive perspective. Learning
  and Instruction, 20(4), 344-348.
Moodle    Pty. Ltd.  (2013). Moodle     (Version  2.3.3) [Learning     environment   software]. Retrieved    from
  https://moodle.org/.
Nelson, M.M. & Schunn, C.D. (2009). The nature of feedback: how different types of peer feedback affect
  writing performance. Instructional Science, 37, 375-401.
Van der Pol, J., Van den Berg, B. A. M., Admiraal, W. F., & Simons, P. R. J. (2008). The nature, reception, and
  use of online peer feedback in higher education. Computers and Education, 51, 1804-1817.

© ISLS                                                                                                        254
