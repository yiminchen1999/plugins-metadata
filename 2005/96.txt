Researching "Collaborative Knowledge Building" in
             Formal Distance Learning Environments
               Vanessa Paz Dennen                                             Trena M. Paulus
Educational Psychology & Learning Systems                        Educational Psychology and Counseling
             Florida State University                                      University of Tennessee
                  vdennen@fsu.edu                                             tpaulus@utk.edu

     Abstract. Distance learning environments provide a rich opportunity for collaborative knowledge
     building, particularly through peer-to-peer dialogue. Much of the discussion in distance learning
     environments occurs in asynchronous forums, and it is content analysis of these discussions that
     constitutes the majority of research in online learning. However few studies in this area provide
     enough information about the context to know what works and what doesn't. Most studies do not
     go beyond downloading and analyzing the transcripts after the course is completed. Studies also
     lack    a solid epistemological   stance,    attempting to  capture evidence   of individual   learning of
     knowledge rather than examining the process of group learning through knowledge construction.
     An ongoing lack of attention to a coherent theoretical foundation, examining transcripts without
     attending    to   their situated contexts,    and  relying  primarily on  reductionist content    analysis
     methods,     will   continue to  limit our   understanding  of the  potentiality and actuality  of online
     collaborative learning environments. In this paper we explore how Stahl's social theory of CSCL
     can be applied to formal online learning environments to address these limitations.

     Keywords: Online discourse, collaborative knowledge building, discourse analysis methods

INTRODUCTION
Internet-based  distance  education   is one   avenue   for  computer-supported    collaborative learning.   However,
distance education contexts vary as widely as any CSCL context in terms of learner populations, technology,
learning goals and learning tasks. Research in distance learning environments have tended to be exploratory
case studies in which discussion transcripts are downloaded and analyzed after the course is completed, either
by the instructor of the course or an outside researcher. The full context of the case study is often not described
as recommended by case study methodology (Stake, 1995; Yin, 1984). Analysis has relied mainly on frequency
counts of participant posts and/or coding and counting phenomena through content analysis (Hara, Bonk &
Angeli, 2000; Henri, 1992). The primary purpose of these studies seem to be to account for individual learning
through the discussion process, but as pointed out by Rourke and Anderson (2004), there is often no clear
epistemological stance taken as to what constitutes learning and how we might examine it.
  In this paper we explore limitations of the current research on distance learning environments, particularly
research which relies on analysis of discussion transcripts. We suggest that Stahl's social theory of CSCL may
be used as a theoretical framework for understanding these environments and ultimately designing them more
effectively.

EPISTEMOLOGICAL STANCE
Traditional notions of learning and assessment tend to favor product-based outcomes over process-based ones,
particularly focusing on individual learning. However, assessing process can be just as important as assessing
product, especially  if  educators   want   to know    which  learning  activities and methods   are   contributing to
collaborative knowledge building. In the case of online discussion, it becomes necessary to first determine the
purpose for being engaged in the activity. What is actually occurring as students talk together? Are the students
learning from  the  conversational    interactions,  or are they simply  participating  because  it is what  they   are
supposed  to   do (e.g., because  someone      is counting  how  many   posts they  make)?  And   if their discussion
participation is being assessed, does that assessment focus on quantity of messages, quality of messages, or
evidence of learning through, for example, a process of constructing new knowledge?
  Determining whether or not learning took place as a result of engaging in discussion is not simple. Students
often intuitively orient towards discussion as a "show what you know" activity rather than an "explore this
topic" activity. In other words, they tend to naturally position themselves toward an objective knowledge that
they will try to learn or adopt, assuming the instructor will assess them based on how much of it they have come
know. Course assessment methods, as noted above, often favor such beliefs. However, if the purpose is for
individual students to communicate what they know to an instructor, why do so in a public forum such as a

                                                            
discussion board? Clearly there are other underlying epistemological beliefs or needs that drive our desire to
engage groups of people in discussion on a particular topic.
   There has been some movement away from this transmission view of learning to the notion of learning as
mediated communication between people. Hill, Wiley, Nelson and Han (2004) characterize this difference as
learning "from" and "with" the Internet to learning "through" the Internet. Online discussions are particularly
rich environments for true knowledge creation to take place, but only when designed in ways that engage
students in dialogue. Thus far most studies do not clearly delineate what is meant by learning. Rather than
viewing participation alone to be evidence of learning, we define learning as a process, demonstrated through
conversation,   in which learners   reflect upon   what   they currently know   and negotiate new   meaning    and
knowledge creation with others through conversation. Together groups come to new understandings through
conversation. Closely examining these processes is how we assess learning.

PROMINENT METHODS AND LIMITATIONS
At the present time, there is lack of clear guidance for which data collection and analysis methods might best
capture and explain the learning that takes place in online discussions. Campos (2004) points out that "research
goals, theoretical perspectives, and methods vary across studies and are not replicated. The result is a very
heterogeneous corpus of scientific research that could be defined as exploratory" (p. 4). Rourke and Anderson
(2004) also report that most studies remain in the preliminary tryout stage and add that many of these studies
lack normative data to be able to generalize the results.
   A useful start, then, may be to categorize and review some of these studies in terms of what research
questions they address, contexts they examine, and epistemological stance taken. Marra, Moore and Klimczak
(2004), Paavola, Lipponen and Hakkarainen (2002) and Meyer (2004) have initiated work in this area. Marra et
al. (2004) compared two commonly used coding schemes (Newman, Webb & Cochran, 1996; Gunawardena,
Lowe & Anderson, 1997) for their relative advantages and disadvantages. They concluded that it was difficult to
apply these content analysis schemes without looking at the context of the discussion task and the discourse as a
whole, arguing that it is necessary to move beyond looking at only the transcript itself. Meyer (2004) applied
four coding schemes (King & Kitchener, 1994; Perry, 1999; Garrison, Anderson & Archer, 2001; Bloom &
Krathwohl, 1956) to the same set of data and noted that the type of triggering question posed by students in
online discussions greatly impacted the outcomes. Meyer suggested that "it might be worthwhile to analyze the
ebb and flow of online discussions as a group [emphasis added] effort, rather than focusing on the individual
postings as a reflection of the student's level of thought" (p. 112). Finally, Paavola et al (2002) moved beyond
critiquing content analysis schemes to comparing three knowledge-creating models, emphasizing that all three
models posit knowledge as "part of a dynamic process of innovation embedded in various skills, emotions, and
hunches of the people involved . . . [and]bring in conceptual artifacts, theories, activities, questions, problems,
metaphors, dialectics as mediating factors" (p. 12).
   Participation,  content, and   structure of online   conversations all are areas  that are being   explored by
researchers. Each focus yields some useful descriptive results about the interactions that take place, but each
also has significant limitations in terms of what we find about how discussion impacts learning processes.

Participation
The very earliest studies of online discussion typically focused on measuring participation as a primary indicator
of interaction, which  was   sometimes  the   sole determinant   of "learning." Indeed, this  type of focus often
mirrored the instructor's assessment of the discussion activity. One cannot dispute that participation is necessary
in order to have interaction as well as to enable learning via discussion boards, but it is not a given that
participation and interaction will result in learning. Quantity of participation is not the same as quality, and even
quality may be broadly defined, since a good question may be just as important as the answer. For example,
Pear and Crone-Todd (2001) claim to look at social constructivist learning in a computer-mediated setting by
measuring the number of minimal and substantive feedback messages students received from other students.
Unfortunately, this study does not address the effects that such feedback had on the learners, nor does it look at
the quality of the feedback. It is possible that a brief or minimal feedback message could be sufficient in some
cases, particularly if all that is needed is affirmation, and that a substantive one might be too authoritative or
could lead the learner off-track.
   Participation may well be an indicator of social presence (Rourke, Anderson, Garrison, & Archer, 2001a), a
construct that may be important to creating a sense of community among online learners. While we do not
minimize the importance of social presence, it does not in and of itself lead to collaborative knowledge building
through dialogue. Vicarious learning through lurking is another phenomenon that has not been fully explored
and is not accounted for through participation counts (Beaudoin, 2002).

                                                          
Content
A shift from quantity of discussion to quality of discussion emerged most notably with Henri's (1992) oft-cited
coding scheme. Most coding schemes created to investigate quality of online discussions draw upon content
analysis methods (Bauer, 2000), translating discourse into either nominal data (e.g., gender, or type of message)
or ordinal data (e.g., scale or rubric-based quality ratings). These frameworks provide researchers ways of
dealing with potentially large quantities of qualitative data and achieving generalizability, but often suffer from
a tension between the rich qualitative data and the resulting interpretive ­ and often reductionist ­ quantitative
methods.
   Concerns about the threats to reliability and validity inherent in these content analysis frameworks have
recently been raised (Rourke & Anderson, 2004; Rourke, Anderson, Garrison & Archer, 2001b; Campos, 2004).
Studies that are cited quite often in the literature on online discussions (e.g. Henri, 1992; Newman, Webb &
Cochran, 1997; Howell-Richardson & Mellar, 1996; Gunawardena et al, 1997; Kanuka & Anderson, 1998) are
criticized for including too few details about coding procedures, being inconsistent in the units of analysis, and
not detailing a solid epistemological stance.
   All too often the transcripts are simply downloaded and the conversation unitized, coded and counted. At the
same time, these studies are positioned as case studies, but in actuality provide few details about the context of
the study ­ what tasks were being completed, the role of the facilitator, etc. This reductionist view eliminates the
context. Fuller data collection methods are needed to understand the CSCL environment and how learning takes
place ­ not learning as received knowledge, but learning as knowledge creation through interacting with a group
that it is then internalized and interpreted by individuals.

Structure
Discussion boards readily generate both quantitative and qualitative data. In terms of quantitative data, one can
count the number and length of messages, the depth of threading, the span of time between messages and
responses, and the number of hits on a particular message. Each of these data types may be reviewed for the
individual contributor as well as in aggregate for a thread or a group of discussants. They can provide indicators
of the  general structure of  interactions that are taking    place    in a class,  but are really  lacking in terms  of
indicating quality or nature of interactions. For example, a one-sentence message could be a thought-provoking
question or an idle statement of agreement with a previous post, and a long message might present a lot of
useful thoughts and encourage others to contribute or it might become overly pedantic and shut down further
discourse.
   Social network analysis is a useful method for demonstrating the relationships in a given social network
(Scott, 2000).  This method   can be used   in  the context     of online   discussion  to  demonstrate  if discourse is
centered  around  people   in positions of  power,   such    as  the   instructor,  or  individuals with other   notable
characteristics. For example, Aviv, Erlich, Ravid and Geva (2003) used network analysis to examine how power
roles affected engagement in critical thinking activities in differently structured online courses.
They were able to use the method to elucidate cliques that formed within the studied classes and determine who
took leadership roles. However, this method still does not shed light on whether or not students are learning via
their engagement in the discourse and favors visible engagement (i.e., message posting). It can reflect students
who are more dominant or extroverted when it come to argumentation, but does not indicate whether or not their
ideas were well-founded, or if others were learning from them.
   Similar to social network analysis is sequential analysis (Bakeman & Gottman, 1997), which looks not at
how individuals or other social entities interact, but rather at how particular actions or events are sequenced,
with characters or roles being only of secondary concern. Jeong's (2003) Data Analysis Tool uses sequential
analysis to quantitatively describe student interaction patterns in an argumentation-oriented learning context.
Two-message sequences were considered the unit of analysis (e.g., initial message and response) in Jeong's
study, which examined the relationship between sequences and potential indicators of critical thinking skills.
While this method is quite useful to develop descriptive models and demonstrate probabilities of particular
interaction types, such as a statement of agreement following one of disagreement, it does not indicate whether
or not learning is taking place through these interactions. Further, it reduces each message to a particular code
which might be considered oversimplification in some contexts. For example, in Jeong's study a message of
conditional or   partial  agreement  would    be  coded      as neither     agreement   nor  disagreement,   but rather
"negotiation."  Such  messages,   however,    might represent      the true  spirit of  negotiating  meaning   amongst
participants or might simply represent a student with a strong set opinion that does not neatly fit either side of an
argument.

                                                         
APPLYING THE METHODS
In this section we  illustrate  how    the various analysis  methods    can  each lend  some   insight, but    not a
comprehensive picture of how groups learn in a social context. Here is a transcript from an asynchronous
discussion forum:

                  Toilets ,Eddie
                  We, as a society, definitely take running water for granted. A few days ago I
                  had no water because of work on a line. My water was off for about six
                  hours.   Horrible. I couldn't  make   ice  tea, take  a shower    or anything.
                  Eventually   the water   did return and all is  well. Just  think of  the poor
                  countries were the running water never even appears!
                  Re:Toilets, Tanya H.
                  When I was younger, we visited the South and in order to take baths we
                  would go outside and get buckets of water from the well heat the water up
                  and poured it into a white wash tub. Talk about inconvenient. But since I
                  was a little girl I thought that it was fun. As an adult, if I had to do that, I
                  would be annoyed.
                  Re: Toilets, Laney
                  Boy I can relate, we had a back up in our basement due to roots from a tree,
                  and my husband and I wanted to stay at my moms because of no water, how
                  spoiled are we? Does anybody remember when Brownsville had to boil their
                  water due to a bacteria? We all were out purchasing water.
                  Re: Toilets, Clarissa
                  yeah...i do  remember    when  the  people of   Brownsville had   to boil their
                  water. my grandmother lives there, and we had to bring her jugs of water to
                  keep her in comfort.
                  Re: Toilets, Donna
                  I have to agree with you all. I know that I do take the modern conveniences
                  for granted.
An examination of this brief thread yields different results based on the analytic framework used as follows:

Participation: Analyzing traditional notions of participation or social presence can show us who is talking, but
not who is lurking. We see that there are five participants posting. Each person posted once. We can see a
friendly tone, informal language, and use of the first person, all of which signal information about the social
nature of the environment. We do not know who may be lurking or what the role of the instructor is, the task, or
the context of the course.

Content: This appears to be an off-topic thread, unless the purpose of the discussion has to do with lack of
modern conveniences. It would likely be coded as "surface" learning (as opposed to deep), off-task, or purely
socializing because there are no explicit references to the course text or concepts. There does not seem to be a
lot of content here related to the formal learning of the course material. We do not know what the purpose of the
discussion is or what information came before and after this thread.

Structure: The thread would need to be compared to others to have any analytic utility using this method. Of
particular note is that the lone male participant is the thread starter (implying gender-interaction patterns), one
respondent asks a question that receives its own response, and that all participants are in agreement. Also each
new post explicitly connects to a previous post, revealing that there is intention to build on previous posts.

Thus we see that measures of participation, content and structure can all provide useful information about online
discussions. However, they don't explicitly address indicators of learning, instead they focus on individual
descriptive elements of the messages that were posted and, in the case of structural analysis, how the message
interrelate based on some variable (gender, timing, etc.) These methods do not yet take full advantage of the
context to shed light on how groups create new knowledge together. In the next section we show how Stahl's
social theory of CSCL may help in this area.

                                                         
NEW PARADIGMS
Stahl's (2002, 2003a, 2003b) social theory of computer-supported collaborative learning focuses on the group as
the unit of analysis. Moving      from  viewing learning   as  a knowledge-transmission      process    to a knowledge-
creation process which occurs in conversation with others, Stahl outlines how all individual knowing is in
essence an interpretation of a meaning that was first made in conversation with others. But it is only through
capturing all verbal and nonverbal communication that we can fully understand the context in which individual
utterances function in the context of a group discussion. It is through analyzing the dialogue in context that we
can understand how knowledge is created collaboratively:

                  The    fact that collaborative    learning  necessarily  makes    learning  visible
                  provides    the methodological    basis for  empirical  analysis  by   researchers.
                  Researchers of collaborative learning are not restricted to indirect evidence
                  of  learning  (such   as pre-test and   post-test differences)   because   they can
                  analyze and interpret the making of meaning as it unfolds in the data at the
                  group level and in individual trajectories of utterances . . . Of course, the
                  analysis must also take into account the activity structure and other socio-
                  historical content in which learning takes place (Stahl, 2003b, p. 35).
Thus, it is by looking at the discussion in its broader context, through microethnographies, conversation and
discourse analysis methods, that we can begin to understand how a group of discussants creates new knowledge
while in conversation. Examining the full context and the dialogic artifacts for moments of new knowledge
creation can in itself be the evidence of an effective group learning environment. To do this we need: 1) closer
attention to the context of the environments; and 2) conversation and discourse analysis of knowledge building
within the context.

Micro-ethnographies
Providing a more comprehensive picture of the context of the discourse is also key to generating lines of
research that will result in useful prescriptive knowledge, such as instructional design theory. With educational
experiences increasingly being offered via interacting online forums by novice online instructors and students,
the more detail that can be provided about the context in which a particular strategy worked or interaction took
place the better. Using additional data collection methods beyond just collecting archives of a class discussion
can help  provide   this contextual  information.   In particular,  surveys, interviews,   and    field notes should   be
considered  as possible    data   collection methods   to  generate     contextual information    and   help  triangulate
discussion-based  findings.   Student  surveys  can  be used   to   see how  attitudes affect  one's    participation and
perception of whether or not learning resulted from a particular activity. For example, Dennen (2004) found that
student perceptions of how a discussion contributes to learning often differ from what the researcher sees in the
data, with  students in  a less   successful treatment  feeling   more   confident  that they   had   learned than    their
classmates in a more successful group. Collecting data directly from students also can help shed light on other
factors, such as unclear directions, technology problems, or competing assignments, that might have affected
participation. Interviews with instructors might yield information about behind the scenes instructions students
were given or volume of off-board communications, such as private email, that surrounded the activity.
    Field notes may seem like an unusual choice of data collection method for studying asynchronous discussion
since discussion  boards   are  self-archiving  and  do   not involve   real-time  activity. Whereas     in  face to  face
environments it is possible to videotape the collaborative interactions to capture not only the dialogue, but
gestures and other nonverbal communication important to meaning-making, this isn't possible online. However,
there is a `feel' to the online experience that cannot be captured only by reading transcripts after the fact. Thus,
tracking a discussion in progress and keeping notes about it can be particularly useful. An observant researcher
may choose to take notes from the perspective of a student, instructor, or outside observer. The prolonged and
continuous  engagement     that results from   the  researcher   watching  the discussion    as   it occurs  permits  the
researcher to comment on what the actual participants might have experienced at different times during the
course. Our research presently tends to document completed discussions, looking at the act of message posting.
However, students engaged in an online course are likely to be affected by what it means to be a reader of
messages, looking for places to post a response or waiting to see if a particular message received a reply.
Analysis of archived discourse fails to adequately capture times when the discussion board feels "slow" or
inactive; has such rapid participation that it almost seems synchronous; is rich or lacking in openings for true
dialogue; or is tense based on a message that may be interpreted in multiple ways. For example, a particular
message may seem surprising or radical when initially posted, but become less so as classmates enter and adopt
that point of view. Capturing these moments during the actual creation of the dialogue may provide insight into

                                                          
the participants' experience, whereas archives of completed discussions may smooth over or obscure any rough
spots that happened during the discussion period.

Conversation and discourse analysis
Rourke    and Anderson   (2004)  and  Rourke,  Anderson,  Garrison    and Archer  (2001b)   describe     the enormous
difficulty of inferring the presence of an underlying construct, such as knowledge construction, from what is
observable in computer conferencing transcripts. "Drawing conclusions about underlying constructs based on
frequency counts of the surface content of communication is a complicated analytical process, though it is rarely
recognized    as such" (Rourke  &    Anderson, 2004,  p. 15).  They   point out that an   iterative process   between
grounded theory and literature review is often used to come up with behaviors that represent the construct of
interest, such   as cognition. Campos    (2004)   adds:  "Curiously   enough,   most of    those studies     considered
qualitative rely on quantitative measurement of qualitative categories. [This can] indeed suggest certain trends.
However, such studies are very limited because summing up categories says nothing about the knowledge
building process. It is only through attention to the process that collaborative conceptual change and learning
can be assessed" (p. 4).
    When analyzing discussion transcripts, Dillenbourg, Baker, Blaye and O'Malley (1996) admit that "deciding
on the meaning of . . . expressions in a given dialogue context is thus quite complex, but necessary if we are to
understand when students are really collaborating and co-constructing problem solutions" (p. 18). They point
out that   a  promising  possibility is to "exploit  selective branches     of linguistics research   on     models of
conversation, discourse or dialogue to provide a more principled theoretical framework for analysis" (p. 19).
Mazur (2004) and Herring (2004) have begun to explore how linguistic methods of conversation and discourse
analysis can be applied to online discussions. Herring's (2004) computer-mediated discourse analysis (CMDA)
is "any analysis of online behavior that is grounded in empirical, textual observations . . . [I]t views online
behavior through the lens of language, and its interpretations are grounded in observations about language and
language   use"  (Herring, 2004,  p. 339). CMDA     draws  upon  theoretical   assumptions  of   linguistic  discourse
analysis, including the notion that recurring patterns are present in discourse which may be identified by the
analyst, even though speakers themselves may not be aware of these patterns. The notion that we do things with
words can be traced back to Austin (1962) and Searle (1969) and speech act theory. This view of language is
particularly useful when seeking to examine how groups complete a process. Traditionally, content analysis has
revealed what participants say online; however, what participants are trying to do with what they say online is
of particular interest when describing a process such as knowledge construction.

APPLYING THE NEW METHODS AND PARADIGMS
We return now to the sample data thread analyzed above, showing how a deeper understanding of the context
and a conversation/discourse perspective is valuable.

Microethnography. Additional information such as the timing of the posts, how this thread fits into the larger
discussion forum, the larger context of this particular thread, how many people were lurking, that the students
may be communicating face to face or through email in addition to the discussion forum, what was the role of
the instructor, what task / prompt were they responding too, and how all of this fits together to capture the
knowledge     they  were creating together.  There  are  often "a-ha!"    moments    that  change   the  flow   of  the
conversation.    These are  often not   captured, or  when    used   in other   methods    tend  to  not  be   part of
contextualization (e.g., looking at timing as a structural element).

In the actual class from which the data sample was taken, these messages were posted in a thread over the
course of    two weeks.  The   students were  reading about   related topics,  such  as the role    that sewages    and
plumbing technologies have played in developing society. The relaying of personal experiences is actually
following a model that the instructor set, encouraging all students to find examples of the concepts being
learned in their own lives and to examine what happened when their experiences and knowledge was pooled.
Within this course, almost every student was an active participant at some point in time, but they tended to wait
and post when they felt they had something to say rather then posting for participation points or to demonstrate
that they had done the reading. This particular thread was briefer than many of the others

Conversation analysis: There are four female participants and one male participant. Eddie started the thread and
everyone else responded. Each person took one turn. The length of the messages gets shorter as the thread
progresses. Everyone generally agrees with each other. Most of the messages consist of statements. There are
several questions and some direct and indirect responses.

                                                         
Discourse analysis:

         Toilets, Eddie
         We, as a society, definitely take running water for granted. A few days ago I had no water because of
         work on a line. My water was off for about six hours. Horrible. I couldn't make ice tea, take a shower
         or anything. Eventually the water did return and all is well. Just think of the poor countries were the
         running water never even appears!

Eddie begins by making a claim that "modern conveniences are taken for granted". He supports this with a
personal story and example from his own life, followed by an appeal to bring others into the conversation
through his exclamation, "just think....!"

         Re: Toilets Tanya H.

         When I was younger, we visited the South and in order to take baths we would go outside and get
         buckets of water from the well heat the water up and poured it into a white wash tub. Talk about
         inconvenient. But since I was a little girl I thought that it was fun. As an adult, if I had to do that, I
         would be annoyed.

Tanya connects by relating her own personal experience/example along the same lines. This is done indirectly
and without direct reference to Eddie's post. She however explicitly connects the idea of "as an adult . . . I
would be annoyed" to Eddie's initial claim that we as a society take conveniences for granted.

         Re: Toilets, Laney

         Boy I can relate, we had a back up in our basement due to roots from a tree, and my husband and I
         wanted to stay at my moms because of no water, how spoiled are we? Does anybody remember when
         Brownsville had to boil their water due to a bacteria? We all were out purchasing water.

Here Laney makes a direct connection (reference to the previous post) "boy I can relate" and also brings in her
own examples. She also restates the idea of taking things for granted by saying "how spoiled are we?" She
searches for an experience that all participants have in common by referring to a local incident in Brownsville.
She asks it as a question to draw others in.

         Re: Toilets, Clarissa

         yeah...i do remember when the people of Brownsville had to boil their water. my grandmother lives
         there, and we had to bring her jugs of water to keep her in comfort.

Clarissa directly responds  to Laney's  question,  followed by a personal     experience. At this point  the posts
become shorter, and there is less exploration and more direct respond.

         Re: Toilets, Donna

         I have to agree with you all. I know that I do take the modern conveniences for granted.

Finally, Donna weighs in with a general agreement, plus an interesting connection back to the broader concept
of "modern convenience" that Eddie had initiated in the first post. The message serves to provide resolution to
the thread, somewhat unusual in asynchronous discussion forums (Hewitt, 2003).

Through  this discussion the   group is exploring  the meaning of modern      conveniences   and  taking them    for
granted and sharing personal stories to illustrate what they each mean and bring it back together again.

Traditional methods of participation, content analysis and structural analysis reveal important insights about
what happens in online discussions. However, adding microethnographies, conversation and discourse analysis
techniques to our repertoire provides a more robust look at how the participants in the overall conversation are
participating in a process of knowledge creation. By looking at the function that their posts serve in a larger
context, meaning making is revealed.

                                                        
CONCLUSION
In closing, research into online conversations in educational settings should be looking more thoroughly at how
groups of learners are engaged in contextualized discourse. As a discipline, distance learning is in need of a
rigorous research framework with solid epistemological grounding that will encourage comprehensive study of
how learning takes place through group interactions on a discussion board. Such a framework will need to
account for all types of participation or learning processes (internal and external, individual and group). It will
need to promote data collection beyond just downloading post-course archives of discussion threads in order to
capture contextual    factors  that impact    ecological   validity. Stahl's  social  theory   of   computer-supported
collaborative learning may provide the necessary theoretical underpinnings to support the development of a new
paradigm of online discourse research, one that looks to methods such as microethnography and conversation
and discourse analysis in addition to more traditional participation, content and structure oriented methods.

REFERENCES
Austin, J.L. (1962.) How to do things with words. Oxford: Clarendon Press.
Aviv, R., Erlich, Z., Ravid, G., & Geva, A. (2003) Network analysis of knowledge construction in asynchronous
       learning networks. Journal of Asynchronous Learning Networks 7(3), 1-23.
Bakeman, R., & Gottman, J. (1997). Observing interaction: An introduction to sequential analysis. Cambridge:
       Cambridge University Press.
Bauer, M. (2000). Classical content analysis: a review. In M. Bauer & G. Gaskell (Eds.), Qualitative researching
       with text, image and sound (pp. 131-151). London: Sage.
Beaudoin, M.F. (2002). Learning or lurking? Tracking the "invisible" online student. The Internet and Higher
       Education 5, 147-155.
Bloom   B.S.  and  Krathwohl,      D.R., (1956).  Taxonomy      of   Educational   Objectives: The    Classification  of
       Educational Goals. New York: Longman, Green.
Campos, M. (2004). A constructivist method for the analysis of networked cognitive communication and the
       assessment   of  collaborative    learning and   knowledge    building. Journal    of Asynchronous     Learning
       Networks 8(2), 1-29.
Dennen, V.P. (2004). Facilitation and student learning: The effects of different instructor styles. Paper presented
       at the annual meeting of the Association for Educational Communications and Technology, Chicago, IL.
Dillenbourg,  P., Baker,   M., Blaye,    A., & O'Malley,   C.  (1996).   The  evolution of   research  on collaborative
       learning.  In   E.  Spada    &  P.    Reiman   (Eds).  Learning    in  Humans    and    Machine:   Towards     an
       interdisciplinary learning science, pp. 189-211. Oxford: Elsevier.
Garrison,  D.R.,  Anderson,    T.,  & Archer,   W.    (2001). Critical   thinking, cognitive presence    and  computer
       conferencing in distance education. American Journal of Distance Education, 15(1), 7-23.
Gunawardena,     C.N., Lowe,   C.A.,   &     Anderson,  T. (1997).   Analysis  of   a global   online  debate    and the
       development     of an  interaction    analysis model   for examining    social construction    of knowledge    in
       computer conferencing. Journal of Educational Computing Research, 17(4), 397-431.
Hara, N., Bonk, C. J., & Angeli, C. (2000). Content analysis of online discussion in an applied educational
       psychology course. Instructional Science, 23(2), 115-152.
Hathorn, L.G., & Ingram, A.L. (2002). Online collaboration: Making it work. Educational Technology, 42(1),
       33-40.
Henri,  F. (1992).   Computer   conferencing      and  content  analysis.  In A.R.    Kaye   (Ed.), Online   education:
       Perspectives on a new environment (pp. 115-136). New York: Praeger.
Henri, F., & Rigault, C. (1996). Collaborative distance education and computer conferencing. In T. T. Liao
       (Ed.), Advanced     educational   technology:   Research    issues and  future  potential  (pp.   45-76). Berlin:
       Springer-Verlag.
Herring, S.C. (2004). Computer-mediated discourse analysis: An approach to researching online behavior. In
       Barab, S.A. & R. Kling & J.H. Gray (Eds.), Designing for virtual communities in the service of learning.
       New York: Cambridge University Press.
Hewitt, J.  (2003,   April).  Toward     an  Understanding    of  How     Threads  Die  in   Asynchronous    Computer
       Conferences. Paper presented at the annual meeting of the American Educational Research Association,
       Chicago, IL.
Hill, J.R., Wiley, D., Nelson, L.M. & Han, S. (2004). Exploring research on internet-based learning: From
       infrastructure  to  interactions.  In   D.H.    Jonassen   (Ed.),  Handbook    for  Research    in  Educational
       Communications      and  Technology,     2nd   Edition  (pp.  433-460).     Mahwah,   NJ:    Lawrence  Erlbaum
       Associates.
Howell-Richardson, C., & Mellar, H. (1996). A methodology for the analysis of patterns of participation within
       computer mediated communication courses. Instructional Science 24, 47-69.

                                                           
Jeong, A.C.   (2003).   The sequential  analysis of group   interaction and  critical thinking in online threaded
       discussions. The American Journal of Distance Education 17(1), 25-43.
Kanuka, H., & Anderson, T. (1998). Online social interchange, discord and knowledge construction. Journal of
       Distance Education 13(1), 57-74.
King, P.M. & Kitchener, K.S. (1994). Developing reflective judgment. San Francisco: Jossey-Bass.
Marra,  R.M.,  Moore,    J.L. &  Klimczak,   A.K.   (2004). Content  analysis  of  online  discussion forums:  A
       comparative analysis of protocols. Educational Technology Research & Development 52 (2), 23-40.
Mazur, J.M. (2004). Conversation analysis for educational technologists: Theoretical and methodological issues
       for researching the structures, processes and meaning of on-line talk. In D.H. Jonassen (Ed.), Handbook
       for Research in Educational Communications and Technology, 2nd Edition. (pp. 1073-1098). Mahwah,
       NJ: Lawrence Erlbaum Associates.
Meyer, K.A. (2004). Evaluating online discussions: Four different frames of analysis. Journal of Asynchronous
       Learning Networks 8(2),101-114.
Newman, D.R., Johnson, C., Webb, B., & Cochrane, C. (1997). Evaluating the quality of learning in computer
       supported co-operative learning. Journal of the American Society for Information Science 48(6), 484-
       495.
Paavola, S., Lipponen, L. & Hakkrarainen, K. (2002). Epistemological foundations for CSCL: A comparison of
       three models of innovative knowledge communities. Computer-supported Collaborative Learning (CSCL
       '02),   Jan.   7-11,   2002,     University  of   Colorado,      Boulder,  CO,     USA.     Available  at:
       http://newmedia.colorado.edu/cscl/22.html
Pear, J.J., &  Crone-Todd,    D.E. (2002). A social  constructivist approach  to  computer-mediated   instruction.
       Computers & Education 38, 221-231.
Perry, Jr.,  W.G. (1999). Forms of ethical and intellectual development in the college years: A scheme. San
       Francisco: Jossey-Bass.
Rourke, L. & Anderson, T. (2004). Validity in quantitative content analysis. Educational Technology Research
       & Development 52(1), 5-18.
Rourke, L., Anderson, T., Garrison, R. & Archer, W. (2001a). Assessing social presence in asynchronous text-
       based computer conferencing. Journal of Distance Education 14(3), 51-70.
Rourke, L., Anderson, T., Garrison, D.R. & Archer, W. (2001b). Methodological issues in the content analysis
       of computer conference transcripts. International Journal of Artificial Intelligence in Education 12, 8-22.
       Scott, J. (2000). Social network analysis. London: Sage Publications.
Searle, J.R. (1969). Speech acts. Cambridge: Cambridge University Press.
Stahl, G. (2003a) Meaning and interpretation in collaboration. In B. Wasson, S. Ludvigsen, & U. Hoppe (Eds.),
       Designing    for Change     in Networked    Learning  Environments:    Proceedings   of  the  International
       Conference on Computer Support for Collaborative Learning (CSCL '03), Kluwer Publishers, Bergen,
       Norway, pp. 523-532. Available at: http://www.cis.drexel.edu/faculty/gerry/cscl/papers/ch20.pdf.
Stahl, G. (2003b) Building collaborative knowing: Elements of a social theory of learning. In J.-W. Strijbos, P.
       Kirschner, & R. Martens (Eds.), What We Know about CSCL in Higher Education, Kluwer, Amsterdam,
       NL. Available at: http://orgwis.gmd.de/~gerry/publications/journals/oun/oun_outline.pdf.
Stahl, G. (2002) Contributions to a theoretical framework for CSCL, In: Proceedings of Computer Supported
       Collaborative     Learning      (CSCL     2002),     Boulder,     CO,     pp.    62-71.    Available   at:
       http://orgwis.gmd.de/~gerry/publications/conferences/2002/cscl2002/cscl2002.pdf.
Stake, R. E. (1995). The art of case study research. Thousand Oaks, CA: Sage Publications.
Turoff, M., Hiltz, S.R., Bieber, M., Fjermestad, J. & Rana, A. (1999) Collaborative discourse structures in
       computer  mediated     group   communications.   Journal of   Computer    Mediated  Communication     4(4).
       Available at http://www.ascusc.org/jcmc/vol4/issue4/turoff.html
Yin, R. K. (1984). Case study research: Design and methods (Vol. 5). Beverly Hills: Sage.

                                                         
