Effects of Technology-based Support for Explanation Construction on
      Learners' Discourse during Design-based Learning in Science

                  Swaroop S. Vattam, Christopher W. Kramer, Hyungsin Kim, Janet L. Kolodner
                    Georgia Institute of Technology, 801 Atlantic Drive, Atlanta, GA 30332-0280
                             Email: {svattam, cwkramer, hyungsin, jlk}@cc.gatech.edu

        Abstract:   We     examine  the effects   of   a   software-based   approach   to scaffolding   explanation
        construction   on  learners'  discussion    in   a design-based    learning   environment.   The   approach
        consists of having learners collaboratively work around a software-based explanation-construction
        tool in the context of addressing their design needs during design investigations. We conducted a
        study where three sets of participants completed a one-week hovercraft unit with the same teacher.
        We have analyzed the data collected from two sets of participants where one set was facilitated by
        only the teacher in their explanation efforts and the other set was facilitated by both the teacher
        and our software called SHADE. Results indicate that participants who used the software engaged
        in  higher  quality  explanatory    discourse    by the   end  of  the  unit. This  research  supports   the
        usefulness of a contextualized explanation-construction tool in promoting explanatory discourse.

Introduction
        Approaches to enculturating learners into the epistemic practice of explanation construction continue to
receive significant attention   in educational   research   because   of   the recognition  that  articulating and   applying
explanations is closely tied to advancing one's conceptual understanding (Coleman, 1998; Sandoval & Reiser, 2004;
Vattam & Kolodner, 2006). Our research has investigated the promotion of explanation construction among middle
school students learning science in a design-based inquiry environment. We have found that learners' development
and their ability to participate effectively in such a practice are heavily dependent on teacher expertise in scaffolding
that practice and modeling the discourse of explaining (Ryan & Kolodner 2004). Having noticed that not all teachers
have this kind of expertise, we have been seeking technology-based approaches to complement teacher facilitation
to help middle-school learners become better scientific explainers (Vattam & Kolodner, 2006).           In this paper we will
present one example of this approach, a software tool called SHADE (Science of Hovercraft Aided by Designing
and Explanation).   SHADE's      explanation-construction     component     is  connected   to   a design   exploration    and
investigation  component.    The    explanation-construction      tool   illustrates  and   frames,  through     an  external
representation, the essential elements of a causal explanation of an observed physical phenomenon.

        Our    purpose  in this study   is  to investigate   the  potential of  such  an  explanation-construction     tool to
overcome some teachers' lack of strong content knowledge and explanation-construction capabilities.              Can such a
tool, integrated into a design-based learning environment in ways that allow learners to recognize its usefulness,
help enculturate learners into becoming better scientific explainers?       More specifically, we examine the affordances
of SHADE as a collaborative explanation-construction tool for enhancing explanatory discourse and explanation
construction in the classroom. Our overall research hypothesis is that (a) by contextualizing explanation in design
needs of  learners, we   can  encourage     them to  want    to   explain, (b)  by   contextualizing explanation    in design
exploration and  investigation,    learners will get   direct experience    at explaining   their  observations, and   (c)  by
employing   a  representational  framework     that models    explanatory    discourse,   learners will  be  scaffolded  into
generating  more    conceptually   and  structurally     elaborate  explanations     during whole-class     discussions    and
presentations.

Background
        Learning    to formulate   explanations  is an   important  aspect  of  the  scientific enterprise (Coleman,   1998).
Recent theoretical work supports the view that it is essential to participate in the discourse practices of disciplinary
communities to gain a deeper understanding of discipline-specific concepts (Lave & Wenger, 1991; Roth, 2001).
Therefore, many inquiry-based learning methods, which seek to place the learners in the role of scientists, face the
prospect of dealing with enculturation of their learners into the epistemic practice of scientific explanation.

                                                              740                                                   CSCL 2007
          Although explanation-based interactions affect individual achievement in the context of group learning,
research shows that learners will not naturally generate efficient explanations on their own and need support to do so
(see studies cited by Coleman, 1998). In our design-based approach to science learning, called Learning by Design
(LBD)    (Kolodner   et   al.,  2003), teachers    enculturate  learners into   scientific  explanation    through     exposure,
experience, and discourse modeling. As learners progress through the LBD unit, learners not only engage in design
engineering, but also conduct experiments and collect data from these experiments to inform their future design
choices. In the context of presenting their experimental procedure, data and conclusions, the teacher attempts to
facilitate explanatory discussion by helping learners focus their comments on explaining their findings in terms of
causal mechanisms. In this way, learners are helped with socially constructing scientific arguments.

          But  our research     has also shown     that some   teachers  are   not as   successful   in   facilitating scientific
explanation as others, especially those who are not as fluent with the science content, as skilled at modeling the
discourse of scientific argumentation, or as able at focusing learner discourse on the underlying science concepts
(Ryan & Kolodner, 2004).

          We have adopted a technology-based approach to complement teacher facilitation in helping middle-school
students  become   better scientific   explainers. Our  first attempt involved    the integration  of   a software   tool called
SIMCARS into an LBD unit, Vehicles in Motion. SIMCARS included an explanation-construction tool that was
designed to be used by learners working in pairs or small groups around a computer in the context of conducting
experiments and collecting data. The explanation-construction tool consisted of an explanation template that served
as an external discursive representation. A discursive representation (Sandoval et al., 2003) is one that represents
elements of a scientific explanation as opposed to, say, simulations which represent a physical phenomenon on a
computer.  Integration    of   SIMCARS    influenced    the Vehicles  unit in  a   two  ways.  First,   by situating   learners'
explanation   construction   in the activity of experimentation     and  data  collection, it situated  their explanation   and
scientific argumentation in their design needs and in the design space. Second, it distributed the responsibility of
scaffolding learners' explanation construction across the teacher and the tool. Learners' inclination to scientifically
explain  their design investigation    findings without   expert  facilitation suggests   that a tool   like SIMCARS      holds
potential to  bridge  the design-science   gap  among    learners   and  help  at least some   individuals    develop   a better
understanding of the content in a less teacher-dependent fashion (Vattam & Kolodner, 2006).

          Discursive  representations   have  been   a  subject of   much  study   in the  context   of   scientific knowledge
construction (Bell & Linn, 2000; Sandoval et al., 2003; Scardamalia & Bereiter, 1994; Toth et al., 2002; Vattam &
Kolodner, 2006). A majority of those studies, including our earlier SIMCARS research, have focused on individual
achievement in the context of group learning. Only some of them have examined the role of such representations as
mediational resources (Roschelle & Teaslay, 1995) facilitating collaborative interactions. Suthers & Hundhausen
(2002) reported the effect of such representations on learner discourse in the context of within-group collaboration.
In this paper, we present a new analysis that explores the influence of discursive representation on learner discourse
in the context of inter-group collaboration.

Study

                                    Figure 1: Model hovercrafts from design challenges

Shade: Software design
          SHADE    software     was  designed   in the  context   of an  LBD-style     unit   called Hovering   around     Tech
(henceforth  referred to  as   "the Hovercraft  unit").  This  unit  was developed    to teach   physics   concepts    related to
working hovercrafts and practices of designers and scientists, all in the context of learners designing and building

                                                              741                                                      CSCL 2007
model hovercrafts and carrying out investigations needed for successful design. The unit was designed such that
over the  course of a week-long    science  summer    camp   (approximately   26 hours),  the learners, working   in small
groups, addressed four successive design challenges that increased in complexity with respect to both functionality
and  science concepts  involved:   a balloon   hovercraft, a flying saucer   hovercraft, a 2-fan hovercraft, and  a  1-fan
hovercraft (see figure 1 for typical models of each kind).

         SHADE     was  developed    to promote   specific   "explanation-construction"    interactions in  the classroom
culture. Our previous research with SIMCARS suggests that such interactions need to be situated within the context
of learners' design needs and design investigations to bridge the design-science gap (Vattam & Kolodner, 2006).
Furthermore, the more designs learners explore, the more opportunities there are for such interactions to take place.
However, opportunities for exploration in the real world are limited due to time and material constraints. Therefore,
there is a need to augment the real-world design environment with a virtual design environment that imitates the real
world but in a way that both expands the design space for the learners and also allows for more efficient exploration
of the space. Therefore, SHADE incorporates a simulation-based virtual design environment in which learners can
explore variations of the four hovercraft designs mentioned above.

                                         Figure 2: (a) Design area. (b) Test area

         To  maximize     design   exploration  and  to maximize    the potential such   exploration  has   for promoting
explanation construction, each of the four design challenges was structured in such a way that half the time our
learners would be designing and testing real hovercraft models and half the time they would be experimenting with
simulated models in the virtual design environment. For instance, in the initial phases of each challenge, they would
"mess   about" (Kolodner   et al., 2003)   with real parts and  build real hovercrafts.  Later, during  the design-driven
investigation phase, when they are investigating issues important to designing a better-working hovercraft, work
would shift to the virtual design environment where they could quickly design new craft and collect consistent data
across  designs. Finally, when   the time  came   for designing  their  best hovercraft, they would    use what  they had
learned through the software to design and build a functioning hovercraft that they could race with other groups'
crafts.

         To facilitate this back and forth movement across real and virtual models, and to help learners transfer
knowledge gathered in one medium to the other, we recognized that there had to be correspondence between the real
and virtual design environments in terms of how the devices look and behave. The virtual design environment of
SHADE has a design area and a test area. Figure 2 (a) shows the design area in SHADE where one can see the
correspondence between virtual crafts and the real models depicted in Figure 1. In the design area, users can quickly
configure  a hovercraft   to  match  their conceptual   design  by  clicking  on  the various   parts and  adjusting  their
parametric values. Figure 2 (b) shows the test area. Learners can test their design in the test area, which animates the
behavior of the design along with a graph that plots the hover height versus the hover time. They can also pause and
step through the simulation.

                                                            742                                                  CSCL 2007
        An    important  aspect of  design-based   investigation   is the  comparison   of  many   design  variations  to
determine the factors that account for the differences in their behavior. To facilitate this process, SHADE includes a
design comparison feature that allows learners to compare multiple designs side-by-side as shown in Figure 3 (a).
After choosing the designs for comparison, they have the option of predicting the outcome of running those designs
side-by-side, generalizing  the prediction as   a rule  of thumb,  and   explaining   the science behind   the predicted
outcome. For instance, let us assume that learners were comparing 3 designs (D1, D2 and D3) similar in every
respect except that the weight of D3 was greater than the weight of D2, which in turn was greater than the weight of
D1. Based on discussions already had in class, learners might predict that "Design 3 will have the lowest hover
height." After running the investigation to see if indeed that was true, they could extract a general rule of thumb, "to
maximize the hover height, keep the hovercraft weight as low as possible." But the prediction and the rule of thumb
alone will not account  for the underlying  science   that would   explain them.   At this stage, there is an  option  for
learners to launch the explanation-construction tool to back up their prediction or justify their rule of thumb. Figure
3(b) shows the prediction and the rule of thumb that a learner entered and the corresponding explanation entered by
the same learner in the explanation-construction tool.

                         Figure 3: (a) Design comparison. (b) Explanation-construction tool

The Hovercraft unit design and integration of the software
        The   hovercraft unit   was developed   to teach   physics concepts  related  to   working  hovercrafts  and  the
practices of designers and scientists, all in the context of designing and building small hovercraft. The one-week unit
was broken down into four design challenges followed by a final presentation to an external audience at the end. For
each design challenge, the following sequence of activities takes place:
·   Messing about: A playful exploratory activity where learners construct a modestly-working device of the kind
    they will be redesigning later and tinker with it to discover its capabilities and ways of making it better.
·   Whiteboarding: As a whole class, groups share their experiences and ideas for achieving the challenge and
    articulate what they need to learn more about.     They also discuss what they think they know, and the teacher
    might present some science content related to what they experienced while messing about.
·   Design-based experiments and poster presentations: Groups systematically explore design variations to learn
    more   about factors (variables)  affecting   the working   of their  designs. When    software is  integrated, it is
    integrated into this step in the sequence of activities. Whether or not learners use the software to investigate or
    run their investigations in the real world, they are encouraged in this step not only to identify trends in their data
    but also to ask questions about and use science content already discussed to explain those trends.     Investigation
    is followed by a "poster session" (Kolodner et al., 2003) where learners present their findings, the trends (rules
    of thumb) they can extract from their data, and their best explanations of those trends.      This, in turn, may be
    followed by another presentation of science content by the teacher and then attempts by the whole group to
    collaboratively construct explanations for each of the trends based on that content.

                                                           743                                                 CSCL 2007
·   Design   best  hovercraft:  Based  on  what they  learn  from    their investigations and from  the investigations of
    others, groups design and build their best hovercraft. Groups also test and compare the performance of their best
    hovercraft with other groups' hovercrafts. Water races are also conducted sometimes.
·   Gallery walk: Learners present their design experiences to each other in a gallery walk (Kolodner et al., 2003),
    asking their peers to help them explain why their designs did or didn't work and suggest ways of fixing the
    problems. Here again, learners engage collaboratively in explanation construction.
·   Scaling new levels: Once groups have their best hovercrafts, they are introduced to harder challenges that test
    the limits of their designs. For example, in the case of the flying saucer, which performs well on smooth floors
    and carpets, we asked the learners to see if their crafts could hover over grass. In most cases their designs fail,
    which motivates a new challenge and sets the context for moving on to address that challenge through the next,
    more sophisticated, type of hovercraft.

Setup
        This    study was  conducted   as part of a  science  summer    camp   organized   by  the Center for Education
Integrating Science, Mathematics, and Computing (CEISMC) at Georgia Tech and attracted a socio-economically
diverse set  of rising 7th and  8th  graders   (ages 13 and   14)    from  the Atlanta  metropolitan  area. One  teacher
collaborated  with the  researchers to implement   the  Hovercraft    unit  three times in three successive  weeks. The
teacher was neither an expert in the science content nor an expert at design-based learning. However, she was an
excellent and energetic teacher in many ways and enthusiastic about learning to use design as a context for science
learning. In each week, we had a different set of learners. There were 16, 13 and 18 participants in Weeks 1, 2 and 3
respectively. Participants in Weeks 1 and 3 seemed similar in terms of their background knowledge and overall
developmental capabilities, as evidenced in discussions during Day 1 of each week. Participants in Week 2 seemed
less motivated and showed less development in terms of their background knowledge and their ability to learn.

Procedure
        Based   on the  natural differences  between   participants  in the  three weeks,  we  have  chosen  to compare
results in Weeks 1 and 3 to learn about effects of integrating the SHADE software into the learning environment.
While we had planned a design study where each week we would have participants use an enhanced version of the
software, the software was not working well enough in Week 1 to use it. Comparing the results of Weeks 1 and 3
allows  us to compare   development    of explanation  capability    among   participants with similar  backgrounds and
developmental capabilities, with and without the scaffolding provided by the explanation tool. Participants in Week
1 received support from the teacher to articulate their explanations, and they ran their experiments in the real world
and used paper-and-pencil based tools to capture their explanations. Participants in Week 3 followed the same unit
with the same teacher but used the software to run experiments and to articulate their explanations. All the sessions
were videotaped using two cameras. The two cameras were positioned such that we were able to capture the whole-
class interactions during discussions, presentations, lectures, etc.

Findings and Analysis
        To   understand  SHADE's    impact   on explanatory   discourse,    we analyzed   discourse during  whole-group
discussions in Weeks 1 and 3 at the beginning of the week, several times during the week, and at the end of the week
(see Figure 4).

                         Figure 4: Stages in the unit when discourse analysis was carried out

Discourse analysis at the beginning of the week
        Day 1 in both conditions started in a similar fashion with an informal class-wide discussion about what
participants already knew about science, engineering, and hovercrafts. Discussions in both weeks were anchored in

                                                          744                                                   CSCL 2007
the question  "What   does hovering   mean?"  This   discussion  was   useful in assessing   the  initial knowledge    and
explanatory capabilities of participants across the weeks. We found that the discussions during the morning session
for both Weeks 1 and 3 were qualitatively similar, consisting of fragmented knowledge of Newton's Laws and ideas
about hovering, with minimal continuity of ideas from one participant to the next. This helped us confirm that the
baseline for comparison of the two groups was similar.

Written discourse analysis during the week
         The written discourse of the participants in Weeks 1 and 3 was analyzed once during the unit and once at
the end. The earlier written discourse was what small groups of learners had written on posters in preparation for
"poster sessions" where    they presented   results of balloon   hovercraft investigations.   We    analyzed the  written
discourse with respect to its form, content, and correctness.
Week 1                                                        Week 3
(1) "the larger the air, the longer the hovering time         (1) "If hovercraft has a smaller diameter, it will less
Why? Because the air is the power."                           surface area and a greater hover height.
(2) "The larger the balloon, the longer it hovers and thehigher it goes.Why - because there is more air that comes out of theballoon and it goes longer."IF: CD diameter decreasesTHEN: Balloon hovercraft [hover height] increasesBECAUSE:WHEN CD diameter decreases THEN lift force increaseWHEN lift increases THEN Balloon hove[r height
(3) "The smaller the nozzle, the higher the                   increases]"
H[over]T[ime]. The larger the nozzle, the higher theH[over]H[eight].Why: when the air passes through a smaller nozzle, theair is more concentrated & blows at a steadier weight,and air passes through a larger nozzle a bust of air liftsthe H[over]C[raft] height."(2) "IF: Nozzle Diameter decreases,THEN: Balloon Hovercraft Hovertime increasesBecause...WHEN: Nozzle diameter decreases THEN [Lift] ForcedecreasesWHEN: Lift force decreasesTHEN Balloon hovercraft
                                                              hover time increases"
         Looking at the representative explanations above, we see that Week 3 groups structured their explanations
as "if X then Y, because when X then A, when A then B ... when C then D, and when D then Y". The structure of
explanations of Week 1 groups, on the other hand, varied from "since X therefore Y" to "X because Y, and Z". We
think the structure  of Week    3 explanations was     better  because participants  modeled   it   on the  cause-linking
framework modeled for them in the software. When we look at the content of written discourse, the Week 3 groups
used more intermediate causal concepts such as net force and lift force in their explanations than did Week 1 groups.
We also see that in Week 1, participants typically provided only one-level explanations. As far as correctness is
concerned, groups in Week 3 show more correctness. But, that cannot be attributed to SHADE alone because the
teacher had improved her understanding of the concepts by Week 3. Therefore, she might have been less misleading
in Week 3 than in Week 1. Therefore, we do not take correctness into account in our analysis.

Verbal discourse analysis during the week
         We analyzed verbal discourse from 5 whole-class discussions during each week on days 1, 2, 4 and 5 (see
Figure 4). The following is an example of verbal discourse analysis of the data gathered from Weeks 1 and 3.

         The context for this verbal discourse was the balloon hovercraft challenge, the same one in which the above
written discourse analysis was carried out. Groups were asked to investigate ways of making a hovercraft using
balloons, bottle caps, and CDs. In both weeks, within thirty minutes, most groups had grasped the techniques needed
to assemble a device and had put together a basic working hovercraft. After demonstrating their crafts to each other,
the teacher reviewed   the scientific method and    presented  the nomenclature    of  a hovercraft,   including hull, air
cushion, cushion pressure, power system, and lift system. It was at this point that discussion during the two weeks
diverged. During  Week   1, participants  conducted    design  investigations in the   real world,  and   during Week   3
participants used SHADE to conduct design investigations and to (optionally) provide explanations during those
investigations. In the poster session that followed in both weeks, groups were encouraged to include results in their
posters along   with appropriate  written explanations.  Teacher   provided   help  as needed    in both   weeks  to  help
participants complete this task. We analyzed the verbal discourse of participants presenting their posters and verbal
discourse of any accompanying whole-class discussions.

                                                           745                                                   CSCL 2007
          Our  analysis  shows    that the   verbal discourse    of participants   in Week     1 contained  impoverished
explanations with respect to science content and focused primarily on the designed artifact. The verbal discourse of
participants in Week 3, on the other hand, were more sophisticated and mimicked the explanations that they had
articulated using SHADE. A snippet from typical verbal discourse from Week 1 and Week 3 are compared below:
Typical Week 1 explanation                                    Typical Week 3 explanation
Student: If I change the size of the balloon it will hover    Student: If the lift force is greater than the gravitational
longer.                                                       force then the net force will be directed upward, but if
Teacher: ... change the" if" statement to make it better      the gravitational force is greater than the lift force then
Student: If I increase the balloon...                         the net force will be directed downward and the
Teacher: Good, if I increase the balloon size then it will    hovercraft would not move.
hover longer.

          A full analysis of the same data shows that the best Week 1 discourse was equivalent to the typical Week 3
discourse, and that the best Week 3 discourse was significantly better than the best Week 1 discourse as depicted
below:
Best Week 1 explanations                                      Best Week 3 explanation
(1) Student1: Because adding weight to the hull is going      Student: If fan diameter increases then flying saucer
to push more gravity down and it is going to push the air     hovercraft hover height increases because, when fan
cushion down and have less air cushion"                       diameter increases then the cushion pressure increases.
(2) Student2: With every action there is an equal andopposite reaction...[so]air under the hull mustovercome gravityWhen cushion pressure increases then lift forceincreases. When lift force increases then net forceincreases. When net force increases then flying saucerhovercraft hover height increases.

Discourse analysis towards the end of the week
          On the last day of the week, small groups presented their experiences in the camp to an external audience
including their family members. The latter part of the morning session of the final day was dedicated to preparing
posters for their presentations. Student groups were given a list of topics to choose from for their posters. They were
also free to choose their own topics. The content of posters and verbal presentations of groups in Weeks 1 and 3
were compared to analyze the differences in learners' discourse towards the end of the unit. We classified these
posters into four categories based on their function, as depicted in Table 1.

Table 1: Classification of final posters and their sample contents

Poster Type                                      Sample contents
Recommendation posters: Their function           "the best flying saucer needs:
was to communicate to the audience how           maximum hover height, a light weight structure, ... , a sturdy body
to build  a good   hovercraft of a  particular   Results from tests:
type.  Typically,  they  contained   a list  of  We  have  concluded   that a flying  saucer   hovers best with  1 battery
recommendations       with     or      without   pack because with 2 ... we concluded that a hovercraft (flying saucer)
associated    explanations.  Sometimes,     the  hovers higher    when  it  has  a bumper    on   the bottom.... Our  last
recommendations were captured implicitly         conclusion is that 30 grams is a good weight for a flying saucer."
in the form of Rules of Thumb.
Investigation posters: Their function was        "ROT: if the surface area increases then the hovercraft hover height
to  communicate      the    results    of   the  decreases.
experiments   conducted   to  understand    the
effect of   a particular variable   (e.g., hull  Why? If the surface area increases, the cushion pressure beneath the
weight,   surface   area)   on   the   overall   hovercraft will  decrease  because    it will have   to support a  larger
performance     of  the   hovercraft.      They  area..."
captured the outcome of the experiments in
terms of rules of thumb.
Comparison     posters:  Their function    was   "Differences in the 1 fan hovercraft and the 2 fan hovercraft
to  communicate      the     comparison      of  GH1: ... one fan is used ... to give the craft lift and to push it forward
different designs.  They    usually contained    ... a ramp is us to direct the air flow under & behind the craft.

                                                             746                                                 CSCL 2007
the decisions behind compared designs and          GH2: ... one fan pushes air down... 2nd fan is placed at the back ...
any      trade-offs   with       or    without     pushes air backwards causing the craft to go forward.
explanations.                                      ...
                                                   * The one fan is lighter, allowing the hovercraft to go higher. This is
                                                   because ...
                                                   * The one fan isn't as forceful as a craft with two fans....
                                                   * The hovercraft with 2 fans is heavier than the hovercraft with 1 fan
                                                   but the extra power makes up for the extra weight..."
Description posters: Their function was to         "What in the world is a skirt?
communicate    description   of   an object of     * How does it contribute to a hovercraft?
interest   (example   ­   hovercraft,     skirt).  It increases the cushion pressure underneath the hovercraft ...
Typically,   they   contained description   of     * What makes a good skirt?
systems   or subsystems   in  terms    of their    Light-weight durable, ...
structural elements    and    also   how  they     * Difference types of skirts!
worked. In the context of describing how it        1. Self-inflatable: won't fold under the hovercraft...
works,   participants explained    the science     2. Bumper Reinforcement: bumper material ... is put inside...
behind   hovercraft   design  in  some    cases.   3. Tape reinforcement: ... also put in the skirt to make it sturdier."
Interestingly, description   posters can  only
be found in Week 3.

         To analyze the final posters and presentation, we first counted the total number of statements made that
warranted an explanation, including recommendations and rules of thumb. We rated these statements according to
simple   statements   (Type   1),   statement with     rudimentary explanations   (Type 2),  and statements      with good
explanations (Type 3). For example:
         Type 1: "...small [balloon] - has the least power, medium [balloon] - has medium power, large
         [balloon] - has the most power..."
         Type 2: "...if the surface area increases then the hovercraft hover height decreases... [because]...
         the cushion pressure beneath the hovercraft will decrease...."
         Type 3: "... [Skirt] contributes to the hovercraft ... increases the cushion pressure underneath the
         hovercraft causing the lift force, net force, and hover height to increase."
Good explanations (Type 3) contained coherent causal explanations. Rudimentary explanations (Type 2) contained
either mere reproduction of formulas without showing any understanding of the formulas or simple explanations
without intermediate causal concepts. Simple statements (Type 1) are statements without justification of any sort.
Type 3 statements are given the highest rating and Type 1 the lowest.

         In Week 1, posters and presentations mostly contained Type 1 and Type 2 statements. The following Table
2 captures the findings from Week 1. As one can see, most statements are Type 2 (8 out of 13, 61.53 %).

Table 2: Results of analysis of Week 1 posters and presentations

         Title                     Poster category             Type 1     Type 2              Type 3
A        Hull weight               comparison                  0          0                   1
B        Surface area              comparison                  1          0                   0
C        Motor power               comparison                  1          2                   0
D        1 fan vs. 2 fans          comparison                  0          2                   0
E        Best flying saucer        recommendation              2          2                   0
F        Balloon hovercraft        recommendation              0          2                   0
                                                   Total = 13  4          8                   1

         In Week 3, posters and presentations had significantly fewer Type 1 statements and contained an equal
number of Type 2 and Type 3 statements. Table 3 captures the findings from Week 3. Most statements are either
Type 3 (5 out of 11, 45.45 %) or Type 2 (5 out of 11, 45.45 %).

                                                              747                                                   CSCL 2007
Table 3: Results of analysis of Week 3 posters and presentations

     Title                            Poster category          Type 1       Type 2        Type 3
A    Difference in 1 & 2 fan          comparison               0            0             2
B    The effect of weight             comparison               0            1             0
C    Surface area                     comparison               0            0             1
D    Best flying saucer               recommendation           0            2             0
E    Best balloon                     recommendation           1            1             0
F    What's a skirt?                  description              0            1             1
G    Hovercraft 101                   description              0            0             1
                                                  Total = 11   1            5             5

       The consolidated results in Table 4 show the overall differences between Weeks 1 and 3 with respect to the
statement types. While 30% of the statements in Week 1 were of Type 1, only 9% were of Type 1 in Week 3. While
only 7% of explanations in Week 1 were of Type 3, almost half (45%) in Week 3 were of Type 3.

Table 4: Consolidated results comparing posters and presentation findings across Weeks 1 and 3

             Type 1                       Type 2                            Type 3
Week 1       4/13      30.76 %            8/13          61.53 %             1/13         7.69 %
Week 3       1/11      9.09 %             5/11          45.45 %             5/11         45.45 %

Discussion
       This study sought to explore the affordances of SHADE as a collaborative explanation-construction tool for
enhancing learners' explanatory discourse and explanation construction in the classroom. We hypothesized that the
learners who used the explanation-construction tool would engage in better explanatory discourse by the end of the
Hovercraft unit in comparison  to   learners who  did not use  the tool, even if all received similar teacher support
throughout the unit. Our results support this claim because both written and verbal discourse of participants who
used the explanation-construction tool in Week 3 was significantly different from that of participants who did not
use the tool in Week 1. Specifically, changes were noticed in three areas. First, participants in Week 3 felt the need
to explain more. More of their claims and findings were communicated with causal explanations when compared to
participants who did not use the tool. Second, participants from Week 3 maintained a more coherent structure in
their explanations consistently across groups throughout the unit. Third, the content of explanations from Week 3
was more elaborate and contained more intermediary causal concepts (e.g., lift and net force) compared to Week 1.

       How    did  SHADE   impact    the learners? The  participants in  Weeks   1 and 3  had similar knowledge   and
capabilities at the start of their hovercraft experiences, but the teacher knew a bit more about hovercraft science and
design-based learning by Week 3.      So  there are two possible   reasons why   the learners in Week  3   might have
performed better:  the teacher's   increased understanding   might have  influenced  the learners' understanding  and
capabilities and/or use of the software might have been responsible.  We have been able to rule out the influence of
the teacher because while our analysis showed that there was some improvement in the teacher's understanding of
science concepts by Week 3, we did not see a significant impact of this on either her explanatory discourse or her
methods of teaching. This suggests that use of SHADE's explanation-construction tool was primarily responsible for
the better quality of explanatory discourse among Week 3 participants.   Our explanation for the increased number of
explanations in  Week   3 is  that   situating SHADE's    explanation-construction   tool in  the  context of  design
investigations gave participants practice both in explaining observations and also in identifying opportunities to
explain. A possible explanation for the differences in the form and content of the explanations between Weeks 1 and
3 is that learners who received structured explanation support in SHADE developed better conceptual frameworks in
which to organize the specific concepts they learned, and the external discursive representation gave participants a
better understanding of the form of a good explanation. This account is in line with the foundational literature we
drew on in SHADE's design which suggested that explanation support would provide specific guidance about the
nature of scientific explanations.

       How did SHADE impact the teacher? Although the software had an equal potential to impact the teacher's
discourse, SHADE influenced learners more than the teacher during this study. That can be explained by the fact

                                                          748                                                CSCL 2007
that the teacher did not use SHADE at all. The constant presence of researchers during all the 3 weeks did not
necessitate the teacher's use of the tool to integrate it into her teaching. Under normal circumstances, though, we can
expect that the teacher would use SHADE before and during the implementation of a unit. This has the potential to
influence  teachers'   discourse  as  well,   in the same   way  that  the software usage    influences  the  learners. We   also
expect that   this change    in teacher's  discourse   will be  an  additional   influence   in enculturating  the  learners into
becoming better scientific explainers. A useful extension of this study would combine the kind of analysis presented
here with discourse analysis of teachers in the classroom after they actively use and integrate the SHADE software.

         A    software tool  like  SHADE      makes   a difference   in how    learners and  teachers   engage  in  collaborative
learning   to become   better   scientific explainers.  Our  in-depth   discourse  analysis  suggests   that external  discursive
representations    embodied   in  the explanation-construction      tool  affect collaborative   knowledge    construction.  Our
results have implications for learning and instruction in design-based learning environments. Often, teachers' lack of
expertise  in   facilitating  knowledge       construction  in  such    environments    hampers     development     of  scientific
understanding    among   learners.    Our     hypothesis is  that   enculturating  learners     and teachers   into explanation
construction   in  the  context    of design-based      investigations    promotes  such     scientific understanding    through
collaborative  knowledge     construction,    and  our  results suggest   that a tool   like SHADE      that models appropriate
discourse has an important role to play as a mediational resource in facilitating collaborative interactions in the
classroom.

Acknowledgements
         This research was supported in part by a grant from the National Science Foundation.                 We wish to thank
Eric Goldstein, Tony Docal and Megan McCollum for their support in this research effort.

References
Bell, P., & Linn, M. C. (2000). Scientific arguments as learning artifacts: designing for learning from the web with
         KIE. International Journal of Science Education, 22(8), 797-817.
Coleman, E. B. (1998). Using Explanatory Knowledge during Collaborative Problem Solving in Science. Journal of
         the Learning Sciences, 7(3/4), 387-427.
Kolodner, J. L., Camp, P. J., Crismond, D., Fasse, B., Gray, J., Holbrook, J., Puntambekar, S., & Ryan, M. (2003).
         Problem-Based Learning Meets Case-Based Reasoning in the Middle-School Science Classroom: Putting
         Learning by DesignTM Into Practice. Journal of the Learning Sciences, 12(4), 495-547.
Lave,  J., &   Wenger,   E.   (1991).  Situated    learning:   Legitimate  peripheral    participation.   Cambridge,    England:
         Cambridge University Press.
Roschelle, J., & Teaslay, S. D. (1995). The construction of shared knowledge in collaborative problem solving. In C.
         E. O'Malley (Ed.), Computer supported collaborative learning (pp. 69­97). Berlin: Springer-Verlag.
Roth, W.-M. (2001). Situating cognition. Journal of the Learning Sciences, 10 (1/2), 27­61.
Ryan, M. T. & Kolodner, J. L. (2004). Using `Rules of Thumb' Practices to Enhance Conceptual Understanding and
         Scientific Reasoning in Project-Based Inquiry Classrooms. Proceedings of the International Conference of
         the Learning Sciences 2004, (pp. 449-456). Los Angeles, CA: Lawrence Erlbaum Associates, Inc
Sandoval, W. A., Crawford, V.M., Bienkowski, M., Hurst, K., & Millwood, K. A. (2003). Effects of explanation
         support on learning genetics. Paper presented at the annual meeting of National Association for Research
         in Science Teaching 2003, Philadelphia.
Sandoval, W. A., & Reiser, B. J. (2004). Explanation-driven inquiry: Integrating conceptual and epistemic supports
         for science inquiry. Science Education, 88, 345­372.
Scardamalia,   M.,  &  Bereiter,   C. (1994).    Computer   support   for  knowledge-building    communities.    Journal  of  the
         Learning Sciences, 3(3), 265-283.
Suthers, D. D., & Hundhausen, C. D. (2002). The effects of representation on students' elaborations in collaborative
         inquiry,   In G.    Stahl (Ed.),   Computer     support   for  collaborative   learning:   Foundations     for a CSCL
         community. Proceedings of CSCL 2002 (pp. 472-480). Mahwah, NJ: Erlbaum.
Toth, E.,  Suthers,  D., &    Lesgold,     A. (2002).  Mapping    to  know:    The effects   of representational   guidance  and
         reflective assessment on scientific inquiry skills. Science Education, 86, 264­286.
Vattam, S. & Kolodner, J. L. (2006). Design-based science learning: important challenges and how technology can
         make a difference. Proceedings of the International Conference of the Learning Sciences 2006, (pp. 799-
         805). Bloomington, Indiana: Lawrence Erlbaum Associates, Inc.

                                                                749                                                     CSCL 2007
