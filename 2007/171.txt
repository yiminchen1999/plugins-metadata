     A Reflective Analysis of Facilitation in an Online Problem-Based
                                                    Learning Activity
                      Sharon J. Derry, University of Wisconsin-Madison, derry@education.wisc.edu
               Matt DelMarcelle, Humana Center of Quality & Excellence, mdelmarcelle@humana.com

         Abstract:     The   first  author   collaborated   with   the   second  to analyze    his    facilitation of  online
         problem-based      learning    (PBL).    A framework      for  analyzing   reflective collaboration       in science
         instruction developed by Radinsky (2000) was adapted to examine threaded discourse in an online
         teacher   education    course.    In the   spirit of Schön     (1983), we  proffer   this method    as    a  tool for
         analyzing and improving one's online facilitation of PBL and for developing scientific hypotheses
         about connections between facilitation and learning.

Introduction
         We    present    a method     for evaluating      discourse  in online   problem-based       learning  (PBL)    groups   while
simultaneously helping new online facilitators (usually graduate students) reflect on how their facilitation affects
that discourse.   We     illustrate the  method     using  data   collected  during    a problem-based      learning   (PBL)   activity
implemented      entirely online    as part   of  a learning-sciences    course    for   pre-service  teachers.    PBL   is a  form   of
instruction   in which    learners  collaboratively   engage    in ecologically    valid  problems     with the    goal  of  promoting
transfer of knowledge to professional practice (Barrows, 1988; Koschmann et al., 1994;). PBL is a student-centered
form  of instruction   that  replaces    the  teacher with    multiple   small-group     facilitators (Dolmans     &  Schmidt,   2000;
Meyers Kelson & Distlehorst, 2000). The facilitator operates as a guide who pushes students to think deeply and
work effectively (Dolmans & Schmidt, 2000; Hmelo & Lin, 2000; Koschmann, Glenn, & Conlee, 2000). During the
online PBL activity in the present study, one of the authors ("MD") served as the facilitator. Following the activity,
we employed a method for analyzing patterns in collaborative discourse, based on a framework by Radinsky et al.
(2000), to create explicit graphical representations of group interaction. These representations became objects for
supporting    analysis of   facilitation strategies,  which   took    place in  post-course  reflection    sessions.   In  the spirit of
Schön (1983), we claim that this method for supporting structured reflection is not just a tool for analyzing and
improving one's teaching, but also a methodology for developing scientific hypotheses about connections between
small-group facilitation and learning.

         In   online   PBL,  the    role of   the facilitator  is  complex   and   multifaceted.      He or she    is  responsible    for
monitoring student and group process as well as the progress and understanding of individuals. Unlike a typical
classroom     setting in  which  the   teacher   does 95    percent   of the questioning     and tends   to focus     on   short-answer
questions   (Dillion,  1990;    Graesser   &   Person,     1994),  PBL   aims   to  be   much  more    student-centered.     Fostering
reflective, constructive discourse wherein much of the questioning is done by the students is a major goal of PBL.
To accomplish this, the facilitator tends to ask a variety of complex questions that focus students not only on the
domain   and     problem    but also   on   group   processes     while  at  the   same   time   scaffolding    students    to   assume
responsibility   for  self-directed  collaborative    learning    (Hmelo,   1998;   Hmelo    &   Lin,  2000;    Persons    &  Graesser,
1999). Looking at face-to-face PBL instruction, Hmelo-Silver (2003) studied Howard Barrows, the acknowledged
founder of PBL and a well-known facilitator, to gain a better understanding expert facilitation. The nature of expert
PBL facilitation was not easily captured: it is a complex and often subtle form of practice. Not surprisingly, it can be
an extremely     challenging    pedagogy    for  new  facilitators   to  master in  face-to-face   situations   (Derry   et al., 2004).
Although    asynchronous     tools   within   online  PBL     environments      my  slow   the speed     of interaction,    decreasing
facilitators' cognitive   load  and    allowing     more   time   for  reflection  during    facilitation,  the online     environment
nevertheless adds an additional aspect of unfamiliarity and, as this study will illustrate, challenges for those learning
to facilitate PBL on line.

The Context
         The     present  study utilized   data   collected   from an   online  course   for preservice    teachers   focusing   on   the
learning sciences, particularly cognitive and sociocultural theories of learning and instruction. A goal of the course
was to promote students' abilities to use learning-science concepts and theories to both analyze and design classroom
instruction. A web-based environment was developed to support this course (Derry et al., 2005) and consists of three
sets of tools: (1) an online PBL activity center in which students can engage in collaborative problem solving and
discussion using a structured whiteboard and threaded discussion board; (2) an online hypertext book called the
Knowledge Web, a collection of interconnected web pages dealing with learning-science concepts that students in

                                                                  171                                                         CSCL 2007
the course use to conduct independent research; and (3) an online video library containing video cases of classroom
instruction with  links to relevant    learning-science concepts     in the  Knowledge    Web.   Facilitators interact with
students through a web interface that gives them access to all student work and activity.

Table 1 Collaborative steps of the PBL activity
Step              Tasks Involved                                               Step Goals
3    Join group.                                 Become familiar with group members, nominate minicases.
4    Select clips from video case to analyze;    Narrow the scope of the analysis to a manageable piece of video (2
     select concepts to utilize in analysis.     minicases) and reasonable number of concepts (3 Concepts).
5    Research concepts selected for analysis.    Become more knowledgeable about concepts chosen for the analysis.
6    Analyze the selected video segments         Collaboratively investigate how the chosen learning-science concepts
     using researched understanding.             may be applied to the chosen minicases.
7    Construct the final analysis.               Synthesize group's research and discussion to produce an analysis of
                                                 how the chosen learning-science concepts apply to the video case.
         The PBL activity implemented for this study consisted of 12 steps, which, for research purposes, could be
divided  into two phases.  The   first phase  of the   activity (steps  1-8) required  students  to both individually   and
collaboratively analyze a video case of instruction, taking a learning-sciences perspective. The second phase of the
activity (steps 9-12) required students to individually design lesson plans and justify them with learning-science
concepts. It was intended that the video analysis portion of the activity in phase 1 would provide students with a rich
understanding of a set of learning-science concepts they could use to help design their instruction in phase 2.        Steps
3 through 7, described in Table 1 above, are the focus of the present study as they are where MD interacted with
students and facilitated collaborative group activity on line. Students worked mostly individually on the activities
that came before and after steps 3 through 7.

Methodology

Data Source
         Two groups of five students worked with MD over a period of several weeks. MD was a teaching assistant
who had some training and a little experience facilitating PBL in face-to-face settings, but he had not worked on
line. The students were advanced undergraduate pre-service teachers majoring in secondary education and one in-
service  graduate-level teacher.   The  activity occurred   completely    online; students   did not  attend  regular  class
meetings for the duration of the activity. The data for our analysis were collected from the threaded discussion board
embedded within the instructional environment. This discussion board was the primary tool for communication.
While email was also used as a method of communication, it was primarily a one-way channel that MD utilized to
occasionally disseminate course announcements. The discourse that emerged on the discussion board provided a
window into students' thinking and also into MD's facilitation and its influence on student thinking.

Analytical Framework
          We adapted a framework for analyzing group discourse developed by Radinsky et al. (Radinsky, 2000;
Radinsky, Liemberer & Gomez, 2000). Radinsky et al. aligned their model with the pedagogical theory of Dewey
(1933) noting "Dewey placed reflection at the center of his model of teaching and learning, as a key piece of the
process  of making sense   of experience"    (p.9). In this framework    reflection is defined   as purposeful  thought  or
activity directed at making sense of "situations ... containing a difficulty or perplexity" (Dewey, 1933; quoted in
Radinsky et al., 2000, p. 9). Reflectiveness is not seen as a momentary phenomenon, but rather as a dispositional
and enduring characteristic of individuals that develops within activity systems.

         Radinsky et al. developed their framework in the context of a middle school science classroom in which
students collaboratively engaged    in analysis  of complex     sets of geological  data. In analyzing  the   data, students
applied the conceptual knowledge of the domain to develop physical models (e.g., miniature tectonic plates) that
explained patterns in the data. While this context is obviously different from the context of the present study, there
were important similarities that justify our borrowing of their framework. First, the activities that Radinsky et al.
analyzed were collaborative in nature; students engaged in complex problems and discussed their thinking during
the activity. Second, the activity that Radinsky et al. studied required students to model complex data sets to learn

                                                            172                                                   CSCL 2007
course content. Similarly, the students in our study engaged in collaborative analysis of a video case of instruction
using conceptual perspectives based on The Learning Sciences to explain and model cases. These video cases are
essentially complex data sets that, within our course, must be parsed and modeled and interpreted using theoretical
and conceptual lenses. This requires students to exert effort to make meaning and is essentially a reflective task.
Table 2 Components of Radinsky et al. (2000) analytical framework
Context   Component                     Description (Operational adaptations developed for this study)
Data    Data Items                      Reflection on specific items within the video data set.
        Data Patterns                   Reflection on broader slices of video, described with broad characterizations;
                                        particular events are not mentioned.
        Real World Items                Reflection on real life examples from outside the data corpus.
        Domain Concepts                 Reflection with concepts from learning-science curriculum of the course.
        Conceptual Models               Domain concepts are used to explain and analyze segments of the video and
                                        advance instructional design principles as causal models.
Task    Action Decisions                Reflection on how to proceed with the activity.
        Characterization of the Task    Reflection on general purpose or goal of task.
        Teacher Guidance                Reflection on the contributions and guidance provided by the facilitator.
        Artifacts and Tools             Reflection on what and how artifacts and tools are to be used in the activity.
        Group Norms                     Reflection on how the group interacts to complete the activity.
Role    Student Identity                Discussion of or reflection on personal identity.
        Beliefs/Understandings          Reflection on current understanding or beliefs related to task.
        Prior Experience                Reflection incorporating prior experiences related to the task.
        Conception of Norms             Reflection on how individuals are to participate both socially and
                                        intellectually in the activity.
        Student Roles                   Personal reflections on how one should participate.
          Radinsky (2000; Radinsky et al., 2000) claimed there were three contexts in which reflection occurred
during an instructional activity: data, task, and role. They described each as follows:
        The `data context' is a representation of what we want students to think about and figure out:
        domain concepts, sets of data for them to study, the real-world items which data represent, and
        models representing all of these things. The 'task context' is a system of activity in which we hope
        this   mode of  thinking   will develop,  through   instruction.  The   `role' context is a  system   of
        individual factors which contribute to a student's mode of participation in inquiry and other kinds
        of classroom activity. (p. 17)
Within each context were five components or topics for reflection, which are listed on the left side of Table 2.

          Radinsky looked for changes in student reflection patterns across the three contexts over time, watching
for a maturing of their "reflective dispositions for investigating complex data." Within the context of our course
focusing on collaboration, transfer and use of learning-science concepts in teaching practice, maturing patterns of
reflection might include evidence of students' developing interest in self-directed learning about the subject, taking
responsibility for  their collaboration,  or   increasingly connecting      the conceptual  analyses required    by   their
instructional activities to their real-world classroom experiences (they were simultaneously enrolled in a practicum).
These dispositions would be manifested in our analyses as students' increasingly reflecting on particular elements
within the data, task and role contexts in ways that suggest owning the task and connecting it to their lives.

Coding
          To   enable  us  to code    our data using   this framework,      Radinsky's   components  were    adapted  and
operationalized to fit our instructional  context (see the  right side   of Table  2). Each  component    in Table  2 was
considered a potential subject for student reflection. Online posts were coded to identify which subjects of reflection
occurred. Below  is an  example    in which  a student's post  contained    a reflection on instructional guidance  and a
learning-science domain concept (modeling) after getting advice from MD about how to proceed with the activity.
(In the example below teacher education students are viewing a video of a high school social studies class in which
secondary students in the video are themselves engaging in PBL and in which the teacher in the video is modeling
self-directed learning behaviors.)

                                                           173                                                  CSCL 2007
         [Student]: As [MD] mentioned, PBL is new to this group of students (Code: INSTRUCTIONAL
         GUIDANCE) and I think that is key for recognizing the modeling (Code: DOMAIN CONCEPT)
         in this situation.

All data from the online posts during steps 3-7 of the online activity were coded. Coding reliability was calculated to
be 85% (percentage of exact agreement) using two coders looking at 20% of the data. After coding, each discussion
thread was analyzed in a process that involved mapping it onto a graphical representation of the Radinsky et al.
framework (see Figure 1 below). The term "thread" refers to a series of posts that resulted from responses to a single
post initiating a topic, which was usually done by the facilitator. Each line (labeled with participant initials and
numbers representing order of post) in the map represented a student or facilitator post. The nodes connected by
each line represent the components within the three contexts of reflection that a particular post addresses. Threads,
which represented distinct phases within the activity, were analyzed with separate graphical representations.

Results
         The collaborative video analysis activity was designed to have five online steps (steps 3-7, see Table 1). In
step 3 the group logged on to the threaded discussion board for the first time to introduce themselves and nominate
minicases (short segments within the video) to focus on for their analysis. In the fourth step of the activity, students
were to reach consensus on two video segments ("minicases") to focus on as a group and on what "learning issues"
(in PBL parlance), that is, what learning-science concepts, they wanted to further investigate in depth for use in their
video analyses. In steps 5 and 6 they conducted individual research into learning-science concepts and brought their
research back to online     discussions of the minicases,   sharing their knowledge     in the process of collaborative
analysis. Step seven was a deadline for submitting the final written analyses of the selected video minicases. In
practice, the facilitated discussion threads did not follow these steps exactly; the groups and facilitator sometimes
collapsed or altered the sequence slightly as required. The analysis reflected actual structure of the discussion as it
unfolded rather than the precise structure of the activity.

         In the following we report illustrative analyses for one group, illuminated by three "Radinsky Maps" and
MD's reflections. The maps were selected to reveal trends in early, middle and late stages of the activity, offering a
picture of how the collaboration evolved. The reflections offer explanations that connect facilitation decisions to the
trajectory of the collaboration.

Three Radinsky Maps

Introductions and Minicase Nomination Thread.
         MD began a thread on the discussion board with a post asking students to introduce themselves and to
nominate  two minicases     for the  group to analyze. Students   responded by providing    what  they were asked for,
introductions and minicase nominations. For example, one student posted:

         [CK]: Hi everyone!! My name is ***, or ***, it doesn't really matter to me. Let's see,
         something interesting about me...I played tuba for four years in the UW Marching Band
         and marched at the Rose Bowl twice. I have already graduated once from the UW, with a
         double major in Animal Science and Ag Journalism. So, needless to say, it seems like I've
         been in school     forever. Now,  I  am working    on  my  dual  certification in biology and
         agriscience. This summer I will be taking a grad class taught by my advisor in Australia. I
         nominated minicase 4 and 6 for the discussion. Look forward to working with everyone
         this semester.

         Other students posted similar introductions but adding further comments about minicases:

         [JA]: I really like case 8 because it shows what a student learned in participating in this
         activity. The student shows he knows two points on either side of the debate concerning
         school vouchers. I think that the case shows this student could reflect the results of the
         class. I like how it shows what the effort put into the activity produces

         Introductions were slow coming in. Before they were complete MD pushed the group to move ahead and
reach consensus on their minicases:

                                                            174                                                CSCL 2007
       [MD]: Hi Everyone, To stay on schedule, we should move on to step 4 (choose which two
       minicases   for the   group    to collaboratively  analyze). I  know  that some    of  you haven't
       posted your introduction yet. Please post one if you haven't done so already. Basically
       what  we   need    to do in    this discussion  is decide  which     two minicases    are the  most
       interesting and    conceptually     richest for  discussion.   Below   I  will summarize      which
       minicases   your   group  members     nominated    in  their initial ideas assignment     and  their
       introductions. Let's use this as a starting point for discussion. What I need people to do is
       just jump in and start making arguments for why they think certain minicases should be
       the ones the group analyzes.

       Please reply to this post with arguments for why certain minicases should be chosen. We
       need  to choose    these minicases    fairly quickly   to stay  on   schedule, so  make   sure you
       participate in this discussion.

       The Radinsky Map for this segment looked as follows:

                                Figure 1. Analysis of Introductions/Selections Thread

       As is shown     in Figure   1,    almost every  post coded   as reflection on   student   identity, action decisions
(nominating a minicase), and data patterns (noting general characteristics of the minicases). Students nominated and
gave reasons for particular minicases, but they did not debate which minicases were the best. Students made their
indifference to this decision public:
       [CP]: It may not seem like it, but it really doesn't matter to me which minicases we choose,
       as long as we get some deciding done and can get on with the whole process. This online
       discussion  stuff  is totally  new   to  me  and   I'm hoping   we   can kick  it into gear so  we
       accomplish what we need to in a timely fashion.

CP's group members quickly echoed this post with similar sentiments.

                                                            175                                                   CSCL 2007
MD Reflections on Introductions/Selection Thread.
        Of the infinite ways a facilitator might introduce this activity MD chose this way for two reasons. First,
the schedule was unforgiving. Second, this was the first time the students had worked in this online environment and
with each other; some confusion and disorientation were expected. It thus seemed necessary to proceduralize the
early part of the activity for efficiency, delaying more complex negotiations until later. So instead of problematizing
this stage of the activity, to facilitate their entry he chose a proactive form of facilitation that tended to lead more
than guide. In his journal MD wrote: "At this point, I envision myself being the person that blazes the path for others
to follow. I want to create structure to this impersonal online environment." The students obliged by following MD's
instructions.

        One effect of this strategy appeared to be that students did not experience ownership of the problem. Nor
did they experience a strong sense of "difficulty or perplexity" that might have promoted more reflection. At this
stage of the problem there were several facets of the activity that could have been problematized by students. For
example, the negotiation of the procedure for picking two minicases could have been subject to discussion. Later, at
the time of MD's prompting of students to move ahead, it was becoming apparent that to stay on schedule the
subtasks of the activity would need to be completed rapidly. While this post made clear that argument was expected,
the criteria by which students were supposed to select and evaluate a minicase were left vague and subjective,
although these too could have been problematized for negotiation.

        MD's     concern for the schedule   and  awareness     that students needed logistical support may      have
contributed to a sparse and narrow reflection pattern. A reflective pattern in which students considered other issues
such as group norms, participation norms, and characterizing the activity in a personally meaningful way might have
occurred if there were a higher level of student ownership of the activity (Engle & Conant, 2002) and a sense of a
compelling problem (Radinsky et al., 2000).

                                     Figure 2. Concept Selection Thread

Concept Selection Thread (Step 4 Continued)

                                                          176                                               CSCL 2007
        The task related to this thread was to select concepts that were likely to be keys to analyses of selected
minicases and on which students would conduct research (the last of step 4 of the activity structure). Again, MD
took the lead in beginning this thread with the following post asking students to share their ideas:
        [MD]:   In   this thread  we  will discuss   what   the relevant    learning science   concepts   for
        minicase 8 are. In the previous discussion, some of you mentioned that this minicase shows
        what people learned from this lesson, what we might call learning outcomes. What other
        learning science concepts seem to be at play here?

As reflected in Figure 2, students are evolving their collaboration toward focus on the data context, specifically
domain  concepts,    data patterns, and  models,   essentially  sharing   as individuals   what potential   learning-sciences
concepts were "liked" or "seemed interesting" for analyzing chosen minicases. For example, JA posted:

        [JA]: In case 8 I like the concept of questioning. I think the way a teacher asks questions
        can make all the difference in a discussion and can make them elaborate when a two word
        reply would do otherwise. I think in the case you can see how the teachers questioning
        asks for both sides of the issue showing a neutral stance and just taking in all the research
        and   again   modeling  appropriate   behavior.  So  I  think  that  the concepts  I  see   for 8 are
        questioning and modeling.

MD Reflections on Concept Selection Thread.
        Similar   to  MD's   introductory  post,  this introductory    post  was  essentially a short-answer      question that
requested information and gave no indication that this step should be problematized. A discourse pattern emerged in
which students followed the facilitator's lead and did not display much agency in directing the activity or discourse.
As is shown in Figure 2, the nexus of students' reflective discourse is in the process of moving into the data context.
While there are posts that reference the task context, and to a lesser extent the role context, the majority of these
were MD's. In these posts, MD characterized the task for the students and clarified how to use the tools. On these
issues, students followed MD's lead. Moreover, they did not challenge or question one another's ideas, which might
have fostered more reflection. This may be due to students interpreting the task set for them as one in which they
find "interesting" ideas to research; hence it would make no sense to engage in controversy.

Collaborative Analysis: Research and Analyze (steps 5 and 6)
         During steps five and six of the collaborative analysis, students began researching the concepts that they
selected in the previous step of the activity and using them to analyze the minicases they selected. Figure 3 shows
the graphical representation   of   one of the   two threads in  which    students   discussed  their   analyses. Each thread
focused on one  minicase    analysis.   Only  one thread  map   is   shown   due to  space  limitations,  but  both maps   are
virtually identical.

         As shown in Figure 3, students' reflective talk did make important connections among data patterns,
domain concepts and instructional design models. An example from CK:

                 [CK]: While there are many forms of assessment, Kyle chose a form of Authentic
                 Assessment for measuring the progress of his students. Authentic Assessment is
                 used when traditional methods are judged too narrow in determining the actual
                 cognitive outcome. In this case Kyle used Performance Testing, or testing where
                 students   create  a   tangible product as  a  way    to show   their understanding     of a
                 subject   matter.  He  also  appeared   to be  assessing    the students  by  asking   them
                 about    the opinions   they had  developed.   .  . . Authentic    Assessment    . . . helps
                 students develop a variety of skills instead of just the ability to regurgitate facts
                 presented to them by the teacher.

To which MD responded:

                 [MD]: I like the point that [CK] has made about authentic assessment. We were
                 able to glimpse at the form of assessment that Kyle was using in this minicase. As
                 [CK] points out, it seemed more authentic than a more objective assessment might
                 seem.    How    might   we   use the  concept    of   transfer  to better understand     this

                                                            177                                                      CSCL 2007
                   assessment?   What    exactly  might  the   "letter to  the president" assessment   be
                   testing?

MD    noted in his journal:  "Because    they are  all doing   their own   research, they don't seem   to be  interested in
questioning the research that others are doing."

                                           Figure 3. Conceptual Analysis Thread

MD Reflections on Conceptual Analysis Thread.
         From the beginning MD had looked forward to steps 5 and 6 (the research and analyses steps) as having
greater potential than earlier phases of the activity to support meaningful discussion. In these threads students did in
fact begin to connect domain concepts and data patterns to build plausible complex models of instruction informed
by learning-science concepts, which was an important instructional goal. This was viewed as a partial success for
this phase of the activity. In addition, MD noted that his questioning strategies had evolved from primarily short-
answer questions to long-answer questions. Nevertheless, discussion was not rich as students did not often challenge
one another, which  limited     their need to reflect  further on what  they   posted. Moreover, their evolved  reflection
patterns became even more narrowed and focused, failing to cover much range even within the data context: there
were  no connections  at    all to real-world  or  personal    experiences,  for example.  Nor  did they   reflect on any
components within other contexts of reflection (task and role). MD concluded that the pattern of explicit leadership
he had demonstrated up to this point, along with the emphasized importance of sticking to the schedule and failure
to set norms for problemitizing issues early in the task, likely hampered students from questioning one another.
Discussion
         MD's    strategy   constrained  students' involvement    in   the decision-making process  early  in the  activity,
which likely produced the narrow and relatively shallow reflection patterns that evolved in this study. In contrast to
the patterns of questioning that Hmelo-Silver (2003) found an expert facilitator demonstrated, MD observed that his
early questioning  strategy employed     mostly  short-answer   questions.  Also  missing from  his repertoire were   task-
oriented questions that would push students to think beyond the domain content of the problem to question the
nature of the problem and determine what process would be used to investigate it. However, multiple factors that
seemed to be at play need to be considered and are discussed below.

                                                            178                                                  CSCL 2007
           First, the strategy that MD employed was successful on several levels. Students participated at persistent
levels throughout the activity. Trial runs were performed on the system prior to this implementation revealing that
(1) participation could not be taken for granted, it needed to be prompted; and (2) logistically complex activities can
quickly result in confusion, lowering participation. In conquering these issues, MD used a strategy that essentially
removed   all   logistical responsibilities   from   students early   in  the  activity, in order   to avoid   the confusion  and
slowness that plagued trial runs of the system. As a result, in addition to participating and completing the activity,
students  were    more focused    on   the  content  of the problem    rather  than  logistics, as  evidenced   by  their evolved
reflection pattern within the data context in steps five and six. This created tradeoffs in that valuable aspects of the
activity were sacrificed for the sake of getting it done. Such tradeoffs are ubiquitous in teaching (e.g., Leonard &
Derry, 2006), but they are problematic.

           The patterns uncovered in this analysis showed two things. First, students were highly focused on the
paths that MD blazed for them: students closely followed instructions both in what they produced, but also in what
they thought about, primarily domain concepts, data patterns, and models. Second, students did not reflect much
within the task and role contexts proposed by Radinsky. This led us to question whether it is necessary to reflect on
these contexts.

           MD's strategy of carrying the logistical load during the early activity was implicitly based upon his initial
assumption that these logistics were not related to the intended learning goals. But this analysis confirms in our
minds that this was a flawed assumption. Engle and Conant (2002) argued that in order for students to productively
engage in a discipline, they need to be presented with intellectual problems in a manner that will provide them with
the authority to frame and solve problems. Not ceding this aspect of the activity to students runs counter to strategies
of successful facilitators (Frederiksen, 1999; Hmelo-Silver, 2003; Palincsar, 1999). The fact that students generally
did  not  reflect on the   task   and  role contexts  suggests   that students   did not  perceive   themselves    as  owning the
problem and hence they were not highly invested in or motivated by it. Students were not questioning or negotiating
process   or their own  participation.     Students   did not argue   (or care?)   about  what  could  serve   as  evidence that a
particular learning-science concept was relevant or appropriate in a particular situation. And despite the course's
focus on transfer, students never reflected on the relationship of their coursework to their teaching experiences or
lives. It is  possible to   see   this problem    in  another   light as  well:  Researchers    are recently   writing  about the
importance    of  developing    students'   21st century   "soft skills"  (e.g., Bereiter   & Scardamalia,     2006).  Such  skills
include   communication,     leadership     and  collaborative   competencies      that  would   likely  be    developed  through
reflective engagement in the task and role contexts.

          The current study was a proof of concept that the Radinsky et al. framework can be generalized to an online
instructional context in a very different domain and for a different purpose: supporting and organizing the reflective
thinking of new online facilitators. For this we found the framework useful and reasonably efficient. The method
had limits however. For example, it depicted the target and content of thought, but does not of itself reveal much
about functional discourse strategies, which might be fostered through facilitation to promote greater reflectivity in
groups. Moreover, we found ourselves questioning where reflectivity ended and 'plain thinking' began, and whether
one statement was more or less reflective than the other. We resolved this temporarily with the assumption that
public discourse in a PBL context is nearly always a form of reflection, although we are not satisfied with that
answer.

          It is also necessary    to   ask  the  extent to which   the tools   and  technologies    employed   in  the  instruction
imposed   constraints  on   the   instructional  process,   the  facilitator,  and students.  MD's     journal reflects numerous
frustrations with limits of the technology, and we readily acknowledge that these exist. It is always likely that a
better chat environment, interface, instructional design, or Internet service could make a big difference. It is possible
that such    technologies  set  a theoretical   limit on  performance     that partially  determines   the effectiveness  of PBL
instruction. However, we sense that facilitators can make a huge difference in any environment, regardless of its
design   strengths and     flaws. With   this   study we   benefit   from the    learning and   shared  experiences    of a  brave
facilitator.

References
Barrows, H. (1988). The tutorial process. Springfield IL. Southern Illinois University Press.
Davidson-Shivers, G. V., Muilenburg, L. Y., & Tanner, E. J. (2001). How do students participate in synchronous
          and asynchronous online discussions? Journal of Educational Computing Research 25(4), 351-366.

                                                                 179                                                     CSCL 2007
Derry, S. J., Seymour, J., Steinkuehler, C., Lee, J., & Siegel, M. (2004). From ambitious vision to partially satisfying
        reality: An  evolving   socio-technical design supporting   community   and collaborative learning  in teacher
        education. In S. A. Barab, R. Kling & J. Gray (Eds.), Designing for virtual communities in the service of
        learning (pp. 256-295). Cambridge, MA: Cambridge        University Press
Derry, S. J., Hmelo-Silver, C. E., Feltovich, J., Nagarajan, A., Chernobilsky, E., & Halfpap, B. (2005). Making a
        mesh of it: A STELLAR approach to teacher professional development. In T. Koschmann, D. D. Suthers &
        T.-W. Chan (Eds.), Proceedings of Computer Support for Collaborative Learning (CSCL) 2005, Taipei,
        Taiwan (pp. 105-114). Mahwah, NJ: Erlbaum
Dillion, J. T. (1990). The practice of questioning. London: Routledge.
Dolmans, D. H. J. M., & Schmidt, H. G. (2000). What directs self-directed learning in a problem-based learning
        curriculum? In D. H. Evensen and C. E. Hmelo (Eds.), Problem-based learning, A research perspective on
        learning interactions. Mahwah, NJ: Erlbaum.
Engle, R.  A., &  Conant,    F. R.  (2002). Guiding    principles for fostering productive  disciplinary  engagement:
        Explaining an emergent argument in a community of learners classroom. Cognition and Instruction, 20(4),
        399-483.
Graesser, A. C., & Person, N. (1994). Question asking during tutoring. American Educational Research Journal, 31,
        104-137.
Hmelo, C. E. (1998). Problem-based learning: Effects on the early acquisition of cognitive skill in medicine. Journal
        of the Learning Sciences, 7, 173-208.
Hmelo, C. E., & Lin, X. (2000). Becoming self-directed learners: Strategy development in problem-based learning.
        In D.    Evensen &   C.  E. Hmelo   (Eds.),  Problem-based    learning: A  research   perspective on learning
        interactions (pp. 227-250). Mahwah NJ: Erlbaum.
Hmelo-Silver, C. E. (2003). Facilitating collaborative knowledge construction. Paper presented at the 36th Hawaii
        International Conference on System Sciences (HICSS'03).
Koschmann, T. D., Myers, A. C., Feltovich, P. J., & Barrows, H. S. (1994). Using technology to assist in realizing
        effective learning and instruction: A principled approach to the use of computers in collaborative learning.
        The Journal of the Learning Sciences, 3, 227-264.
Koschmann,   T., Glenn,  P., &  Conlee,  M.  (2000). When   is a  problem-based  tutorial not tutorial? Analyzing the
        tutor's role in the emergence of a learning issue. In D. H. Evensen and C. E. Hmelo (Eds.), Problem-based
        learning, A research perspective on learning interactions (pp. 53-73). Mahwah, NJ: Erlbaum.
Lin, X., Hmelo, C., Kinzer, C. K., & Secules, T.J. (1999). Designing technology to support reflection. Educational
        Technology Research and Development, 47(3), 43-62.
Meyers Kelson, A.C., & Distlehorst, L. H. (2000). Groups in problem-based learning (PBL): Essential elements in
        theory   and practice.  In D. H. Evensen    and  C. E. Hmelo    (Eds.), Problem-based   learning,  A research
        perspective on learning interactions (pp. 167-184). Mahwah, NJ: Erlbaum.
Person, N. K., & Graesser, A. C. (1999). Evolution of discourse during cross-age tutoring. In A. M. O'Donnell & A.
        King (Eds.), Cognitive perspectives on peer learning (pp. 69-86). Mahwah: Erlbaum.
Radinsky, J., Leimberer, J. M., & Gomez, L. M. (2000). Reflective inquiry with complex data: A case study of
        dispositional learning.  Paper  presented   at the  annual  meeting of  the American   Educational   Research
        Association (New Orleans, LA, April 24-28, 2000).
Radinsky, J. (2000). Making sense of complex data: A framework for studying students' development of reflective
        inquiry dispositions. Unpublished Doctoral Dissertation, Northwestern University, Chicago.
Schön, D. A. (1983). The reflective practitioner: How professionals think in action. New York: Basic Books
Williams, S. M. & Hmelo, C. E. (1998). Guest editors' introduction. The Journal of the Learning Sciences, 7, 265-
        270.
Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 0107032 (ROLE)
and the Joyce Foundation under Grant No. 4D6F744. Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reflect the views of the National Science
Foundation or the Joyce Foundation.

                                                          180                                                CSCL 2007
