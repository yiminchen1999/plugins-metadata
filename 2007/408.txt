              A Learnable Content & Participation Analysis Toolkit
          for Assessing CSCL Learning Outcomes and Processes
                       Nancy Law1, Johnny Yuen1, Ronghuai Huang2, Yanyan Li2, Nicol Pan1
   1Centre for Information Technology in Education, University of Hong Kong, Pokfulam, Hong Kong, China
                                            Primary contact: nlaw@hku.hk
            2Knowledge Science and Engineering Institute, Beijing Normal University, Beijing, China
                                       Primary contact: huangrh@bnu.edu.cn

        Abstract: In this paper, the authors first review the different kinds of analysis methods used by
        researchers   to assess students' learning  outcomes   and  processes     to propose   a categorization
        framework that can be applicable for assessment methods of CSCL discourse irrespective of the
        theoretical underpinning of the assessment method. A conceptual design for the construction of a
        suite of learnable content and participation analysis tools is proposed to provide intelligent support
        to analysis of online discourse. It is argued in this paper that the implementation of such a toolkit
        will facilitate collaboration and critical co-construction of knowledge about CSCL outcomes and
        processes among researchers. An example is also provided for the use of VINCA, a prototype for
        the toolkit,  in comparing   the  cognitive engagement   of    two  groups   of students   through text
        analysis.

Introduction
        Since the launch of the World Wide Web in 1991, the use of CMC to support learning has taken flight and
its importance as a field of study is increasing rapidly. There is a wide coverage of research interests and diverse
theoretical and methodological approaches adopted in this burgeoning field of research. While there is no lack of
descriptions and reviews of methods and indicators used to analyze online discourse data, the challenge of finding
an appropriate set of instruments for specific purposes in CSCL research has not become easier. In fact, it is not even
easy to compare and learn from the findings of research conducted by different researchers because of the great
methodological diversities. Computers & Education published in 2006 a special section on methodological reviews
of CSCL research that highlighted issues of accuracy, validity and reliability of the methods adopted (Valcke &
Martens, 2006). Any attempt to make methodological categorizations for CSCL research is extremely difficult and
many different approaches can be found in the literature (e.g. De Wever, Schellens, Valcke, & Van Keer, 2006;
Dringus & Ellis, 2005; Mason, 1992; Rourke, Anderson, Garrison, & Archer, 2001; Strijbos, Martens, Prins, &
Jochems,  2006).  There   are  several key   underlying  characteristics    of the   researches  that contribute to the
methodological diversity and complexity of CSCL research: diversities in the researchers' theoretical underpinnings
(De Wever, Schellens, Valcke, & Van Keer, 2006), their different research interests which span learning outcomes
and learning processes of various kinds as well as mechanisms and models of collaborative learning. The greatest
complexity relates to methodologies used in content analysis of discourse data. There is great diversity even in terms
of the unit of analysis which can vary from a sentence to a paragraph, to a thematic unit, to a message or even a
discourse (De Wever,     Schellens, Valcke,  &  Van  Keer, 2006;  Strijbos,    Martens, Prins,   & Jochems,  2006)  and
reliability is a great challenge even for segmentation of analysis units.

        This paper argues that there is a need for two kinds of tools to facilitate communication and co-construction
of knowledge in the community of CSCL researchers: a common set of descriptors that researchers should provide
to report on their methodologies and analyses results irrespective of their theoretical underpinnings; and a suite of
learnable content and participation analysis tools to facilitate collaboration and critical co-construction of knowledge
about CSCL.   The  next   section   proposes a framework   for  categorizing   methods   used   to analyze the   learning
processes and learning outcomes in CSCL settings with illustrations from published research. This categorization
framework will provide a set of descriptors that researchers could use when they report on their methodologies and
analysis results. Based on this framework, this paper goes on to describe the conceptual design for a suite of analysis
tools and extensible knowledge bases that would provide intelligent support to the researchers in analyzing learning
processes and outcomes and indicators proposed. The suite of tools would be capable of "learning" such that the
performance  of   the tools in  its  effectiveness as an   intelligent    support would  improve    through  continuous
enhancement of the knowledge base through use. The analysis results and the knowledge base derived from the use
of such intelligent tools by different researchers can also be compared to facilitate comparison of different research

                                                          408                                                  CSCL 2007
approaches and methodologies. Finally, VINCA, a prototype for such a suite of tools will be briefly described.

Methods for Analyzing CSCL Processes & Outcomes: a Categorization Framework
             To  date,  research  in    the  area       of   CSCL       may    be  underpinned          by    a       variety of theoretical  subscriptions.
However, the purposes are relatively similar, and can be grouped into three categories based on the kind of research
questions they ask. One category is the "what" questions ­ what have students learnt in this process. This could
include    cognitive,   metacognitive        and  sociometacognitive               outcomes.        The     second        is  those addressing             the     "how"
questions       and six dimensions       can  be  identified            in the  published research                in   characterizing    the CSCL               process:
participative,      social, interactive,     cognitive,          metacognitive      and sociometacognitive.                   The   last category               is those
addressing the "why" questions to explore theoretical models of CSCL, how the process indicators may be related to
the outcome         indicators.  Obviously,      not         only     the  why  questions         are      strongly      underpinned     by  the           theoretical
subscriptions of the researchers, but also the formulation of what counts as indicators for the what and how questions.
On the other hand, irrespective of the theoretical underpinnings, the direct outputs from the analyses of discourse
data, whether the methods adopted are quantitative or qualitative, are indicators for the learning outcomes and/or the
learning processes (i.e. indicators for the what and how questions). Answers to the why questions are constructed by
researchers on the basis of these two kinds of indicators. Furthermore, the same set of indicators may be used in
different researches that adopt different theoretical perspectives and/or address different why questions.

Table 1.     A categorization framework for methods of analyzing CSCL processes and outcomes illustrated with examples drawn
             from published research.

                                                                           Methods, indicators & examples of use
 What (learning outcomes)
 Cognitive                Quality of constructed knowledge using        Critical thinking (+ve & -ve indicators for
                          SOLO taxonomy (unistructural,                 10 categories: novelty, relevance,
                          multistructural, relational & extended        importance, linking ideas, justification,
                          abstract) (Veldhuis-Diermanse, 2002)          critical assessment, practical utility, etc.)
                                                                        (Newman, Johnson, Webb, & Cochrane,
                                                                        1997)
 Cognitive/               Critical thinking (+ve & -ve indicators       Categories of knowledge construction (new
 Metacognitive            for 4 categories: clarification, inference,   ideas, explanations & evaluation) (Veerman
                          judgment & strategies) (Bullen, 1998)         & Veldhuis-Diermanse, 2001)
 Sociometacognitive       Phase of knowledge co-construction            Level of knowledge building (Law &                 Level of discussion (higher-level,
                          (Gunawardena, Lowe, & Anderson,               Wong, 2003), (Law, 2005)                           progressive & lower-level discussions)
                          1997)                                                                                            (Jarvela & Hakkinen, 2002)
 How (learning processes)
 Participative           Level of participation (no. of messages)       Density, intensity (Fahy, Crawford, & Ally,
                         (Henri, 1992)                                  2001)
 Social                   Socially oriented statements (Henri,          Social presence (affective, interactive &          Stage of perspective taking of discussion
                          1992)                                         cohesive responses) (Rourke, Anderson,             (undifferentiated & egocentric,
                                                                        Garrison, & Archer, 1999)                          differentiated & subjective role-taking,
                                                                                                                           self-reflective & reciprocal, 3rd person &
                                                                                                                           mutual, in-septh & societal-symbolic)
                                                                                                                           (Jarvela & Hakkinen, 2002)
 Interactive              Social network analysis (Aviv, Erlich,        Vertical & horizontal interactions (Zhu,           Kinds of content exchanged, directedness
                          Ravid, & Geva, 2003; Palonen &                1996)                                              (vertical questioning, horizontal
                          Hakkarainen, 2000)                                                                               questioning, statements & supports,
                                                                                                                           reflecting, scaffolding, references/
                                                                                                                           authorities) (Fahy, Crawford, & Ally, 2001)
 Cognitive                Cognitive skills (clarification -
                          elementary & in-depth, inference,
                          judgment, strategies) (Henri, 1992)
 Metacognitive            Metacognitive knowledge (about
                          person, task & strategies) and skills
                          (evaluation, planning, regulation &
                          self-awareness) (Henri, 1992)
 Sociometacognitive       Knowledge building developmental              Types of interactions (questions ­
                          trajectory (Law & Wong, 2003) (Law,           information seeking or dialogue oriented,
                          2005)                                         answers to provide information,
                                                                        information sharing, discussion, comment,
                                                                        reflection & scaffolding) (Zhu, 1996)

             It is  argued  here  in    this  section        that     a categorization  framework                 for   analysis  methods     and            indicators
applicable      to  a diversity  of    CSCL    learning           theories    and  research       interests           would   be profitable  for       facilitating
comparison, collaboration and critical co-construction of knowledge within the CSCL community at the three levels
of  research     questions   listed    above.  Table         1   presents     one  suggested       categorization            framework   with         examples        of
methods and indicators drawn from published research. We find that in terms of assessing learning outcomes, three

                                                                               409                                                                           CSCL 2007
categories of  outcomes    were  targeted in the    literature, cognitive, metacognitive  and socio-metacognitive.   An
example of a scheme for assessing cognitive outcomes in the learning of specific contents or concepts is one used in
Veldhuis-Diermanse (2002) which adopted the SOLO taxonomy developed by Biggs & Collis (Biggs & Collis, 1982;
Zhu, 1996). However, we find that most of the analysis schemes for cognitive outcomes in the published literature
do not focus on the learning of content or concepts but on assessing students' critical thinking ability. Further, we
find that some of the indicators such as evaluation, judgment and strategies were considered as cognitive by some
researchers (e.g. Bullen, 1998; Veerman & Veldhuis-Diermanse, 2001) but as metacognitive by others (e.g. Henri,
1992). Instead of assessing the learning outcome of individuals in the collaborative process, some researchers were
interested in assessing the socio-metacognitive ability of a collaborating group to co-construct knowledge through
discourse (e.g. Gunawardena, Lowe, & Anderson, 1997; Jarvela & Hakkinen, 2002; Law, 2005; Law & Wong,
2003).

        Sometimes, CSCL discourse was analyzed to examine the learning processes that took place through the
online discourse. Six categories of indicators for the learning process can be identified in the literature: participative,
social, interactive, cognitive, metacognitive and socio-metacognitive. Examples of participation indicators are levels
of participation (Henri, 1992) and the density and intensity of the discussion (Fahy, Crawford, & Ally, 2001).
Indicators for the social dimension of the discourse include a simple count of socially oriented statements (Henri,
1992), presence of affective, interactive and cohesive responses (Rourke, Anderson, Garrison, & Archer, 1999) and
the stage  of  perspective taking of  the discussion  (Jarvela    & Hakkinen,   2002). Examples    of indicators for the
interactivity of the CSCL discourse include social network analysis (Aviv, Erlich, Ravid, & Geva, 2003; Palonen &
Hakkarainen, 2000), presence of vertical & horizontal interactions (Zhu, 1996), the kinds of content exchanged and
directedness  of the discourse  (Fahy, Crawford,    &  Ally,    2001). Indicators for the cognitive,  metacognitive  and
socio-metacognitive characteristics of the discourse can also be taken as indicators for the respective kinds of
learning outcomes at the points when the process data was captured. In fact, the cognitive skills and metacognitive
knowledge and skills as defined by Henri (1992) bears similarity to the critical thinking skills indicators of Bullen
(1998) and categories of knowledge construction indicators of Veerman & Veldhuis-Diermanse (2001) developed for
the assessment of learning outcomes. Law & Wong (2003) coded the socio-metacognitive characteristics of CSCL
discourses as outcomes reached at various points in time to track the developmental trajectory of groups. These
indicate that CSCL researchers generally perceive learning process characteristics as important outcomes.

        Several researchers have commented on the different units of analysis from sentences to thematic units,
paragraphs, messages and discourses being adopted by different researchers when analyzing CSCL discourse (De
Wever, Schellens, Valcke, & Van Keer, 2006; Rourke, Anderson, Garrison, & Archer, 2001; Strijbos, Martens, Prins,
& Jochems, 2006). We find such differences to exist not only between analysis schemes for different categories of
indicators, but also within the same category. For example Bullen (1998) used a message while Newman, Webb &
Cochrane (1995) used a thematic unit as the unit of analysis for coding critical thinking as cognitive learning
outcomes. So far, it is not clear what impact such differences have on the analysis results and findings. It is even less
clear what kind of similarities or differences exist between different sets of indicators of the same analysis category.

        It  is proposed    here that researchers    should clearly   indicate, for each   analysis method  they  use,    a
categorization for the indicators (i.e. which of the learning outcome(s) and learning process(es) are these indicators
measuring) as well as the unit of analysis employed as a basic nomenclature for methodological description. Such
nomenclature would already facilitate easier comparison of assessment schemes and indicators. More importantly, if
there is an assessment toolkit which can document the operationalization of different analysis schemes indexed
according to this nomenclature, this toolkit would be able to present comparisons of analysis outputs from different
schemes and methods and facilitate more in-depth methodological comparisons and discussions. Furthermore, if the
assessment toolkit can have built-in intelligence to support analysis of CSCL discourse based on the input coding
schemes and be able to learn from coding actions of researchers to derive and improve on the coding rules, this will
greatly facilitate the sharing of knowledge and skills in discourse analysis and hence contribute significantly to
advancement in this research area.

Towards a unified toolset for analyzing CSCL
            There are different tools currently used by CSCL researchers for analyzing online discourse. However,
there are several important inadequacies in the tools that we have available currently that make discourse analysis a
tedious, inefficient and often ineffective process:

                                                           410                                                  CSCL 2007
1.   The tools for different analyses are not integrated so that lots of time is wasted in transforming data into
     different formats for the different analyses.
2.   Quantitative indicators have been criticized to be insufficient to reflect the quality of learning (Meyer, 2004)
     and content analysis is necessary to provide deeper understanding of the learning outcomes and processes.
     However, the only tools that are readily accessible to support content analysis are qualitative data analysis
     tools such as ATLAS.ti or N-vivo. These tools support the definition of coding schemes, search, creation of
     coding indices and exploration of different logical combinations of codes. However, the coding process itself
     is still largely manual and the main coding support is to highlight selected keywords in the discourse text.
3.   The qualitative analysis tools themselves are incapable of learning so that no matter how much discourse
     analysis has been conducted by the tool, it would not make the coding process any less tedious for the coder.
4.   While researchers can share the coding schemes they have developed, there is no tool that can provide a
     mechanism for different researchers to share their coding expertise.
5.   Participation and interaction indicators and content analysis codes are generated separately by different tools,
     making it much more difficult to conduct a sequence of multiple analyses on the same set of data. Examples of
     profitable  multiple  analyses   include generating the   social  network for  discourse associated    with selected
     discourse units that exhibit characteristics of specific cognitive processes, and displaying the coding labels for
     discourse units from group members with a high centrality index.
6.   Coding of CSCL discourse is based on the interpretation of the discourse texts. Text mining techniques thus
     has the potential of providing the backbone for semi-intelligent coding tools but such technology has not been
     incorporated in the commonly available content analysis tools.

        Matrinez   et. al.    (2006) proposed   a mixed-evaluation     framework   and a  software  suite   to study the
participatory aspects of learning in CSCL. Their work represented advances in designing software suites that bridges
social network analysis with qualitative and quantitative analysis of interview and survey data to overcome some of
the conceptual  and  technical   challenges   mentioned  above.  Donmez     et al. (2005)  reported on    the  successful
deployment of the TagHelper technology in the supporting autonmatic multidimensional categorical coding of CSCL
data. The work reported here is an effort to build on and extend related work in the area. In the following section we
describe the design of an analysis framework and a suite of analysis tools with extensible knowledge bases that
would 1) provide intelligent support to the researchers in analyzing CSCL discourse using analyses schemes that fit
within this proposed categorization framework for assessment methods and 2) support comparison of analyses using
different sets of indicators.

A Conceptual Design for Learnable CSCL Assessment Tools
        Based on the above reviews, we have developed a conceptual design for a suite of learnable content and
participation analysis tools for use in CSCL research (see Figure 1). At the core of this toolkit is a coding schemes
and  coding  rules database    which   keeps  a  well organized    set of coding   schemes indexed   according    to the
categorization framework presented in Table 1 above. The database also keeps record of the coding rules that have
been used by various researchers for the same coding scheme and the coding effectiveness for those rules. The
toolkit contains modules that can learn from coding operations to continuously improve the coding rules, as well as
provide mechanisms for users to compare and/or to merge the coding schemes and/or coding rules developed by
different users. The toolkit contains the following key components:

Preparatory Components
        These components are designed to transform discourse data collected from any CSCL platform into a form
that can be processed by the analysis tools and to provide a mechanism for users to define the coding schemes and
coding rules. There are three main preparatory components:

Data Preparation Component
        This component allows the user to take discussion data from a number of popular CSCL platforms such as
threaded discussion, Wiki and Knowledge Forum® and transform automatically into a standard relational database
format. It will also allow the user to define the data structure from unspecified discussion platforms so that the
appropriate data preparation    process can   be  performed.   The resulting discussion  record database    stores basic
information such as author, date and time of post, the threaded discussion structure, message title and message body.
There is also a discourse selection component to allow the user to select a subset of the discourse data for analysis
according to the authors, the time period of the discourse took place, or other characteristics as desired.

                                                           411                                                  CSCL 2007
Figure 1. A conceptual design for a suite of learnable content and participation analysis tools for use in CSCL
research.

Discourse Segmentation Component
          This component   allows the   user to segment the   discourse  into  appropriate units of discourse text for
analysis. Thus the user can define the text units into sentences, paragraphs, themes, messages or any other units as
the researcher finds appropriate. The output would be stored in the segmented discourse database ready for content
analysis operations.

Coding Schemes and Coding Rules Editor
          This editor will allow the user to create and store coding schemes and text pattern rules, which can be
stored in the coding schemes & coding rules database. The coding schemes can have a hierarchical structure. Each
code can also be associated with text patterns and other rules that user has found to have a high probability of being
found in discourse text with that code.

Analysis Components
          There are  three main   analysis components   in   this toolkit: the  participation and interaction analysis
component, the text analysis component and the coding support component. Each component may have several
modules.

Participation and Interaction Analysis Component
          In CSCL studies, researchers are interested in collecting user participation statistics at the individual and
the interpersonal interaction levels. There are thus two modules in this component. The individual participation
analysis module provides basic statistics on an individual's number of posts, replies, or number of keywords used in
the discourse.  For  the inter-person participation analysis  module, it   can produce output  data to generate social
network analysis displays as well as statistics on interaction such as betweenness, centrality, clustering cohesion and
so on. Using this same module, one should also be able to return participation and interaction analysis results for
selected coded data, e.g. the individual participation statistics for discourse statistics showing high levels of critical
thinking; or one could compare the centrality of the same group of participants for socially oriented v.s. inquiry
oriented discourse.

                                                          412                                                 CSCL 2007
Text Analysis Component
         The modules in this component provide text analysis results that can help the user to formulate semantic
analysis strategies on CSCL discourse data. One of the modules in this component performs keywords analysis. For
any set of discourse data, this module can generate the list of keywords and key phrases used and the respective
usage frequency. It can also compare the similarity between users in terms of their keywords usage. The second
module performs domain ontology analysis. Very often, teachers and researchers are interested to know to what
extend students' discussion overlaps with experts' or textbooks' conception of the focal content in the discussion.
This module compares the domain ontology of the discourse with the concept map of the topic drawn by teachers or
experts. Various statistics can be also be generated to reflect participants' performance, such as the similarity of the
group's  ideas when  compared      to the experts,  individual  members'  contribution in  terms  of relevance     of ideas,
novelty  of ideas, and    extensiveness   of ideas  to the discourse. A   third module    in this component     is the text
concordancing module, which essentially allows the user to extract all text segments containing a keyword together
with a user-specifiable length of text before and after the keyword. This is very useful since the semantic context of
a piece of text cannot be clearly reflected by the presence of a single keyword or phrase. An example of how this
module can be used to support further content analysis will be provided in the next section.

         To summarize, the three types of analyses to be conducted by the modules in this text analysis component
expose discourse dynamics at the semantic level. The outputs from the modules in this component in addition to
being useful in themselves as a form of content analysis, can also help the user to generate insights to improve the
analysis  framework  as    well as the   coding  support  component   for conducting  further  content  and participation
analyses.

Coding Support Component
         The coding support component supports researchers to conduct content analysis in more efficient ways
through text mining of the discourse. As an intelligent tool, after the user has selected the coding scheme(s) to
conduct coding, it should be able to provide aids, like highlighting discourse segments that match with the text
patterns in the coding rules database and suggest appropriate codes for those segments. Since it is envisaged that
there are limitations to the effectiveness of automatic coding based on the coding rules alone, the user will be able to
decide which of the coding suggestions to accept. The coding hits and coding errors will be recorded. Further, the
user may identify missed segments which should have been coded and add these codes in manually. These coding
misses will also be recorded. After the coding process has been completed, the coded segments and the coding
statistics (i.e. the frequency of occurrence of the various codes) will be generated for the user. This output can be
exported in a database format for further quantitative and code co-location explorations. In addition, there are two
more outputs from this process, the coding effectiveness statistics for each set of coding rules fired and three lists of
discourse segments for the coding hits, coding errors and coding misses respectively in database format. These last
two sets of outputs will be further processed by the learning mechanisms component. This is the core content
analysis component in this suite of assessment tools and it is also potentially the most powerful one since it is
improvable  with   increased  use  through   the learning  mechanisms  component    included   in this toolkit. When    the
coding effectiveness of the coding rules improves, this component can be further developed to provide a training
module for new coders. The coding results for the same set of discourse data by different coders can also be
compared using this component to provide inter-coder reliability statistics as well.

         An index of the coded discourse segments can also be fed back to the segmented discourse database to
support further text selection criteria to allow more focused multi-step analysis of the CSCL discourse such as
participation and interaction analysis for discourse having specific characteristics.

Learning Mechanisms Component
         There are two modules in this component designed to refine and improve on the coding scheme and the
coding rules database that form the knowledge base for the coding support component. One module is the coding
rules refinement module which makes use of the hits, mistakes and misses lists of discourse segments and the coding
effectiveness  statistics for the  rules associated with   each code  generated  by   the coding  support component      to
improve on the coding rules. The second is the coding scheme and rules modification module which takes as its
input the text analysis output and the user's instructions on the kinds of modifications desired. This module should
be able to interpret keywords, keywords concordancing results, and results from domain ontology analysis.

                                                            413                                                  CSCL 2007
Example Content Analyses Using VINCA, a Toolkit Prototype
         The Visual INtelligent Content Analyzer (VINCA) is a CSCL discourse assessment tool jointly developed
by the Centre for Information Technology in Education of the University of Hong Kong and the Knowledge Science
and Engineering Institute of the Beijing Normal University to implement the design ideas described above. To date,
a prototype for some of the preparatory components and the text analysis components have been implemented while
the participation analysis and learning mechanism component has still to be developed (Huang & Li, 2006). This
prototype is able to process textual records of discussion in both English and Chinese. In this section, we will give
an example of how the text analysis component provides content analysis support that help to locate indicators of
learning, irrespective of theoretical underpinnings, from online discussion logs. It is our intention that findings
generated by the text analysis components will become one important source of information about students' learning
that can be integrated with the participation analysis component to provide useful information to the teacher as well
as learners about the progress of the discussion (Mochizuki et al. (2005) reported on an interesting study in which
students' learning and interactions were influenced by the visualization of the proximity of their contribution to set
keywords entered by the teacher).

         Two groups matched in academic ability participated in an online knowledge building (Scardamalia &
Bereiter, 2003) activity on the topic of "slimming" using Knowledge Forum® organized as part of their formal
school curriculum. Three instruments were designed and administered to the two groups of students after they have
completed the online activity to assess the impact of the activity on them. The three instruments were a weight-loss
and nutrition concept test, a daily food intake inventory, and a weight-loss, exercise and body-image survey. It was
found that the learning activity had significant impact on the understanding and attitudes on the students in one of
the two groups (group A) but not on the other (group B) as measured by both the weight-loss and nutrition concept
test scores and the self-image scores. However, group A had lower counts than group B on some commonly used
quantitative indicators of discourse engagement: the total number of notes posted, the total number of threads, the
length of entries and the total number of keywords in the messages recorded on Knowledge Forum®. VINCA was
employed to see if it can provide some useful information on why such an outcome might come about. In particular,
VINCA was used find out whether the two groups differed in the quality of the online learning discourse. The
analysis provided evidence that group A in fact had a different engagement pattern to group B in the online discourse.
This study was briefly reported in Law & the Learning Community Project Team (2006). Due to the limitation of
space, only one of the analyses done using VINCA is reported below as an illustration of how this tool can be used
to identify different levels of engagement. Detailed reporting on the whole study will be the subject of another paper.

         To identify indicators of learning through the online discussion, VINCA's text analysis component started
with retrieving all keywords and their counts from the whole discourse of the two groups of students. The aim is to
identify keywords  that may   be  indicative of students' cognitive   and/or metacognitive   engagement.    From    the
keywords retrieved, three groups were identified to be useful as indicators of deep engagement that will likely lead
to deep learning. One such group of keywords was indicative of reflection such as consider, think, know, believe,
feel and agree (1). The second group was indicative of the author making a claim or a proposition, and included
words such as in fact, therefore, moreover, explain and based on. A third group was words indicative of the author
making a query, such as what, why, how and words used in questions. Table 2 presents the density of use of these 3
types of keywords  in these  two  groups' discourses. It is  apparent from   the density of use   of these 3  types of
keywords that more cognitive engagement were present in group A's online discourse.

         While the keywords were useful, it was also found that statements containing specific keywords per se may
not actually be related to reflections, propositions or queries. Upon closer inspection, statements containing the
personal pronoun "I" in the proximity of these three types of keywords were more likely to be statements that
involved reflections, claims or queries. In order to increase validity in identifying indicators of learning from online
discussion discourse, a  two  step identification process    was designed.   Firstly, concordance    segments of text
containing 20 words before and after the word "I" was extracted by the data preparation component in VINCA.
These data subsets were then analyzed by the keywords analyzer in the text analysis component. The density of use
of the 3 identified types of keywords in this selected set of text is also presented in Table 2. The result shows that
group A again has a higher density in the use of these 3 types of keywords. Furthermore, the density difference
between these two groups is even higher in this subset of text segments containing the personal pronoun "I".

         To summarize, using the keywords analysis and text concordance analysis modules in VINCA, we have
found quantitative difference in  the density  of keywords    associated with deeper    cognitive and  metacognitive

                                                         414                                                 CSCL 2007
        ReflectionClaim sQueries
engagement identified through a two-step process between the two groups of students which suggested that group A
was more engaged in the online discourse. This triangulates well with the finding that group A achieved better
learning outcomes based on the weight-loss and nutrition concept test and self body-image survey. These results
demonstrate that VINCA is potentially useful in providing useful indicators of learning engagement beyond simple
quantitative measures of writing engagement such as the total number of keywords or word counts. We hope these
would contribute to further discussions and developments in analyzing CSCL discourse.

Table 2 A comparison of the word count and word density for the three selected groups of keywords indicating
        reflection, explanation and query posted by the two groups of students in (a) the whole discourse, and (b)
        the concordanced text segments containing the personal pronoun "I",

                                           Word Count & word density for                      Word Count & word density for selected
                                           selected keywords in the whole                       keywords in the concordanced text
                                                        discourse                                         segments containing "I"
                                              Group A                Group B                      Group A                        Group B
        Number of keywordsidentified          1552                     5396                              738                       2758
           Total number of
          occurrence for all                  4824                    26546                         1834                           9986
         keywords identified

                                                      Density                 Density                          Density                        Density
                                                      per 1000                per 1000                         per 1000                      per 1000
                                          Number of  keywords    Number of   keywords          Number of      keywords       Number of       keywords
                    Keywords             occurrence occurrence   occurrence  occurrence       occurrence     occurrence      occurrence     occurrence
                           Consider        19         3.94         34         1.28             15             8.18            10            1.00
                                Think      5          1.04         27         1.02              4             2.18            19            1.90
                           Feel            8          1.66         11         0.41              6             3.27             4            0.40
                            Believe        2          0.41          7         0.26              1             0.55             3            0.30
                                Know       2          0.41          7         0.26              2             1.09             2            0.20
                                Sense      2          0.41          7         0.26              2             1.09             2            0.20
                                Agree      4          0.83          1         0.04              3             1.64             1            0.10
                     Category Total        42         8.71         94         3.54             33             17.99           41            4.11
                       Density Diff.                 8.71-3.54 = 5.17                                        17.99-4.11 = 13.88

                                In fact    15         3.11         17         0.64              9             4.91             9            0.90
                          Therefore        6          1.24         33         1.24              3             1.64            17            1.70
                           Besides         18         3.73         96         3.62             13             7.09            43            4.31
                          Moreover         5          1.04         16         0.60              3             1.64            10            1.00
                            Explain        1          0.21          1         0.04              0             0.00             1            0.10
                          Based on         4          0.83         10         0.38              1             0.55             2            0.20
                     Category Total        49         10.16       173         6.52             29             15.81           82            8.21
                       Density Diff.                 10.16-6.52 = 3.64                                        15.81-8.21 = 7.6

                          What             5          1.04          0         0.00              3             1.64             0            0.00
                         Why               2          0.41          0         0.00              2             1.09             0            0.00
                                 How       3          0.62          2         0.08              1             0.55             0            0.00
                                How to     1          0.21          7         0.26              0             0.00             4            0.40
                           Question
                           indicator       9          1.87          9         0.34              3             1.64             3            0.30
                           Question
                           indicator       10         2.07          6         0.23              7             3.82             5            0.50
                         Category
                                Total      30         6.22         24         0.90             16             8.72            12            1.20
                      Density Diff.                   6.22-0.9 = 5.32                                          8.72-1.2 = 7.52

                                                                               415                                                               CSCL 2007
Conclusion
           In this paper, we have put forward a framework for categorizing methods used by CSCL researchers to
analyze online discourse to assess students' learning outcomes and processes. Specifically, three types of outcomes
(cognitive,    metacognitive   and socio-metacognitive)   and  six  types  of processes  (participative, social,         interactive,
cognitive, metacognitive and socio-metacognitive) were identified. A proposal that this categorization together with
the unit of analysis adopted should form a basic nomenclature for use by CSCL researchers in reporting on the
assessment methodologies they use in analyzing CSCL discourse to facilitate easier methodological comparison and
explorations.   A   conceptual   design  for  a suite of learnable   CSCL    assessment  tools  that makes           use of  such  a
nomenclature     is also  presented. This    proposed  toolkit contains three   analysis components,     a participation         and
interaction analysis component, a text analysis component and a coding support component which will provide
quantitative participation and interaction statistics, make intelligent coding suggestions for content analysis as well
as iterative multi-method analysis. A very attractive feature of this set of tools is the availability of a coding schemes
and coding rule database which forms the knowledge base for the content analysis components. The learnability is
accomplished through the learning mechanisms component which contains modules that can modify and improve on
the coding schemes and coding rules contained in the database. The learning mechanisms component contains a
coding   rules  refinement   module    that can  make  improvements     to the  coding   rules on  the  basis        of  the coding
effectiveness of the coding support mechanism and a coding scheme and rules modification module which can make
improvements on existing coding schemes and rules on the basis of the text analysis output. An example of how
VINCA, a prototype developed on the basis of this conceptual design, helped to compare the levels of cognitive
engagement for the online discourse from two groups matched in academic ability was also provided to illustrate the
viability and usefulness of such assessment tools to generate analytical insights irrespective of the learning theory
underpinning the CSCL design. It is expected that when the full suite of tools has been developed such that content
analysis results can be analyzed and interpreted together with the results from participation and interaction analysis,
we will be able to gain a much better understanding of what distinguishes a productive CSCL discourse and how
that can be fostered. It will also contribute towards theory building about CSCL. as well as cont the suite to support
mixed-method evaluation for evaluating different levels of engagement in CSCL based on the framework suggested.
It  is hoped   that more   researchers   will be interested in  developing   and   sharing assessment   tools        based   on  this
conceptual design. This will facilitate collaboration and critical co-construction of knowledge about CSCL among
researchers and improve our understanding of the outcomes and processes of online collaborative learning.

Endnotes
(1)    The actual text written by the students were mostly in Chinese. The words listed here are just translations.

References
Aviv, R., Erlich, Z., Ravid, G., & Geva, A. (2003). Network Analysis of Knowledge Construction in Asynchronous
           Learning Networks. Journal of Asynchronous Learning Networks, 7(3), 1-23.
Biggs, J. B., & Collis, K. F. (1982). Evaluating the quality of learning: The SOLO taxonomy. New York: Academic
           Press.
Bullen, M. (1998). Participation and critical thinking in online university distance education. Journal of Distance
           Education, 13(2), 1-32.
De Wever, B., Schellens, T., Valcke, M., & Van Keer, H. (2006). Content analysis schemes to analyze transcripts of
           online asynchronous discussion groups: A review. Computers & Education, 46(1), 6-28.
Donmez, P., Rose, C., Stegmann, K., Weinberger, A., & Fischer, F. (2005). Supporting CSCL with Automatic Corpus
           Analysis Technology. International Conference on Computer Supported Collaborative Learning (CSCL
           2005), Taipei.
Dringus, L. P., & Ellis, T. (2005). Using data mining as a strategy for assessing asynchronous discussion forums.
           Computers & Education, 45(1), 141-160.
Fahy,   P. J., Crawford,   G.,  &  Ally, M.   (2001). Patterns  of  interaction in a computer     conference         transcript. The
           International Review of Research in Open and Distance Learning, 2(1), 1-24.
Gunawardena, C. N., Lowe, C. A., & Anderson, T. (1997). Analysis of a Global Online Debate and the Development
           of  an   Interaction Analysis    Model  for   Examining    Social  Construction   of   Knowledge           in Computer
           Conferencing. Journal of Educational Computing Research, 17(4), 397-431.
Henri,   F.    (1992). Computer    conferencing    and   content   analysis.  Collaborative    learning  through          computer
           conferencing: The Najaden papers, 90, 117-136.
Huang,     R., &  Li,  Y. (2006,   Oct 27   2006). Computer    Supported   Content   Analysis   - Challenges,         research   and

                                                               416                                                         CSCL 2007
        developments.    Workshop      on  Application   of  Wireless   Communications     and    Data    Mining,  from
        http://www.cite.hku.hk/events/doc/2006/computer_supported_content_analysis.pdf
Jarvela, S., & Hakkinen, P. (2002). Web-based Cases in Teaching and Learning - the Quality of Discussions and a
        Stage of Perspective Taking in Asynchronous Communication. Interactive Learning Environments, 10(1),
        1-22.
Law, N. (2005). Assessing Learning Outcomes in CSCL Settings. International Conference on Computer Supported
        Collaborative Learning (CSCL 2005), Taipei.
Law, N., & the Learning Community Project Team. (2006, Oct 27 2006). Indicators for advances Indicators for
        advances in knowledge building ­ Application of content analysis tools to two sets of CSCL discourse data
        from two comparable classes. Workshop on Application of Wireless Communications and Data Mining,
        from http://www.cite.hku.hk/events/doc/2006/cite_team_oct_27_workshop_presentation.pdf
Law, N., & Wong, E. (2003). Developmental Trajectory in Knowledge Building: An Investigation. In B.Wasson, S.
        Ludvigsen & U. Hoppe (Eds.), Designing for Change in Networked Learning Environments: Proceedings
        of  the International    Conference  on Computer    Support  for Collaborative   Learning    2003  (pp.  57-66).
        Dordrecht: Kluwer Academic Publishers.
Martínez, A., Dimitriadis,   Y., Gómez-Sánchez,     E., Rubia-Avi,  B.,  Jorrín-Abellán, I., &   Marcos,   J. A. (2006).
        Studying   participation   networks  in   collaboration  using   mixed  methods.     International    Journal of
        Computer-Supported Collaborative Learning, 1(3), 383-408.
Mason,  R.  (1992). Evaluation    methodologies   for   computer  conferencing applications.     In A.  E. Kaye   (Ed.),
        Collaborative Learning Through Computer Conferencing (pp. 105-116). Berlin: Springer- Verlag.
Meyer, K. A. (2004). Evaluating online discussions: Four different frames of analysis. Journal of Asynchronous
        Learning Networks, 8(2), 101-114.
Mochizuki,  T., Kato,  H.,   Yaegashi, K.,  Nagata, T.,  Nishimori, T.,  Hisamatsu, S.,  et  al. (2005).  Promotion   of
        self-assessment for learners in online discussion using the visualization software. Proceedings of th2005
        conference on Computer support for collaborative learning: learning 2005: the next 10 years!, 440-449.
Newman, D. R., Johnson, C., Webb, B., & Cochrane, C. (1997). Evaluating the quality of learning in computer
        supported co-operative learning. Journal of the American Society for Information Science, 48(6), 484-495.
Newman, D. R., Webb, B., & Cochrane, C. (1995). How to measure critical thinking in face-to-face and computer
        supported seminars do things impossible in face-to-face meetings. through content analysis. Interpersonal
        Computing and Technology Journal, 3(2), 56-77.
Palonen, T., & Hakkarainen, K. (2000). Patterns of interaction in computer-supported learning: A social network
        analysis. Proceedings of the Fourth International Conference of the Learning Sciences, 334-339.
Rourke, L., Anderson,    T., Garrison,  D.  R., &  Archer,  W.   (1999). Assessing  social   presence  in  asynchronous
        text-based computer conferencing. Journal of Distance Education, 14(2), 50-71.
Rourke, L., Anderson, T., Garrison, D. R., & Archer, W. (2001). Methodological issues in the content analysis of
        computer conference transcripts. International Journal of Artificial Intelligence in Education, 12(1), 8-22.
Scardamalia, M., & Bereiter, C. (2003). Knowledge Building. In J. W. Guthrie (Ed.), Encyclopedia of Education.
        New York, USA: Macmillan Reference.
Strijbos, J.-W., Martens, R. L., Prins, F. J., & Jochems, W. M. G. (2006). Content analysis: What are they talking
        about? Computers & Education, 46(1), 29-48.
Valcke, M., & Martens, R. (2006). The problem arena of researching computer supported collaborative learning:
        Introduction to the special section. Computers & Education, 46(1), 1-5.
Veerman, A., & Veldhuis-Diermanse, E. (2001). Collaborative learning through computer-mediated communication
        in academic education. European perspectives on computer supported collaborative learning: Proceedings
        of  the  first European    conference   on  computer    supported  collaborative     learning. Maastricht,    the
        Netherlands: Maastricht McLuhan Institute.
Veldhuis-Diermanse, A. E. (2002). CSCLearning? Participation, learning activities and knowledge construction in
        computer-supported     collaborative  learning   in higher  education.  Unpublished      doctoral  dissertation,
        Wageningen University, The Netherlands.
Zhu, E. (1996). Meaning Negotiation, Knowledge Construction, and Mentoring in a Distance Learning Course. from
        http://eric.ed.gov/ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED397849.

Acknowledgements
This research is supported by a seed grant from the University Research Committee of the University of Hong Kong
awarded to the IT strategic research theme, and Chinese Ministry of Education Higher University Technology
Innovation Cultivation Foundation (No. 705038-01).

                                                           417                                                  CSCL 2007
