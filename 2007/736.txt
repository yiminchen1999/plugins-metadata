The Use of "Knowledge Types" as Scripting Tool to Enhance Critical
                                Thinking in Online Discussions

                      Hilde Van Keer, Tammy Schellens, Bram De Wever, and Martin Valcke,
                 Department of Education, Ghent University, H. Dunantlaan 2, 9000 Gent, Belgium
                 [Hilde.VanKeer] [Tammy.Schellens] [Bram.DeWever]          [Martin.Valcke] @UGent.be

          Abstract: The present study focuses on a particular scripting tool, namely the use of "knowledge
          types" as a way to structure university students' discourse in asynchronous discussion groups and
          consequently promote their learning. More specifically, the aim of the study is to determine how
          requiring students to label their contributions by means of the stages of the progressive inquiry
          model affects the ongoing critical thinking processes reflected in the discussion.
          Preliminary results  indicate that using this    scripting tool can  ­under  certain circumstances-
          enhance critical thinking in online discussions.

Objective
          The present study focuses on the use of scripts to scaffold students' online discourse and to facilitate their
critical thinking. The concept "script", however, encompasses a broad range of methods, techniques, and
approaches. In this respect it is difficult to speak about the overall efficacy of CSCL scripts (Dillenbourg, 2002). In
the present study, we are interested in the impact of a particular kind of scripting - the use of knowledge types - on
the knowledge construction processes reflected in asynchronous discussions. As part of the course "Instructional
Sciences", 287 first-year university students were engaged in asynchronous discussion groups. Two research
conditions were distinguished. In the experimental condition, students were required to tag their contributions by
means of knowledge types. In the control condition students were engaged in an identical assignment. However, no
requirements were made with regard to labeling the knowledge type reflected in one's contributions. In both
research conditions cross-ages peer tutors were following the discussion.
The study is guided by the following research questions: 1) Do students, who were required to tag their discussion
contributions by means of knowledge types, differ from students engaged in regular asynchronous discussions with
regard to (a) the overall depth of critical thinking, (b) the depth of critical thinking for different categories and
indicators, and (c) the depth of critical thinking at successive critical thinking stages distinguished by Garrison.
2) What is the impact of differential tutor behavior?

Theoretical Framework

Critical Thinking
          The present study focuses on the possible impact of collaborative learning on critical thinking, which is
often cited as aim or outcome of education (Perkins & Murphy, 2006). The evolution towards an information age
has focused attention on good thinking as an important element of life success. These changing conditions require
new outcomes, such as critical thinking, to be included as a focus of education. Old standards of being able to score
well on a standardized test of basic skills, cannot be the only means by which the academic success or failure of our
students can be judged (Huitt, 1992). Oliver (2001) argues that critical thinking skills represent an important issue
for education and that these skills are particularly important nowadays in order to make meaningful use of electronic
information.  In this respect, collaborative  learning is  desirable  but only   when grounded   in disciplined  critical
thinking.
          But although most   educators  agree on  the importance    of critical thinking for learning, there is no   real
agreement yet on the exact meaning of the term `critical thinking'. For this research we go along with the definitions
of Chance (1986) and Scriven and Paul (1992) who respectively define critical thinking as the ability to analyze
facts, generate and organize ideas, defend opinions, make comparisons, draw inferences, evaluate arguments, and
solve  problems   (Chance,    1986)  and   as  the  intellectually   disciplined  process  of  actively  and   skillfully
conceptualizing, applying, analyzing, synthesizing, and/or evaluating information gathered from, or generated by,
observation, experience, reflection, reasoning, or communication, as a guide to belief and action (Scriven & Paul,
1992).

                                                           736                                                 CSCL 2007
         A   number  of  theorists have    considered   critical thinking as  a problem-solving    process   (e.g., Brookfield,
1987; Garrison, 1992). Garrison (1992) more particularly identifies five phases of critical thinking. According to his
theory, critical thinkers move through the stages of identifying a problem, defining it more clearly, exploring the
problem  and   possible  solutions, evaluating   their   applicability,  and  integrating  this   understanding  with   existing
knowledge. The model employed to analyze the discourse in the present study is based on Garrison's model which is
a dynamic     cognitive  one, similar  to  models     of problem-solving      used in  cognitive   psychology    and  artificial
intelligence. Although Garrison initially developed it as a means of studying individual learning, it requires shared
understanding with others and is therefore suitable for studying group learning as well.

Scripting
         A   central topic of  CSCL    research   is  how   online  discussion  and   critical thinking   in particular can be
facilitated. One possible  approach    is  to realize `computer-supported      collaboration   scripts'. Collaboration  scripts
essentially concern activities that promote learning, but which rarely occur spontaneously within the discourse of
learners (O'Donnell, 1999). Scripts can be implemented as a kind of guideline. More specifically, a script can be
defined as a detailed and more explicit didactic contract between the teacher and the group of students regarding
their mode of collaboration (Dillenbourg, 2002). This approach is particularly interesting to specify, sequence and
eventually to allocate different learning activities to learners (Weinberger, Ertl, Fischer, & Mandl, 2005).
         In this study we investigate a computer-supported collaboration script, which provides a controlled list of
message  types   from   which the   student   must  select  before  replying  or creating  a   message.   In the experimental
condition, students were required to tag their messages by means of knowledge types, based on the FLE3 knowledge
building environment.    This environment     is designed    to  support  the collaborative    process of progressive   inquiry
learning. The basic idea is that students gain deeper understanding by engaging in a research-like process where they
generate problems,   formulate    hypotheses,  and   search  out  explanatory   scientific information    collaboratively  with
other students (Chen, 2004). More specifically, in the discussions students were asked to label each contribution
with a  category   reflecting one   of the  stages   of  the progressive   inquiry  model.     The provided    categories were
"Problem",    "My  Explanation",   "Scientific   Explanation",    "Evaluation   of the Process",    and   "Summary".    In this
respect, students are asked to step back and to reflect upon the ongoing discussion and on how to contribute to
optimize the debate. Moreover, the labels visualize the possible predominance or absence of one or more knowledge
types. This can help students to create an overview of the knowledge-building activity as it unfolds and to improve
their collaboration and ability to solve open-ended problems.

Method

Participants and Procedure
         All  students   enrolled for  the course   "Instructional  Sciences"   participated   in the  present study  (N=286).
Students were divided into discussion groups of about 8 students, with students randomly assigned to one of the 35
groups and groups randomly assigned to the research conditions. The discussion assignment was the same for all
discussion groups in the study, regardless of the research condition the groups were in. Students in the experimental
condition were required to tag their contributions by means of knowledge types. The online discussion environment
offered a checklist interpreting the different contribution types advancing the discussion process. For each label,
students received a description of what a particular knowledge type implies in terms of a discussion contribution.
Taken into account that transcripts of 35 discussion groups for 4 themes represent a massive amount of data, 9
groups (N=71) were randomly selected for analysis.
         The asynchronous discussions were a formal part of the course. Students participated during a complete
semester. Four successive discussion themes of two weeks each were dealt with. During the first face-to-face session
of the course, the CSCL environment was demonstrated and the objectives of participating in the discussion were
communicated    to   the students: active  processing    of  the  theoretical base  introduced    during  weekly    face-to-face
working sessions and application of this knowledge while solving authentic cases. Additional information regarding
the expected participation and the criteria for qualitative messages was made available on the course website.
         Fourth-year     students   operated     as  online   tutors to   support   freshmen       in  their   discussions. A
preliminary peer tutor training was organized in a three hour face-to-face session before the onset of the
discussion groups. Tutors were introduced to the multidimensional nature of tutoring in order to master a
relevant mix of tutoring skills.

                                                              737                                                     CSCL 2007
Content Analysis
         Content analysis was applied in order to study the critical thinking processes reflected in the discussions.
More particularly, a content analysis scheme based on Newman, Webb, and Cochrane (1995) was used. Newman et
al. (1995) developed this content analysis instrument based on Garrison's (1992) five stages of critical thinking and
Henri's (1992) cognitive skills. They identify 10 critical thinking categories: relevance, importance, novelty, outside
knowledge, ambiguities, linking ideas, justification, critical assessment, practical utility, and width of the discussion.
For each    category,  a  number    of positive   and  negative  indicators  are   formulated   and most   indicators are    fairly
obvious opposites (Newman et al., 1995). Within the framework of the present study all critical thinking categories
and indicators   distinguished   by    Newman     et al.  (1995) were   adopted.    For each  of   the 9 groups,  the complete
communication in relation to the 4 discussion assignments was analyzed. Two trained coders coded the messages
independently. Inter-rater reliability was calculated and found satisfactory for each category of critical thinking.

Results and Conclusion
         Through    analysis    of  variance   we    contrasted  students'  critical thinking   in  the  experimental  labeling
condition with the presence of critical thinking in the control condition. In a first step of the analysis, we compared
the overall depth of critical thinking. To enable more detailed statements with regard to the differential impact of
both research conditions on students' critical thinking in the discussions, in a second step the global measure of
overall depth of critical thinking was split up by analyzing the ratios for each critical thinking category and the
incidence of the separate critical thinking indicators in the content analysis scheme of Newman et al. (1995). In
order to study the depth of critical thinking taking place in each of Garrison's stages of critical thinking (1992), in
the third step of the analysis each indicator was related to the stage in which it is most expected.
         The   results   concerning    the   comparison   of  both  research   conditions  do   not reveal  an  univocal  image
favoring one research condition. As to the overall depth of critical thinking no significant differences between the
labeling   and control   condition   were    found   (F(1,  1515)=0.970,    p=.325).  Further,  the  conditions  did  not    differ
significantly concerning the discussion of ambiguities (F(1, 1511)=3.277, p=.070), the width of the discussion (F(1,
1506)=0.147,   p=.702),     the introduction   of  new    ideas (F(1,  1472)=0.306,   p=.580)   and    outside knowledge     (F(1,
1345)=2.358, p=.125), the linking of information (F(1, 1343)=0.280, p=.597), and the discussion of the practical
utility of the shared    information   (F(1,   86)=2.057,   p=.155).  Students   in the  experimental   condition however     did
significantly outperform students in the control condition with regard to the relevance (F(1, 1515)=7.454, p=.006)
and importance (F(1, 1515)=3.891, p=.049) of their messages. On the other hand, the control condition attained
higher critical thinking ratios for the following categories: justification (F(1, 1304)=4.738, p=.030) and critical
assessment (F(1, 750)=7.489, p=.006). With regard to Garrison's stages of critical thinking (1992), the analyses
reveal  that students    in the  control     condition posted   significantly   more  messages     focusing on   evaluating   the
applicability of possible solutions to the presented problem (F(1, 1515)=7.277, p=.007), while students in the
knowledge type condition posted significantly more messages focusing on integrating new knowledge with existing
knowledge    (F(1,  1514)=4.473,       p=.035).    Taking     these results into   account, we   cannot  conclude    that asking
students to label their contributions in the discussion has an overall positive impact on their critical thinking. This
could be due to the fact that students were not very consistent in their labeling behavior. Since the discussion system
does not compel students to attach a label to their contributions, it appeared that only in 49.5% of the cases students
in the labeling condition actually tagged their messages by one of the categories reflecting a stage of the progressive
inquiry  model.  Moreover,      the results   indicate that  students'  labels  were rather  one-sided:  40.3%   of  the  tagged
messages received the label "my explanation". These results indicate that students probably need more instructions
and training   before    participating    in discussions   where   they have    to  assign  labels  to messages.  This    finding
corroborates the research of Jeong & Joung (2007) who found that students without previous training only labeled
52% of their messages correctly.
         Apart   from  the  findings   that  students  in  the  experimental   condition   were not always  consistent    in their
labeling behavior and relatively one-sided in the selection of a label for a specific contribution, the equivocal results
concerning   the distinction    between    both   research  conditions  could   be  due to  the tutor  support  that the  groups
experienced as well. Research more specifically indicates that different tutor styles can be distinguished, leading to a
diversity of supportive behavior (De Smet, Van Keer, & Valcke, in press). To verify this hypothesis concerning the
impact   of  differential tutor  support,    the  abovementioned     analyses   of  variance  were   repeated,  including    tutor
variables as covariates in the models. More specifically, the following covariates were included: tutors' participation
and presence    in the   discussions   and    the extent   to  which  they  try to  elicit student  contributions  focusing   on
identifying  a problem,     defining   it more   clearly, exploring   the problem    and   possible solutions,  evaluating   their
applicability, and integrating this understanding with existing knowledge. The results of the analyses of covariance

                                                                738                                                    CSCL 2007
corroborate the  significant impact   of   differential tutor support.   Moreover,  after correction   for the  impact  of  the
characteristics of tutors'  contributions,   a more     unambiguous      picture of the differences    between   the research
conditions  appears. For   none of  the  critical  thinking ratios   the control condition  outperformed    the experimental
labeling condition. No significant differences were found for the following critical thinking categories: relevance
(F(1, 1507)=1.195, p=.139), width of the discussion (F(1, 1498)=0.443, p=.506), outside knowledge (F(1,
1337)=2.181,     p=.140),   justification  (F(1,   1296)=0.447,      p=.504),    and  utility (F(1,    78)=0.515,    p=.475).
Further, the results indicate significantly higher critical thinking ratios for the overall depth of critical thinking (F(1,
1507)=11.480, p=.001), the importance of the contributions (F(1, 1506)=15.862, p<.001), the discussion of
ambiguities (F(1, 1503)=9.166, p=.003), the input of new information and ideas (F(1, 1464)=6.707, p=.010),
the linking of information (F(1, 1335)=5.658, p=.018), and for the critical assessment (F(1, 742)=5.591,
p=.018) reflected in messages in the condition in which students tagged their messages by means of knowledge
types. With regard to Garrison's stages of critical thinking (1992), the analyses reveal that students in the knowledge
type condition posted significantly more messages focusing on defining the problem (F(1, 1506)=13.205, p<.000)
and on integrating the new knowledge with existing knowledge (F(1, 1506)=16.725, p<.000).
These results are in line with the suggestion of Jeong & Joung (2007) who claimed that asking students to label their
messages   could improve    argumentation   but   only  under  certain   circumstances  and   when additional   strategies  are
introduced. Involving peer tutors in the discussion can be seen as a possible way to make scripting by labeling work.

References
Brookfield, S. D. (1987) Developing Critical Thinkers. Challenging adults to explore alternative ways of thinking
         and acting. Milton Keynes: Open University Press.
Chance,  P. (1986).  Thinking   in  the classroom:   A   survey   of programs.    New  York:   Teachers   College, Columbia
         University
Chen, W. (2004). "Supporting Teachers Intervention in Collaborative Knowledge Building". Proceedings of 16th
         European    Conference    on   Artificial Intelligence   (ECAI'2004).    Workshop    on  Artificial   Intelligence in
         Computer Supported Collaborative Learning. Valencia, Spain. E.Gaudioso & L.Talavera (Eds). pp. 1-5.
De Smet, M., Van Keer, H., & Valcke, M. (In press). Blending asynchronous discussion groups and peer tutoring in
         higher education: An exploratory study of online peer tutoring behaviour. Computers and Education.
Dillenbourg, P. (2002). Over-scripting CSCL: The risks of blending collaborative learning with instructional design.
         In P. A. Kirschner (Ed.),      Three  worlds   of  CSCL:    Can  we  support  CSCL?    (pp.   61-91). Heerlen:  Open
         Universiteit Nederland.
Garrison, D. R. (1992). Critical thinking and self-directed learning in adult education: An analysis of responsibility
         and control issues. Adult Education Quarterly, 42, 136-148.
Henri, F.(1991).   Computer     conferencing   and   content   analysis.   In O'Malley,    C.  (Ed.)   Computer    Supported
         Collaborative Learning. Heidelberg: Springer-Verslag.
Huitt, W. (1992). Problem solving and decision making: Consideration of individual differences using the Myers-
         Briggs Type Indicator. Journal of Psychological Type, 24, 33-44.
Jeong, A. & Joung, S. (2007). Scaffolding collaborative argumentation in asynchronous discussions with message
         constraints and message labels. Computers & Education, 48,427-445.
Newman, D. R., Webb, B., & Cochrane, C. (1995). A content analysis method to measure critical thinking in face-
         to-face and computer supported group learning. Interpersonal Computing and Technology, 3, 56-77.
O'Donnell, A. M. (1999). Structuring dyadic interaction through scripted cooperation. In A. M. O'Donnell & A.
         King (Eds.), Cognitive perspectives on peer learning (pp. 179-196). Mahwah, NJ: Erlbaum.
Oliver, R. (2001). Exploring the development of critical thinking skills through a web-supported, problem-based
         learning  environment.    In   J. Stephenson    (Ed.),   Teaching    and   learning  online:   pedagogies   for   new
         technologies, London: Kogan Page, 98-111.
Perkins, C., & Murphy, E. (2006). Identifying and measuring individual engagement in critical thinking in online
         discussions: An exploratory case study. Educational Technology & Society, 9, 298-307.
Scriven, M., &   Paul,  R.  (1992). Critical   thinking  defined.  Paper  presented  at the   Critical Thinking  Conference,
         November 1992, Atlanta, GA.
Weinberger,  A.,  Ertl, B.,  Fischer, F.,  &  Mandl,    H.  (2005).  Epistemic   and social   scripts  in computer-supported
         collaborative learning. Instructional Science, 33, 1-30.

                                                              739                                                    CSCL 2007
