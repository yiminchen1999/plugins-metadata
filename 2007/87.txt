Interaction Analysis in Asynchronous Discussions: Lessons learned
               on the learners' perspective, using the DIAS system
                                    Tharrenos Bratitsis & Angelique Dimitracopoulou
           LTEE Laboratory, University of the Aegean, 1, Av. Democratias, Rhodes, 85100, Greece
                                Email: bratitsis@aegean.gr, adimitr@rhodes.aegean.gr

          Abstract:    DIAS   is an  Asynchronous    Discussion   Forum   Software,   developed     in order   to offer
          extended monitoring and interaction analysis support, by providing a wide range of Interaction
          Analysis     (IA) indicators  jointly used   in various  situations,   to all possible   users   (individual
          students, groups, teachers/moderators, researchers/observers), appropriate for their various roles in
          different activities. In this paper we present a brief overview of the research and results regarding
          the students as IA tool users, deriving from four conducted studies, in educational contexts.

Introduction
          Asynchronous      discussion  forae  are nowadays   widely   used  in  formal    or informal  educational     contexts,
applying   principles   of  constructivism,  emphasizing   in social  interaction   during    learning activities. Research    is
focusing   towards     finding   methods   for  supporting   critical  thinking   through     interactions,  occurring   within
asynchronous discussions, in order to achieve high quality learning. Such a goal requires tools, frameworks and
methods for the facilitation of monitoring and/or self-reflection and therefore selfregulation that could be supported
by the  automated      analysis of  the complex  interactions that  occur.  D.I.A.S.    is a  forum  platform  with  integrated
Interaction Analysis (IA) tools. In the current paper, we present a general overview of the research questions and
results, focusing on one of the perspectives: this of adult learners (students) as IA indicators' users.

Theoretical Background
          Critical Thinking is an intellectual process allowing learners to construct new knowledge through problem
solving and collaboration. While implementing discourse activities by means of discussion forae, higher levels of
interaction are needed to encourage learners to think critically, as indicated throughout the literature (Henri, 1992;
Gunawardena et al, 1997; Garisson et al, 2001), along with internal reflection. It is often necessary for the learner to
externalize   his/her  thoughts  in order to  achieve  proper reflection,   thus promoting    message   writing   in discussion
forae as   an ideal reflective   process. Intensive discussion    and social interaction   may   lead  to multiple   knowledge
construction phases (Schellens & Valcke, 2005). Our main research axis is peer support in asynchronous discussion
learning   activities, in  order to trigger metacognition,    which   leads to   selfregulation,  as well   as to  facilitate the
moderator's tasks. Our intention is to build tools by applying Interaction Analysis techniques in discussions' activity
data, visualizing and providing quantitative information directly to technology-based activities' participants, in order
to self assess their activity (Dimitracopoulou et al, 2005; Dimitracopoulou & Bruillard, in press). The IA results are
presented in an appropriate format (graphical, numerical, literal), interpretable by the users, providing an insight of
their own current or previous activity allowing them to reflect on a cognitive or metacognitive level, and thus act in
order to self-regulate their activities. Additionally, IA provides information to the activity observers, in order to
analyse the complex cognitive and social phenomena that may occur. The expected outcome is the optimization of
the activity through: a) better activity design, regulation, coordination and evaluation by the forum moderator, and b)
refined participation and learning outcome for the students through reflection, self-assessment and self-regulation.

The Discussion Interaction Analysis System (DIAS)
          While examining Forum and Forum Type software, we found several drawbacks in participants' support.
These include minimum analysis information provision, information provided only to a portion of the participants
(e.g. the  teachers),  closed   and/or  complex,   non-transparent analysis  systems    or   even lack  of  empirical   research
(Bratitsis &   Dimitracopoulou,     2006).  This   led us to  the development    of  the   DIAS   system,   a  fully functional
discussion forum platform. We took into account that users involved in a `learning activity' form various cognitive
systems, as individuals (students and teachers in various roles) or members of groups or even communities, thus
expressing different needs for support. Different indicators' sets are addressed to students, teachers, moderators (the
latter having increased information needs while monitoring, assessing, evaluating), or researchers along with the
corresponding Interpretation Schema for various discussion strategies or usage scenarios. An Interpretation Schema

                                                               87                                                     CSCL 2007
explains how to combine different indicators, in order to extract additional, more qualitative information. All the
indicators   are produced  by   measuring   quantitative   activity data. The   implemented   charts  vary    from   having low
(presenting very simple and understandable information) to high interpretative value (providing several aspects of
information,   which   can be   different,  depending    on   the type  of   user who   is reading    the  indicator).   Finally,
customizability, flexibility and interoperability are considered to be crucial characteristics for independent analysis
tools, such as DIAS. More related information can be found in Bratitsis & Dimitracopoulou (2006; in press).

Research Results' Overview
          Four   case studies  implementing     a different   educational  activity approach    have  been    designed   in situ,
constituting the core teaching method for the corresponding semester courses. Similar data collection and analysis
methods were used, including questionnaires, experimental (allowed to review IA indicators) and control groups
(not reviewing indicators) monitoring and semi-structured interviews with every participant. Some of the questions
asked aimed at: (a) Detecting the most/least popular indicators and the latent reasons, (b) Detecting and explain
user behavior alterations due to the indicators' presence, (c) Measure the           frequency of reviewing the indicators,
and   (d) Distinguishing   users'   information   preference  (individual   or  group data,  personal     or  related to others'
actions?). During interviews, all the system's indicators were reviewed and discussed upon, in order to examine
their transparency.   Additionally   we    intended  to record  utilization  ideas  and initial reactions     to the  indicators'
information ­ "What do you understand by observing this diagram?", "Would this affect you and in what way?",
"Do you think this information is important and why?". The most powerful indicators in matters of explanatory
value were correlated with the discussions' actual content, in order to examine possible relations.

          Examining the "influence of IA indicators on the users", we came to the concrete conclusion that they
operate   as a   very powerful  motive     for participation. Users  being   positively surprised   by    the dynamics   of   the
presented information were very enthusiastic and eager to use the IA indicators during the discussion activity (94 out
of 98  agree).   Regarding    "how  often  did  the  users review   the indicators",  almost  60%     did so  every   time  they
connected and 80% at least 2-3 times per week. Researching the "kind of information users were interested in", 70%
of them preferred comparative information, in order to assess their actions in regard to those of their collaborators.
Individual indicators were less preferred (50% of the users), mostly for confirming their impression of their personal
activity. Another important issue for the IA field is "how users decode visualizations". Apparently, that most of the
indicators were adequately transparent. Using simple diagrams, such as bar-charts, XY-charts and scattered charts
facilitates understanding, since everybody is familiar with them. A careful choice of colors may be an additional
facility. For example, a gradient transition from blue to red color in the background of the Classification Indicator
(Bratitsis & Dimitracopoulou, 2006) indicates the desired area for a user to be placed upon. Additionally, through
the interviews, we decided that instructions are necessary in order to better utilize the IA indicators. In some cases,
users understood the main concept of a diagram, but were unable to "read between the lines", detecting more refined
information. Furthermore, combinations of different indicators, in the form of an Interpretative Schemas, should
also be provided, as it is difficult for a simple user to think of all the possibilities, regardless of his/her role.

          Another,    equally  significant issue  is "how  the  indicators   affect the users   and   the learning    process at
extension. Do they help users develop their selfregulation processes? Do they help monitor and assess dialogic
activities?" Apart from functioning as a strong participation motive, which one could ascribe to the users' sensation
of being monitored by the teacher, results of further analysis of users' actions were very encouraging. For example,
postgraduate   students  who   understood   SNA     diagrams  were   tighter connected   with   their collaborators,   than just
reading and writing more messages (in some cases at the expense of content quality). They tried to truly interact
with more collaborators, which resulted in more profitable conversations. Another example is the effect of the Tree
Structure indicator (Bratitsis & Dimitracopoulou, in press), which shows the number of threads within a discussion
forum that an individual user has participated in. Students reviewing this indicator participated in more threads than
those who didn't. These simple examples lead to the conclusion that IA indicators do affect users and the learning
process   at extension. Their   effort to  improve   their interaction  status within the  discussion   activity  consequently
increased the prerequisites for high order thinking and learning. Higher interaction facilitates critical thinking and
sustains effective discussions (Palloff & Pratt, 1999; Garisson et al, 2001; Schellens & Valcke, 2005). In matters of
"facilitating  understanding    and  assessment     of discussions  activities' goals",  the  indicators   helped    students to
evaluate their participation and see if they respected the discussion and the collaborative process. For example, in a
multiple phase activity, some students admitted that various group activity indicators assisted them in better noticing
increased  activity   periods, thus distinguishing   the emerged    course  phases.  In that manner,   they   assisted   them in
understanding the effective activity planning and indicated how and when they should act. More ideas generated by

                                                               88                                                      CSCL 2007
students (while using the indicators on their behalf) clearly showed that specific indicators improve monitoring of
the process and better assessment of the current situation.

Discussion ­ Future Work
          Our main    conclusion  is  that the  use  of   IA indicators  in  asynchronous    discussions   is  an engaging   and
efficient approach. The overall impression was very positive and we were able to observe shifting in users' behavior,
as they appeared more active and productive. Some indicators were more preferred than others, regardless of the
teaching settings, whereas some of them are better utilized under specific context and activity settings. For example
SNA diagrams seem more appropriate when heavy interaction among smaller groups is pursued, whereas Activity
Indicators (Bratitsis & Dimitracopoulou, 2006; in press) seem more appropriate in cases of open ended discussions
with a large number of participants. We consider that a large number of case studies are necessary in order to extract
concrete  results for  that matter.   The  complexity     of the IA   process   evaluation  and  the  variety  of the  produced
diagrams, indicate that this method is useful for medium and large-sized groups of students, as it is easier to review
the actual messages for groups of less than 5 or 6 people. Having produced several Interpretative Schemas, which
were positively evaluated by the participants (Bratitsis & Dimitracopoulou, 2006; in press), we were very surprised
to see that users came up with new ideas for utilizing indicators. New indicators were built in the process, as new
needs were expressed. This seems to be a perpetual process, which may lead to the creation of an "Idea Repository".
Detailed  instructions  are required,  if  we  wish    users to exploit the  indicators.   Otherwise,  the produced    diagrams
would seem like an additional workload, with no clear meaning. Consequently, users would avoid taking them into
account

          Future  plans include   conducting    additional   case  studies,  in order   to explore further    aspects of the IA
perspective. Results found under specific learning settings, should be tested for validity under different settings (for
example using a different collaborative learning strategy). Furthermore, new questions arose. Does age influence the
users' perception  and   decoding     of  visualized   information?   Do     indicators presenting   similar  information   with
different visualizations affect users in a different way? If yes, when should each approach be used? For example,
some indicators present comparative activity information using absolute values and others use percentages. Thus,
the gaps  within  the charts appear   larger   in the  first case. Could this   be a  reluctant factor for   a less active  user,
assuming   that bigger  effort is needed   in  order   to improve    his/her position?  On  the other  hand,   could  this affect
likewise more active users, leading them to reduce contributions? Would using smaller gaps affect user motivation?
The variety of new questions is quite big, but all of them relate to research refinement of the indicators' effect on the
users individually, as a group or a community and the learning process. The overall conclusion that applying IA
methods for building tools to support the participants of an asynchronous discussion activity is one step at the right
direction, should be the main lesson learned from this approach.

References
Bratitsis, T., Dimitracopoulou, A. (2006a). Indicators for Measuring Quality in Asynchronous Discussion Forae. The
          12th International Workshop on Groupware, CRIWG 2006, Spain: Springer Verlag, 54-61
Bratitsis T.  &  Dimitracopoulou      A.  (in  press), Monitoring    and Analysing      Group Interactions    in  Asynchronous
          Discussions with DIAS system, Special Issue, International Journal of e-Collaboration ( IJeC).
Dimitracopoulou    A.  &    Bruillard E.   (in press).    Interfaces de  Forum   enrichies   par la   visualization   d'analyses
          automatiques des interactions et du contenu. Special Issue, Sciences et Techniques Educatives
Dimitracopoulou, A et al. (2005). State of the art of Interaction Analysis for Metacognitive Support & Diagnosis. IA
          JEIRP Deliverable D.31.1.1. Kaleidoscope NoE. Available online at: www.noe-kaleidoscope.org
Garisson, D.R., Anderson, T., Archer, W. (2001). Critical thinking, cognitive presence and computer conferencing
          in distance education. American Journal of Distance Education, 15(1), 7-23
Gunawardena, C.N., Lowe, C.A., Anderson, T. (1997). Analysis of a global online debate and the development of an
          interaction analysis  model    for  examining    social  construction  of  knowledge     in computer    conferencing,
          Journal of Educational Computing Research, 17(4), 397-431.
Henri, F. (1992). Computer conferencing and content analysis. In A.R. Kaye (Ed). Collaborative learning through
          computer conferencing: The Najaden papers, 117-136. Berlin: Springer-Verlag.
Palloff, R.M., Pratt, K. (1999). Building Learning Communities in Cyberspace: Effective strategies for the online
          classroom, Jossey-Bass Publishers, San Fransisco.
Schellens, T., Valcke, M. (2005). Collaborative learning in asynchronous discussion groups: What about the impact
          on cognitive processing? Computers in Human Behavior, 21, 957-975

                                                                89                                                     CSCL 2007
